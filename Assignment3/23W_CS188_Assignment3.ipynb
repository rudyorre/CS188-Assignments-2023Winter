{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "31XHfIbPF1qz"
   },
   "source": [
    "# Assignment3 \n",
    "\n",
    "Welcome to the second assignment! We're excited to see what you'll create using the techniques you've learned in this course.\n",
    "\n",
    "First of all, please type your name and UID in the following format:\n",
    "\n",
    "Firstname Lastname, #UID\n",
    "\n",
    "For example: Sicheng Mo, #401234567\n",
    "\n",
    "In this assignment we will use Vision Transformer, and pretrained ResNet to to classification on the Miniplaces dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpk4pVqZfmrP"
   },
   "outputs": [],
   "source": [
    "#@title Your Info { display-mode: \"form\" }\n",
    "\n",
    "Name = 'Sicheng Mo' #@param {type:\"string\"}\n",
    "UID = '401234567' #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X42ej8ZhF1q4"
   },
   "source": [
    "## Goals\n",
    "The goals of this assignment are to:\n",
    "\n",
    "1. Build ResNet and Finetune it different strategies\n",
    "2. Learn how to visualize CNNs.\n",
    "3. Build a Vision Transformer from scratch .\n",
    " \n",
    "\n",
    "By the end of this assignment, you will have gained experience with:\n",
    "\n",
    "- Working with PyTorch and the MiniPlaces dataset for image classification.\n",
    "- Implementing and training different types of neural networks using PyTorch.\n",
    "- Debugging and troubleshooting issues that may arise during the development process.\n",
    "\n",
    "Please note that it may take some time to run the entire notebook and prepare the submission version. Make sure to allocate enough time for this task and start early. If you have any questions or run into any issues, please feel free to raise them in the Piazza forum or search the internet for debugging purposes. However, please do not directly copy code from other sources.\n",
    "\n",
    "This assignment is due on ***Feb 26th***.\n",
    "\n",
    "**Do not use any Code AI to finish the assignment.**\n",
    "\n",
    "\n",
    "Good luck and happy coding! Remember, the most important thing is to have fun and learn something new.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_L6ZjCc5F1q5"
   },
   "source": [
    "## Setup Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKJx4TLuF1q5"
   },
   "source": [
    "To begin, you will need to download the MiniPlaces dataset using the provided link. \n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "Recall the introduction about the storage system of CoLab we went through in the assignment 1. For efficient development of our models, we will still use the temporary storage space to hold our data. This means that every time you open up this notebook, we will need to re-download and process the dataset. Don't worry though - this shouldn't take long, usually just a minute or less. Okay, let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNtgNN3FF1q6"
   },
   "outputs": [],
   "source": [
    "!pip install einops\n",
    "# Downloading this file takes about a few seconds.\n",
    "# Download the tar.gz file from google drive using its file ID.\n",
    "!pip3 install --upgrade gdown --quiet\n",
    "#!gdown 16GYHdSWS3iMYwMPv5FpeDZN2rH7PR0F2 # this is the file ID of miniplaces dataset\n",
    "!gdown 1CyIQOJienhNITwGcQ9h-nv8z6GOjV2HX\n",
    "# back-up commands (try the following it previous file id is overload)\n",
    "# !gdown 1CyIQOJienhNITwGcQ9h-nv8z6GOjV2HX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tB8tvZhIF1q9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from tqdm import tqdm\n",
    "import urllib.request\n",
    "\n",
    "def setup(file_link_dict={},\n",
    "          folder_name='Assignment3'):\n",
    "  # Let's make our assignment directory\n",
    "  CS188_path = './'\n",
    "  os.makedirs(os.path.join(CS188_path, 'Assignment3', 'data'), exist_ok=True)\n",
    "  # Now, let's specify the assignment path we will be working with as the root.\n",
    "  root_dir = os.path.join(CS188_path, 'Assignment3')\n",
    "  # Open the tar.gz file\n",
    "  tar = tarfile.open(\"data.tar.gz\", \"r:gz\")\n",
    "  # Extract the file \"./Assignment2/data\" folder\n",
    "  total_size = sum(f.size for f in tar.getmembers())\n",
    "  with tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Extracting tar.gz file\") as pbar:\n",
    "      for member in tar.getmembers():\n",
    "          tar.extract(member, os.path.join(root_dir, 'data'))\n",
    "          pbar.update(member.size)\n",
    "  # Close the tar.gz file\n",
    "  tar.close()\n",
    "  # Next, we download the train/val/test txt files:\n",
    "  for file_name, file_link in file_link_dict.items():\n",
    "      print(f'Downloding {file_name}.txt from {file_link}')\n",
    "      urllib.request.urlretrieve(file_link, f'{root_dir}/data/{file_name}.txt')\n",
    "  return root_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wx-2pvciF1q-"
   },
   "outputs": [],
   "source": [
    "val_url = 'https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/val.txt'\n",
    "train_url = 'https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/train.txt'\n",
    "root_dir = setup(\n",
    "    file_link_dict={'train':train_url, 'val':val_url},\n",
    "    folder_name='Assignment3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D4MyTph9F1q_"
   },
   "source": [
    "### Define the data transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1v9w2JugF1rA"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define data transformation\n",
    "# You can copy your data transform from Assignment2. \n",
    "# Notice we are resize images to 128x128 instead of 64x64.\n",
    "data_transform = transforms.Compose([\n",
    "    ################# Your Implementations #####################################\n",
    "    # TODO: Resize image to 128x128\n",
    "    pass\n",
    "    ################# End of your Implementations ##############################\n",
    "    transforms.ToTensor(),\n",
    "    ################# Your Implementations #####################################\n",
    "    # TODO: Normalize image using ImageNet statistics\n",
    "    pass\n",
    "    ################# End of your Implementations ##############################\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXrmPXGEF1rB"
   },
   "source": [
    "### Define the dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KoHHge5YF1rB"
   },
   "outputs": [],
   "source": [
    "# You can copy your dataset from Assignment2. \n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class MiniPlaces(Dataset):\n",
    "    def __init__(self, root_dir, split, transform=None, label_dict=None):\n",
    "        \"\"\"\n",
    "        Initialize the MiniPlaces dataset with the root directory for the images, \n",
    "        the split (train/val/test), an optional data transformation, \n",
    "        and an optional label dictionary.\n",
    "        \n",
    "        Args:\n",
    "            root_dir (str): Root directory for the MiniPlaces images.\n",
    "            split (str): Split to use ('train', 'val', or 'test').\n",
    "            transform (callable, optional): Optional data transformation to apply to the images.\n",
    "            label_dict (dict, optional): Optional dictionary mapping integer labels to class names.\n",
    "        \"\"\"\n",
    "        assert split in ['train', 'val', 'test']\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.filenames = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Take a second to think why we need this line.\n",
    "        # Hints: training set / validation set / test set.\n",
    "        self.label_dict = label_dict if label_dict is not None else {}\n",
    "\n",
    "        # You should\n",
    "        #   1. Load the train/val text file based on the `split` argument and\n",
    "        #     store the image filenames and labels.\n",
    "        #   2. Extract the class names from the image filenames and store them in \n",
    "        #     self.label_dict.\n",
    "        #   3. Construct a label dict that maps integer labels to class names, if \n",
    "        #     the current split is \"train\" \n",
    "        ################# Your Implementations #####################################\n",
    "        pass\n",
    "\n",
    "        ################# End of your Implementations ##############################\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of images in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            int: Number of images in the dataset.\n",
    "        \"\"\"\n",
    "        ################# Your Implementations #####################################\n",
    "        # Return the number of images in the dataset\n",
    "        return len(self.filenames)\n",
    "        ################# End of your Implementations ##############################\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return a single image and its corresponding label when given an index.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the image to retrieve.\n",
    "            \n",
    "        Returns:\n",
    "            tuple: Tuple containing the image and its label.\n",
    "        \"\"\"\n",
    "        image = None\n",
    "        label = None\n",
    "        ################# Your Implementations #####################################\n",
    "        # Load and preprocess image using self.root_dir, \n",
    "        # self.filenames[idx], and self.transform (if specified)\n",
    "        pass\n",
    "\n",
    "        ################# End of your Implementations ##############################\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_th2HTIF1rC"
   },
   "source": [
    "### Define the train method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfX-uwDDF1rC"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
    "    \"\"\"\n",
    "    Train the MLP classifier on the training set and evaluate it on the validation set every epoch.\n",
    "    \n",
    "    Args:\n",
    "        model (MLP): MLP classifier to train.\n",
    "        train_loader (torch.utils.data.DataLoader): Data loader for the training set.\n",
    "        val_loader (torch.utils.data.DataLoader): Data loader for the validation set.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer to use for training.\n",
    "        criterion (callable): Loss function to use for training.\n",
    "        device (torch.device): Device to use for training.\n",
    "        num_epochs (int): Number of epochs to train the model.\n",
    "    \"\"\"\n",
    "    # Place model on device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        # Use tqdm to display a progress bar during training\n",
    "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}') as pbar:\n",
    "            for inputs, labels in train_loader:\n",
    "                # Move inputs and labels to device\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Zero out gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Compute the logits and loss\n",
    "                logits = model(inputs)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                # Backpropagate the loss\n",
    "                loss.backward()\n",
    "                \n",
    "                # Update the weights\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Update the progress bar\n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        # Evaluate the model on the validation set\n",
    "        avg_loss, accuracy = evaluate(model, val_loader, criterion, device)\n",
    "        print(f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}')\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the MLP classifier on the test set.\n",
    "    \n",
    "    Args:\n",
    "        model (MLP): MLP classifier to evaluate.\n",
    "        test_loader (torch.utils.data.DataLoader): Data loader for the test set.\n",
    "        criterion (callable): Loss function to use for evaluation.\n",
    "        device (torch.device): Device to use for evaluation.\n",
    "        \n",
    "    Returns:\n",
    "        float: Average loss on the test set.\n",
    "        float: Accuracy on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        num_correct = 0\n",
    "        num_samples = 0\n",
    "        \n",
    "        for inputs, labels in test_loader:\n",
    "            # Move inputs and labels to device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Compute the logits and loss\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Compute the accuracy\n",
    "            _, predictions = torch.max(logits, dim=1)\n",
    "            num_correct += (predictions == labels).sum().item()\n",
    "            num_samples += len(inputs)\n",
    "            \n",
    "    # Compute the average loss and accuracy\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = num_correct / num_samples\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the KNN"
   ],
   "metadata": {
    "id": "BFVUfTT6Z6OT"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2Z9TE_UF1rF"
   },
   "outputs": [],
   "source": [
    "def compute_distances_no_loops(x_train, x_test):\n",
    "  num_train = x_train.shape[0]\n",
    "  num_test = x_test.shape[0]\n",
    "  dists = x_train.new_zeros(num_train, num_test)\n",
    "\n",
    "  A = x_train.reshape(num_train,-1)\n",
    "  B = x_test.reshape(num_test,-1)\n",
    "  AB2 = A.mm(B.T)*2\n",
    "  dists = ((A**2).sum(dim = 1).reshape(-1,1) - AB2 + (B**2).sum(dim = 1).reshape(1,-1))**(1/2)\n",
    "  return dists\n",
    "\n",
    "def predict_labels(dists, y_train, k=1):\n",
    "  num_train, num_test = dists.shape\n",
    "  y_pred = torch.zeros(num_test, dtype=torch.int64)\n",
    "\n",
    "  values, indices = torch.topk(dists, k, dim=0, largest=False)\n",
    "  for i in range(indices.shape[1]):\n",
    "    _, idx = torch.max(y_train[indices[:,i]].bincount(), dim = 0)\n",
    "    y_pred[i] = idx\n",
    "  return indices, y_pred\n",
    "\n",
    "class KnnClassifier:\n",
    "  def __init__(self, x_train, y_train):\n",
    "    self.x_train = x_train\n",
    "    self.y_train = y_train\n",
    "    \n",
    "  def predict(self, x_test, k=1):\n",
    "    y_test_pred = None\n",
    "\n",
    "    dists = compute_distances_no_loops(self.x_train, x_test)\n",
    "    _, y_test_pred =  predict_labels(dists, self.y_train, k)\n",
    "\n",
    "    return y_test_pred\n",
    "  \n",
    "  def check_accuracy(self, x_test, y_test, k=1, quiet=False):\n",
    "    y_test_pred = self.predict(x_test, k=k)\n",
    "    num_samples = x_test.shape[0]\n",
    "    num_correct = (y_test == y_test_pred).sum().item()\n",
    "    accuracy = 100.0 * num_correct / num_samples\n",
    "    msg = (f'Got {num_correct} / {num_samples} correct; '\n",
    "           f'accuracy is {accuracy:.2f}%')\n",
    "    if not quiet:\n",
    "      print(msg)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pob9aqlkF1rF"
   },
   "outputs": [],
   "source": [
    "# Also, seed everything for reproducibility\n",
    "# code from https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964#file-seed_everything-py\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMTQ7y7lF1rF"
   },
   "outputs": [],
   "source": [
    "# Define the device to use for training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device == torch.device('cuda'):\n",
    "    print(f'Using device: {device}. Good to go!')\n",
    "else:\n",
    "    print('Please set GPU via Edit -> Notebook Settings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y67XuOdxF1rG"
   },
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAgPy9WcR9X5"
   },
   "source": [
    "## Q1 ResNet (30 pts)\n",
    "\n",
    "In this question, you will learn how to use pretrained model and apply transfer learning on ResNet18."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbkkxjvMU67L"
   },
   "source": [
    "### Q1.1 Build the ResNet (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exoAuQN5F1rU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.models as models\n",
    "class Resnet(nn.Module):\n",
    "    def __init__(self, mode='finetune',pretrained=True):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        use the resnet18 model from torchvision models. Remember to set pretrained as true\n",
    "        \n",
    "        mode has three options:\n",
    "        1) features: to extract features only, we do not want the last fully connected layer of \n",
    "            resnet18. Use nn.Identity() to replace this layer.\n",
    "        2) linear: For this model, we want to freeze resnet18 features, then train a linear \n",
    "            classifier which takes the features before FC (again we do not want \n",
    "            resnet18 FC). And then write our own FC layer: which takes in the features and \n",
    "            output scores of size 100 (because we have 100 categories).\n",
    "            Because we want to freeze resnet18 features, we have to iterate through parameters()\n",
    "            of our model, and manually set some parameters to requires_grad = False\n",
    "            Or use other methods to freeze the features\n",
    "        3) finetune: Same as 2), except that we we do not need to freeze the features and\n",
    "           can finetune on the pretrained resnet model.\n",
    "        \"\"\"\n",
    "        self.resnet = None\n",
    "    ########################################Your Code#################################### \n",
    "      pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #####################################################################################\n",
    "\n",
    "    def forward(self, x):\n",
    "    ########################################Your Code#################################### \n",
    "        pass\n",
    "    #####################################################################################\n",
    "        return x\n",
    "    \n",
    "    def to(self,device):\n",
    "        return self.resnet.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FA4FCBixCarC"
   },
   "source": [
    "### Q1.2 Feature comparison (5 pts)\n",
    "Recall that in Assignment1, we used color histogram features for KNN classification.\n",
    "In this assignment, we will use features from ResNet18 and compare the results.\n",
    "#### Color Features\n",
    "We will first used the color features as in Assignment1. You do not need to implement anything here. Copy your implementation of KNN in model.py, then run the codes below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DO6bqQHAwqOr"
   },
   "outputs": [],
   "source": [
    "seed_everything(0)\n",
    "data_root = os.path.join(root_dir, 'data')\n",
    "train_dataset = MiniPlaces(\n",
    "    root_dir=data_root, split='train', \n",
    "    transform=data_transform)\n",
    "\n",
    "val_dataset = MiniPlaces(\n",
    "    root_dir=data_root, split='val', \n",
    "    transform=data_transform,\n",
    "    label_dict=train_dataset.label_dict)\n",
    "\n",
    "\n",
    "\n",
    "sub_sample = list(range(0, len(train_dataset), 50))\n",
    "print(len(sub_sample))\n",
    "training_data1 = torch.utils.data.Subset(train_dataset, sub_sample)\n",
    "training_loader1 = torch.utils.data.DataLoader(training_data1, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "sub_sample = list(range(0, len(val_dataset),16))\n",
    "val_dataset1 = torch.utils.data.Subset(val_dataset, sub_sample)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(val_dataset1, batch_size=128, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for i, data in enumerate(tqdm((training_loader1))):\n",
    "    inputs, label = data\n",
    "\n",
    "    all_features.append(inputs)\n",
    "    all_labels.append(label)\n",
    "\n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2_Ip8odx2nF"
   },
   "outputs": [],
   "source": [
    "seed_everything(0)\n",
    "\n",
    "\n",
    "total_acc = 0\n",
    "true_pos = 0\n",
    "total = 0\n",
    "knn = KnnClassifier(all_features, all_labels)\n",
    "for i, data in enumerate(tqdm((test_loader))):\n",
    "    inputs, label = data\n",
    "    acc = knn.check_accuracy(inputs, label, k=10, quiet=True)\n",
    "    true_pos += acc*inputs.shape[0]\n",
    "    total += inputs.shape[0]\n",
    "    \n",
    "total_acc = true_pos / total\n",
    "print (\"total accuracy is %.2f\"%(total_acc/100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oMGIHr9_yiW"
   },
   "source": [
    "I got an accuracy of 4% using my own implementation. How about you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w45nyWfWzcTJ"
   },
   "source": [
    "\n",
    "Implement the Resnet class above. First write the case where \"mode=feature\". This means that we want to discard the final FC layer of ResNet18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8mbTFMl8zZKU"
   },
   "outputs": [],
   "source": [
    "resnet18 = Resnet(mode='feature',pretrained=True)\n",
    "resnet18.to(device)\n",
    "\n",
    "\n",
    "all_features = []\n",
    "all_labels = []\n",
    "\n",
    "for i, data in enumerate(tqdm((training_loader1))):\n",
    "    inputs, label = data\n",
    "    inputs = inputs.cuda()\n",
    "    cnn_features = resnet18(inputs).cpu()\n",
    "    \n",
    "    all_features.append(cnn_features)\n",
    "    all_labels.append(label)\n",
    "\n",
    "all_features = torch.cat(all_features, dim=0)\n",
    "all_labels = torch.cat(all_labels, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z5hRiOSWzuNi"
   },
   "outputs": [],
   "source": [
    "knn = KnnClassifier(all_features, all_labels)\n",
    "true_pos = 0\n",
    "total = 0\n",
    "for i, data in enumerate(tqdm((test_loader))):\n",
    "    inputs, label = data\n",
    "    inputs = inputs.cuda()\n",
    "    features = resnet18(inputs).cpu()\n",
    "    acc = knn.check_accuracy(features, label, k=5, quiet=True)\n",
    "    true_pos += acc*inputs.shape[0]\n",
    "    total += inputs.shape[0]\n",
    "    \n",
    "total_acc = true_pos / total\n",
    "print (\"total accuracy is %.2f\"%(total_acc/100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbAkvoGn_3GQ"
   },
   "source": [
    "I got an accuracy of 10% using my own implementation. How about you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqC58lscztxa"
   },
   "source": [
    "What's your opinion on using different features? Pros & Cons?\n",
    "\n",
    "Your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwHkysWXHaAs"
   },
   "source": [
    "### Q1.3 Pretrained ResNet features + Linear Classifier (5 pts)\n",
    "\n",
    "Then we implement the “linear” mode in Resnet class in model.py. Remember to freeze the features of ResNet. In this implementation we use linear classifier to do classification on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tAjaC7YBHaAs"
   },
   "outputs": [],
   "source": [
    "seed_everything(0)\n",
    "\n",
    "# Define the model, optimizer, and criterion (loss_fn)\n",
    "model = Resnet(mode='linear',pretrained=True)\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(), \n",
    "    lr=0.005, \n",
    "    momentum=0.9)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Define the dataset and data transform with flatten functions appended\n",
    "data_root = os.path.join(root_dir, 'data')\n",
    "train_dataset = MiniPlaces(\n",
    "    root_dir=data_root, split='train', \n",
    "    transform=data_transform)\n",
    "\n",
    "val_dataset = MiniPlaces(\n",
    "    root_dir=data_root, split='val', \n",
    "    transform=data_transform,\n",
    "    label_dict=train_dataset.label_dict)\n",
    "\n",
    "# Define the batch size and number of workers\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqtaJPLhD64R"
   },
   "source": [
    "I got an accuracy of 40.45% using my own implementation. How about you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsGFydepSssy"
   },
   "source": [
    "Answing in 2-3 sentence. How does linear evaluation differ from finetuning, and when should we opt for it instead?\n",
    "\n",
    "\n",
    "[Your answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdSLohl5U_YE"
   },
   "source": [
    "### Q1.4 Finetune pretrained ResNet (15 pts)\n",
    "Fine-tuning a pre-trained ResNet model on a specific task, such as image classification or object detection, can help improve its performance on that task by adapting the model's learned representations to the particular dataset and task at hand. This is because the pre-trained ResNet has already learned useful feature representations on a large dataset, and fine-tuning allows it to further specialize those representations to the specific task.\n",
    "\n",
    "In this implementation, instead of freezing resnet features, we want to finetune it. Implement the \"finetune\" mode in Resnet class in model.py. The implementation is the same as the \"linear\" mode, except that we do not need to set requires_grad to False to freeze the features. Here, we are going to finetune the ResNet using different strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BUZNBNrWuJ5"
   },
   "source": [
    "#### Q1.4.1 Fully finetune with same learning rate (4 pts)\n",
    "End-to-end finetune with SGD optimizer. Setting lr to 0.01 and momentum to 0.9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ioTgrPVmLMf"
   },
   "outputs": [],
   "source": [
    "seed_everything(0)\n",
    "\n",
    "# Define the model, optimizer, and criterion (loss_fn)\n",
    "resnet_finetune1 = Resnet(mode='finetune',pretrained=True)\n",
    "\n",
    "optimizer = None\n",
    "################# Your Implementations #################################\n",
    "pass\n",
    "\n",
    "################# Your Implementations #################################\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Define the dataset and data transform with flatten functions appended\n",
    "data_root = os.path.join(root_dir, 'data')\n",
    "train_dataset = MiniPlaces(\n",
    "    root_dir=data_root, split='train', \n",
    "    transform=data_transform)\n",
    "\n",
    "val_dataset = MiniPlaces(\n",
    "    root_dir=data_root, split='val', \n",
    "    transform=data_transform,\n",
    "    label_dict=train_dataset.label_dict)\n",
    "\n",
    "# Define the batch size and number of workers\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "train(resnet_finetune1, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZXvlFvmMkEW"
   },
   "source": [
    "I got an accuracy of 45.62% using my own implementation. How about you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktHS7xCMWzEL"
   },
   "source": [
    "#### Q1.4.2 Fully finetune with different learning rates (4 pts)\n",
    "\n",
    "Lower layers in a network typically learn low-level features such as edges and textures that are useful across a wide range of tasks, and therefore may require smaller updates to prevent overfitting. Meanwhile, higher layers may learn more task-specific features that require larger updates to improve performance.\n",
    "\n",
    "Here, we finetune the pretrained ResNet with SGD optimizer with momentum = 0.9. We will need to assign different learning rate to different layers.\n",
    "\n",
    "\n",
    "*   Setting lr to 0.01 for the last fc layer.\n",
    "*   Setting lr to 0.001 for the rest layers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Rid4RJYmnrG"
   },
   "outputs": [],
   "source": [
    "seed_everything(0)\n",
    "\n",
    "# Define the model, optimizer, and criterion (loss_fn)\n",
    "resnet_finetune2 = Resnet(mode='finetune',pretrained=True)\n",
    "\n",
    "optimizer = None\n",
    "################# Your Implementations #################################\n",
    "# Hint, read the official document for SGD to check how to set different group of params\n",
    "# https://pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
    "\n",
    "pass\n",
    "\n",
    "\n",
    "################# Your Implementations #################################\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Define the dataset and data transform with flatten functions appended\n",
    "data_root = os.path.join(root_dir, 'data')\n",
    "train_dataset = MiniPlaces(\n",
    "    root_dir=data_root, split='train', \n",
    "    transform=data_transform)\n",
    "\n",
    "val_dataset = MiniPlaces(\n",
    "    root_dir=data_root, split='val', \n",
    "    transform=data_transform,\n",
    "    label_dict=train_dataset.label_dict)\n",
    "\n",
    "# Define the batch size and number of workers\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "train(resnet_finetune2, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sh5yf5ZoZLUf"
   },
   "source": [
    "I got an accuracy of 47.23% using my own implementation. How about you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwD8qM8OXNdC"
   },
   "source": [
    "#### Q1.4.3 Fintune with few forzen layers (3 pts)\n",
    "Freezing some layers during fine-tuning can help to prevent overfitting and speed up the training process. When we fine-tune a pre-trained neural network, we typically want to retain the learned feature representations in the lower layers of the network, which are often more general and transferable across different tasks. By freezing these lower layers, we prevent their weights from being updated during fine-tuning, which helps to ensure that the model retains its learned feature representations. This can be particularly important if we have a small amount of data available for the specific task we are fine-tuning for.\n",
    "\n",
    "In this step, you will define a new optimizer to forzen all the parameters in resnet.layer1 and resnet.layer2. \n",
    "\n",
    "Here, we finetune the pretrained ResNet with SGD optimizer with momentum = 0.9. We will need to assign different learning rate to different layers.\n",
    "\n",
    "\n",
    "*   Setting lr to 0.01 for the last fc layer.\n",
    "*   Setting lr to 0 for the resnet.layer1 and resnet.layer2 layers.\n",
    "*   Setting lr to 0.001 for the rest layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSSDYIAO7FuE"
   },
   "outputs": [],
   "source": [
    "seed_everything(0)\n",
    "\n",
    "# Define the model, optimizer, and criterion (loss_fn)\n",
    "resnet_finetune3 = Resnet(mode='finetune',pretrained=True)\n",
    "\n",
    "\n",
    "################# Your Implementations #################################\n",
    "pass\n",
    "\n",
    "\n",
    "################# End of your Implementations ##########################\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Define the dataset and data transform with flatten functions appended\n",
    "data_root = os.path.join(root_dir, 'data')\n",
    "train_dataset = MiniPlaces(\n",
    "    root_dir=data_root, split='train', \n",
    "    transform=data_transform)\n",
    "\n",
    "val_dataset = MiniPlaces(\n",
    "    root_dir=data_root, split='val', \n",
    "    transform=data_transform,\n",
    "    label_dict=train_dataset.label_dict)\n",
    "\n",
    "# Define the batch size and number of workers\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "train(resnet_finetune3, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbYxF8Bd8uoe"
   },
   "source": [
    "#### Q1.2.4 Comparsion bewteen different method (4 pts)\n",
    "Q1: Which method perform better in method 1 and method 2. Give a explaination in two to three sentences.\n",
    "\n",
    "\n",
    "Q2: Which method perform better in method 2 and method 3. Give a explaination in two to three sentences.\n",
    "\n",
    "\n",
    "################# Your Answer #################################\n",
    "\n",
    "\n",
    "\n",
    "################# END of Your Answer ##########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty5OoHhESPrO"
   },
   "source": [
    "## Q2 Visulization (10 pts)\n",
    "The attention maps in Convolutional Neural Networks (CNNs) and Transformers are used to visualize the regions of an input image that the model is paying attention to when making predictions. However, the way these attention maps are computed and the information they convey are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohJx-EwATHEz"
   },
   "source": [
    "### Q2.1 CNN Visualization (10 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7wRpzJQHaAu"
   },
   "source": [
    "\n",
    "Now we will implement Prof. Zhou's famous paper: \"Learning Deep Features for Discriminative Localization\", in which he proposed class activation mapping (CAM). The main equation is:\n",
    "$$S_{c}=\\sum_{k} w_{k}^{c} \\sum_{x, y} f_{k}(x, y)=\\sum_{x, y} \\sum_{k} w_{k}^{c} f_{k}(x, y)$$\n",
    "where $f_{k}(x, y)$ represents the activation of unit k in the last convolutional layer, which is layer4 in resnet18.\n",
    "\n",
    "For more detailed implementation, please refer to the github repo: https://github.com/zhoubolei/CAM\n",
    "\n",
    "Please implement the CAM function in model.py. Specifically, given convolutional features, weights and a class index, CAM will output the reasons for classifying the image to the class, thus making CNN interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eE62mOwFEVhn"
   },
   "outputs": [],
   "source": [
    "def CAM(feature_conv, weight_softmax, class_idx):\n",
    "    \"\"\"\n",
    "    Implement CAM here\n",
    "    generate the class activation maps upsample to 256x256\n",
    "    refer to: https://github.com/zhoubolei/CAM\n",
    "    \"\"\"\n",
    "    output_cam = []\n",
    "    ###########################Your Code#######################################\n",
    "    pass\n",
    "\n",
    "\n",
    "    #############################################################################\n",
    "    return output_cam\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FctjCANGDiw8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "seed_everything(0)\n",
    "resnet_finetune3.eval()\n",
    "\n",
    "finalconv_name = 'layer4'\n",
    "# hook the feature extractor\n",
    "features_blobs = []\n",
    "\n",
    "def hook_feature(module, input, output):\n",
    "    features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "resnet_finetune3.resnet._modules.get(finalconv_name).register_forward_hook(hook_feature)\n",
    "\n",
    "params = list(resnet_finetune3.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].data.cpu().numpy())\n",
    "\n",
    "classes = []\n",
    "for i in range(100):\n",
    "  classes.append(str(i))\n",
    "\n",
    "val_dataset = MiniPlaces(\n",
    "    root_dir=data_root, split='val', \n",
    "    transform=data_transform,\n",
    "    label_dict=train_dataset.label_dict)\n",
    "# val_dataset1 = val_dataset[:20]\n",
    "val_dataset1 = torch.utils.data.Subset(val_dataset, list(range(0,10000,1000)))\n",
    "print(len(val_dataset1))\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset1, batch_size=1, num_workers=num_workers, shuffle=True)\n",
    "\n",
    "figure = plt.figure(figsize=(24, 6))\n",
    "cols, rows = 10, 2\n",
    "counter = 0 \n",
    "\n",
    "for i, data in enumerate(tqdm((val_loader))):\n",
    "    features_blobs = []\n",
    "    img, label = data\n",
    "    img = img.cuda()\n",
    "    \n",
    "    logit = resnet_finetune3(img)\n",
    "    \n",
    "    h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "    probs, idx = h_x.sort(0, True)\n",
    "    probs = probs.cpu().numpy()\n",
    "    idx = idx.cpu().numpy()\n",
    "\n",
    "    # generate class activation mapping for the top1 prediction\n",
    "    CAMs = CAM(features_blobs[0], weight_softmax, [idx[0]])\n",
    "\n",
    "    img = img.squeeze().permute(1,2,0)\n",
    "    height, width, _ = img.shape\n",
    "    img = img.cpu() * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n",
    "    img = img.cpu().numpy()\n",
    "    heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "    heatmap = heatmap/255\n",
    "    result = heatmap * 0.7 + img * 0.3\n",
    "    \n",
    "    figure.add_subplot(rows, cols, i+1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(classes[label])\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "\n",
    "  \n",
    "    figure.add_subplot(rows, cols, i+11)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(classes[idx[0]])\n",
    "\n",
    "    plt.imshow(result, cmap=\"gray\")\n",
    "    counter +=1 \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "532GJGCHF1rI"
   },
   "source": [
    "## Q3: Steps to build a ViT from scratch (60 pts)\n",
    "Vision Transformer (ViT) is a state-of-the-art neural network architecture for image classification tasks. Unlike traditional convolutional neural networks (CNNs), which have been the standard in computer vision for many years, ViT relies on a self-attention mechanism to extract features from images. This approach has shown to achieve competitive results on various benchmark datasets, while also offering the flexibility to handle tasks that require attention over long-range dependencies in images. ViT has quickly gained popularity in the computer vision community, and has spurred further research into the use of self-attention mechanisms in other areas of deep learning.\n",
    "\n",
    "You will implement the ViT model on the Miniplaces dataset.\n",
    "\n",
    "To implement ViT model for image classification, you will need to follow these steps：\n",
    "1.  Extract feature vectors from the input images using a trainable linear projection layer, which converts the 2D image patches into 1D feature vectors.\n",
    "2. Positional encoding: Add a learnable positional encoding to each feature vector, which provides spatial information to the model.\n",
    "3. Transformer encoder: Stack multiple Transformer encoder layers to process the encoded features, which allows the model to learn both local and global interactions between the image patches.\n",
    "4. Classification head: Add a classification head on top of the final encoded feature vector, which maps the learned representations to the corresponding class labels.\n",
    "5. Training and evaluation: Train the ViT model using an appropriate optimization algorithm and loss function, and evaluate its performance on the validation and testing sets.\n",
    "\n",
    "If you are not familiair with ViT model, then you can read our textbook [Transformers for Vision](https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html#fig-vit), or review our [discussion slides](https://drive.google.com/file/d/1RKSnE9MOAGBu9T-_2TaBEm4ASF189Fms/view). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3WejhKIF1rI"
   },
   "source": [
    "### Q3.1: Tokenization (10 pts): \n",
    "At this step, we need to divide each image into a set of non-overlapping patches, and treat each patch as a token. This is the key step that distinguishes ViT from other computer vision models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yktdHkZrKPVw"
   },
   "source": [
    "#### Q3.1.1 Tokenize_image Method (5pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDPametLF1rJ"
   },
   "outputs": [],
   "source": [
    "def tokenize_image(img, patch_size=16, stride=16):\n",
    "    \"\"\"\n",
    "    Tokenize an image into non-overlapping image patches.\n",
    "    Args:\n",
    "        img (torch.Tensor): The input image with shape (C, H, W).\n",
    "        patch_size (int): The size of each patch.\n",
    "        stride (int): The stride of the sliding window.\n",
    "    Returns:\n",
    "        patches (torch.Tensor): The tokenized patches with shape (N, patch_size*patch_size*C).\n",
    "    \"\"\"\n",
    "    \n",
    "    C, H, W = img.shape\n",
    "    patches = []\n",
    "    ################# Your Implementations #################################\n",
    "    # Hints: write two for loop to loop over this image \n",
    "    # Each patch is flattened into a 1-dimensional vector and stacked into a \n",
    "    # tensor with shape (N, C * patch_size(H) * patch_size(W)), where N is the number of patches. \n",
    "    # We only consider the case image size can be modulo by the patch_size\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    ################# End of your Implementations ##########################\n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVfIbK5tF1rJ"
   },
   "outputs": [],
   "source": [
    "# test your implementation of tokenize_image\n",
    "random_img = torch.rand(3,64,64)\n",
    "patched_img = tokenize_image(random_img,8,8)\n",
    "\n",
    "for i in [32,16,8,4,2]:\n",
    "    out = tokenize_image(random_img,i,i)\n",
    "    \n",
    "    fast_patch = Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = i, p2 = i)\n",
    "    \n",
    "    answer = fast_patch(random_img.unsqueeze(0))\n",
    "    equal = torch.allclose(out,answer.squeeze(0))\n",
    "    #print('Difference: ', equal)\n",
    "    if equal:\n",
    "      print('Good! For patch_size: %d, the output match' %(i))\n",
    "    else:\n",
    "      print('Uh-oh! For patch_size: %d, the output are different' %(i))\n",
    "      break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZo2ygYUKfWC"
   },
   "source": [
    "#### Q3.1.1 linear projection layer (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQcWs0XTIC7P"
   },
   "source": [
    "At this step, you will need to implement the linear projection linear project layer combined with tokenize operation. \n",
    "\n",
    "This layer is used to transfer a single image to the image embedding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZ9tqkaPK1g7"
   },
   "outputs": [],
   "source": [
    "class Tokenization_layer(nn.Module):\n",
    "  def __init__(self, dim, patch_dim,patch_height, patch_width):\n",
    "    super().__init__()\n",
    "    \"\"\"\n",
    "        Args:\n",
    "          dim (int): input and output dimension.\n",
    "          patch_dim(int): falttened vectot dimension for image patch\n",
    "          patch_height (int): height of one image patch\n",
    "          patch_weight (int): weight of one image patch\n",
    "        \n",
    "        You can use Pytorch's built-in function and the above Rearrange method.\n",
    "        Input and output shapes of each layer:\n",
    "        1) Rerrange the image: (batch_size, channels, H,W) -> (batch_size,N,patch_dim)\n",
    "        2) Norm Layer1 (LayerNorm): (batch_size,N,patch_dim) -> (batch_size,N,patch_dim)\n",
    "        3) Linear Projection layer: (batch_size,N,patch_dim) -> (batch_size,N,dim)\n",
    "        4) Norm Layer2 (LayerNorm): (batch_size,N,dim)-> (batch_size,N,dim)\n",
    "    \"\"\"\n",
    "\n",
    "    self.to_patch = Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width)\n",
    "    self.norm1 = None \n",
    "    self.fc1 = None\n",
    "    self.norm2 = None\n",
    "\n",
    "    ################# Your Implementations #################################\n",
    "    # Hints: You can use the Rearrange method above to achieve faster patch operation\n",
    "    # Append this layer to nn.Sequential \n",
    "    # Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width)\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    ################# End of your Implementations ##########################\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      x (torch.Tensor): input tensor in the shape of (batch_size,C,H,W)\n",
    "    Return: \n",
    "      out (torch.Tensor): output patch embedding tensor in the shape of (batch_size,N,dim)\n",
    "\n",
    "     The input tensor 'x' should pass through the following layers:\n",
    "     1) self.to_patch: Rerrange image \n",
    "     2) self.norm1: LayerNorm\n",
    "     3) self.fc1: Fully-Connected layer\n",
    "     4) self.norm2: LayerNorm\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    out = None\n",
    "    ################# Your Implementations #################################\n",
    "    pass\n",
    "\n",
    "    \n",
    "    \n",
    "    ################# End of your Implementations ##########################\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9b7ESVsF1rK"
   },
   "source": [
    "### Q3.2 Attention (15 pts): \n",
    "You will need to follow the steps to implement multi-head attention in this question. \n",
    "1. **Obtain Q,K,V vectors**: To obtain the Q, K, and V vectors, the input vectors are processed through three distinct single linear layers. In our implementation, we use a single linear layer with 3xD output channels, and then we divide the output into three chunks. We consider the first chunk as the Q vectors, the second chunk as the K vectors, and the last chunk as the V vectors. \n",
    "\n",
    "2. **Calculate similarity**: Compute the similarity scores between query vectors and a set of key vectors using a dot product.\n",
    "\n",
    "3. **Apply softmax**: Apply a softmax function to normalize the similarity scores across the key vectors. This creates a probability distribution that represents the relative importance of each key vector with respect to the query vector.\n",
    "\n",
    "4. **Compute weighted sum**: Compute a weighted sum of the** value vectors**, where the weights are the probability distribution obtained in step 2. This produces a context vector that summarizes the most relevant information from the value vectors with respect to the query vector.\n",
    "\n",
    "5. **Concatenate output**: The outputs of each head are then concatenated and passed through another linear projection to produce the final output.\n",
    "\n",
    "For more details, you can read our [textbook](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7er82fMhF1rK"
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dim (int): input and output dimension.\n",
    "          heads (int): number of attention heads.\n",
    "          dim_head (int): input dimension of each attention head.\n",
    "          dropout (float): dropout rate for attention and final_linear layer.\n",
    "\n",
    "        Initialize a attention block.\n",
    "        You can use Pytorch's built-in function.\n",
    "        Input and output shapes of each layer:\n",
    "        1) Define the inner dimension as number of heads* dimension of each head\n",
    "        2) to_qkv: (batch_size, dim) -> (batch_size,3*inner_dimension)\n",
    "        3) final_linear: (batch_size, inner_dim) -> (batch_size, dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.heads = heads\n",
    "        self.dim_head = None\n",
    "        self.to_qkv = None\n",
    "        self.dropout = None\n",
    "        \n",
    "        self.inner_dim = dim_head *  heads    \n",
    "        self.to_qkv = nn.Linear(dim, self.inner_dim * 3, bias = False)\n",
    "        ################# Your Implementations #################################\n",
    "\n",
    "       \n",
    "        pass\n",
    "        \n",
    "        ################# End of your Implementations ##########################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of the attention block.\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor in the shape of (batch_size,N,dim).\n",
    "        Returns:\n",
    "            out (torch.Tensor): output tensor in the shape of (batch_size,N,dim).\n",
    "        \n",
    "        The input tensor 'x' should pass through the following layers:\n",
    "        1) to_qkv: (batch_size,N,dim) -> (batch_size,N,3*inner_dimension)\n",
    "        2) Divide the ouput of to qkv to q,k,v and then divide them in to n heads \n",
    "            (batch_size,N,inner_dim) -> (batch_size,N,num_head,head_dim)\n",
    "        3) Use torch.matmul to get the product of q and k\n",
    "        4) Divide the above tensor by the squre root of head dimension\n",
    "        5) Apply softmax and then dropout on the above tensor\n",
    "        6) Mutiply the above tensor with v to get attention\n",
    "        7) Concatenate the attentions from multi-heads \n",
    "            (batch_size,N,num_head,head_dim) -> (batch_size,N,inner_dim)\n",
    "        8) Pass the output from last step to a fully connected layer \n",
    "        9) Apply dropout for the last step output    \n",
    "        '''\n",
    "        out = None\n",
    "        \n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "        ################# Your Implementations #################################\n",
    "        pass\n",
    "\n",
    "\n",
    "        # Hint you can use :\n",
    "        #    out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        # to concatenate the output from all attention heads\n",
    "        # This operation will change the tensor shape from (batch_size,N,num_head,head_dim)\n",
    "        # to  (batch_size,N,inner_dim)\n",
    "        \n",
    "        \n",
    "        ################# End of your Implementations ##########################\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7yya6zjF1rL"
   },
   "outputs": [],
   "source": [
    "# You can use this cell to check if the output shape of attention'\n",
    "for dim in [512,768,1096]:\n",
    "  test_tensor = torch.rand(2,196,dim)\n",
    "  att_layer = Attention(dim,8,64,0.4)\n",
    "  output_tensor = att_layer(test_tensor)\n",
    "  equal =  test_tensor.shape == output_tensor.shape\n",
    "  if equal:\n",
    "    print('Good! For input dim: %d, the output shape is correct' %(dim))\n",
    "  else:\n",
    "    print('Uh-oh! For input dim: %d, the output shape is wrong' %(dim))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRGAesmOfk0A"
   },
   "source": [
    "The norm layer in Vision Transformer (ViT) is a layer that performs layer normalization on the input. It is typically applied after the Multi-Head Attention (MHA) and the MLP layers in the ViT architecture. The norm layer is used to help the model learn better representations by ensuring that the activations are normalized and centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jFbmcWQCF1rL"
   },
   "outputs": [],
   "source": [
    "### PreNorm function\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        # keey the resiual connection here\n",
    "        return self.fn(self.norm(x), **kwargs)+x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l217s0ofF1rL"
   },
   "outputs": [],
   "source": [
    "#You can use \n",
    "a = PreNorm(768, Attention(768, heads = 8, dim_head = 64, dropout = 0.2))\n",
    "# to create a combination of layer norm and any other layer\n",
    "test_tensor = torch.rand(2,196,768)\n",
    "# you can use the following line to do the forward pass\n",
    "output_tensor = a(test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De7Z8ywcF1rM"
   },
   "source": [
    "###Q3.3 PositionwiseFeedForward(5 pt)\n",
    "You will need to implement the posiotionwiseFeedForward layer in Vision Transformer. \n",
    "\n",
    "The FFN layer is called \"position-wise\" because it applies a separate feedforward network to each position in the sequence independently. It consists of two linear transformations with a non-linear activation function in between, typically GELU. The first linear transformation maps the input feature vector from its original dimension to a higher-dimensional space, and the second linear transformation maps it back to the original dimension. The output of the FFN layer is the element-wise sum of the input and the transformed feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zP9qXq3AF1rM"
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "    def __init__(self, dim, mlp_dim, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        \"\"\"\n",
    "         Args:\n",
    "          dim (int): input and output dimension.\n",
    "          mlp_dim (int): the output dimension of the fist first layer.\n",
    "          dropout (float): dropout rate for both linear layers.\n",
    "\n",
    "        Initialize an MLP.\n",
    "        You can use Pytorch's built-in nn.Linear function.\n",
    "        Input and output sizes of each layer:\n",
    "          1) fc1: dim, mlp_dim\n",
    "          2) fc2: mlp_dim, dim\n",
    "        \"\"\" \n",
    "\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.dropout = None\n",
    "        self.activation = nn.GELU()\n",
    "        ################# Your Implementations #################################\n",
    "          \n",
    "        pass \n",
    "        \n",
    "        ################# End of your Implementations ##########################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor in the shape of (batch_size,N,dim).\n",
    "        Returns:\n",
    "            out (torch.Tensor): output tensor in the shape of (batch_size,N,dim).\n",
    "        \n",
    "        The input tensor 'x' should pass through the following layers:\n",
    "        1) fc1: (batch_size,N,dim) ->  (batch_size,N,mlp_dim)\n",
    "        2) Apply activation function \n",
    "        3) Apply dropout\n",
    "        3) fc2: (batch_size,N,mlp_dim) -> (batch_size,N,dim)\n",
    "        4) Apply dropout\n",
    "        '''\n",
    "        \n",
    "        out = None\n",
    "        ################# Your Implementations #################################\n",
    "        \n",
    "        pass\n",
    "        \n",
    "        ################# End of your Implementations ##########################\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0o1D-3EsF1rM"
   },
   "outputs": [],
   "source": [
    "# You can use this cell to check if the output shape of PositionwiseFeedForward\n",
    "for dim in [512,768,1096]:\n",
    "  test_tensor = torch.rand(2,196,dim)\n",
    "  ffn = PositionwiseFeedForward(dim,dim*4,0.1)\n",
    "  output_tensor = ffn(test_tensor)\n",
    "  equal =  test_tensor.shape == output_tensor.shape\n",
    "  if equal:\n",
    "    print('Good! For input dim: %d, the output shape is correct' %(dim))\n",
    "  else:\n",
    "    print('Uh-oh! For input dim: %d, the output shape is wrong' %(dim))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoRkYfdoF1rM"
   },
   "source": [
    "### Q3.4 TransformerBlock (5pt)\n",
    "Now you can follow the steps and use above class to implement the standard transformer block as demostrated in the following image.\n",
    "\n",
    " <img src=\"https://web.cs.ucla.edu/~smo3/cs188/assignment3/transformer_block.png\"  width=\"20%\" height=\"40%\">\n",
    "\n",
    "1. Apply Layer-norm to the input tensor\n",
    "2. Apply the Multi-Head Attention (MHA) layer to the output tensor from step1. The MHA layer takes in the input tensor, and returns the attention scores and the attention output tensor.\n",
    "3. Add the residual connection to the output of the MHA layer.\n",
    "4. Apply Layer-norm to output of last step\n",
    "5. Apply the Position-wise Feedforward Network (FFN) layer to the output of the previous step. The FFN layer takes in the output tensor, and returns the transformed output tensor.\n",
    "6. Add the residual connection to the output of the FFN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMCLL9IJF1rN"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        \"Implements Transformer block.\"\n",
    "        super().__init__()\n",
    "        '''\n",
    "        Args:\n",
    "          dim (int): input and output dimension.\n",
    "          heads (int): number of attention heads.\n",
    "          dim_head (int): input dimension of each attention head.\n",
    "          mlp_dim (int): \n",
    "          dropout (float): dropout rate for attention and FFN layers.\n",
    "        \n",
    "        '''\n",
    "        # Use the PreNorm,Attention and PositionwiseFeedForword class to build your \n",
    "        # Transformer block\n",
    "        self.attn = None\n",
    "        self.ff = None\n",
    "        \n",
    "        ################# Your Implementations #################################\n",
    "        pass\n",
    "        ################# End of your Implementations ##########################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor in the shape of (batch_size,N,dim).\n",
    "        Returns:\n",
    "            out (torch.Tensor): output tensor in the shape of (batch_size,N,dim).\n",
    "        \"\"\"\n",
    "        ################# Your Implementations #################################\n",
    "        pass\n",
    "        ################# End of your Implementations ##########################\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLAFJkurF1rN"
   },
   "outputs": [],
   "source": [
    "# You can use this cell to check if the output shape of Transformer\n",
    "for dim in [512,768,1096]:\n",
    "  test_tensor = torch.rand(2,196,dim)\n",
    "  transformer_block = Transformer(dim,8,64,dim*4,0.1)\n",
    "  output_tensor = transformer_block(test_tensor)\n",
    "  equal =  test_tensor.shape == output_tensor.shape\n",
    "  if equal:\n",
    "    print('Good! For input dim: %d, the output shape is correct' %(dim))\n",
    "  else:\n",
    "    print('Uh-oh! For input dim: %d, the output shape is wrong' %(dim))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYrPbuC3F1rN"
   },
   "source": [
    "###Q3.5 ViTModel (20 pts)\n",
    "Now you can use above classes to build your Vision Transfromer. Recall the ViT Architecture. \n",
    "\n",
    " <img src=\"https://web.cs.ucla.edu/~smo3/cs188/assignment3/vit.png\"  width=\"40%\" height=\"40%\">\n",
    "\n",
    " Recall the pipline for Vision Transformer model:\n",
    "\n",
    "1. Load the input images and preprocess them into a set of image patches. The patches should be non-overlapping and should cover the entire input image. Each patch should be flattened into a vector and projected into a lower-dimensional/equal-dimensional space using a linear layer.\n",
    "\n",
    "2. Add cls token and learnable positional embeddings to the projected patch vectors. The positional embedding should encode the spatial location of each patch in the input image.\n",
    "\n",
    "3. Stack several Transformer blocks to process the patch vectors. Each Transformer block should consist of a Multi-Head Attention (MHA) layer and a Position-wise Feedforward Network (FFN) layer, with residual connections and layer normalization applied after each layer.\n",
    "\n",
    "3. Apply a mean pooling operation over the output of the last Transformer block or take the output vector related to the cls token to obtain a fixed-size feature vector.\n",
    "\n",
    "5. Feed the feature vector into a fully-connected classification head to predict the class label of the input image.\n",
    "\n",
    "6. Train the model using a supervised learning objective, such as cross-entropy loss, and backpropagation to update the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8h8oMh2vF1rN"
   },
   "outputs": [],
   "source": [
    "# helper method\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    \"Implements Vision Transfromer\"\n",
    "    def __init__(self, *, \n",
    "                 image_size,\n",
    "                 patch_size, \n",
    "                 num_classes, \n",
    "                 dim, \n",
    "                 depth, \n",
    "                 heads, \n",
    "                 mlp_dim, \n",
    "                 pool = 'cls', \n",
    "                 channels = 3, \n",
    "                 dim_head = 64, \n",
    "                 dropout = 0., \n",
    "                 emb_dropout = 0.,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          image_size (int): the height/weight of the input image.\n",
    "          patch_size (int): image patch size. In the ViT paper, this value is 16.\n",
    "          num_classes (num_class): Number of image classes for MLP prediction head.\n",
    "          dim (int): patch and position embedding dimension.\n",
    "          depth (int): number of stacked transformer blocks.\n",
    "          heads (int): number of attention heads.\n",
    "          mlp_dim (int): inner dimension for MLP in transformer blocks.\n",
    "          pool (str): choice between \"cls\" and \"mean\".\n",
    "                      For cls, you will need to use the cls token for perdiction\n",
    "                      For mean, you will need to take the mean of last transformer output \n",
    "          channels (int): Input image channels. Set to 3 for RGB image.\n",
    "          dropout (float): dropout rate for transformer blocks.\n",
    "          emb_dropout (float): dropout rate for patch embedding.\n",
    "        \n",
    "        \"\"\"\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = 0\n",
    "        patch_dim = 0\n",
    "        \n",
    "        ################# Your Implementations #################################\n",
    "        # TODO: Compute the num_patches and patch_dim\n",
    "        \n",
    "        pass\n",
    "        \n",
    "        ################# End of your Implementations ##########################\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "        self.pool = pool\n",
    "\n",
    "        self.to_path_embedding = None\n",
    "\n",
    "        self.pos_embedding = None\n",
    "        self.cls_token = None\n",
    "        self.dropout = None\n",
    "        self.transformers = nn.ModuleList([])\n",
    "        self.mlp_head = None\n",
    "        ################# Your Implementations #################################\n",
    "        # TODO: \n",
    "        # 1) Define self.to_path_embedding usinng the Tokenization_layer class\n",
    "        # 2) Define learnable 1-D pos_embedding using torch.randn, the number of \n",
    "        #    embedding should be num_patches+1\n",
    "        # 3) Define learnable 1-D cls_token with dimension = dim. You can use nn.Parameter \n",
    "        #    to define the learnable \n",
    "        # 4) Define dropout with emb_dropout\n",
    "        # 5) Define depth num of Transformers\n",
    "        # 6) Using nn.Sqeuential to create the MLP head including two layers:\n",
    "        #    The first layer in the MLP head is a LayerNorm layer.\n",
    "        #    The second layer in the MLP head is a linear layer change dimension to num_classes\n",
    "\n",
    "        pass\n",
    "\n",
    "        ################# End of your Implementations ##########################\n",
    "\n",
    "        \n",
    "    def forward(self, img):\n",
    "        '''\n",
    "        Args:\n",
    "            x (torch.Tensor): input tensor in the shape of (batch_size,N,dim).\n",
    "        Returns:\n",
    "            out (torch.Tensor): output tensor in the shape of (batch_size,num_class).\n",
    "        \n",
    "        The input tensor 'x' should pass through the following layers:\n",
    "        1) self.to_patch_embedding: (batch_size,C,H,W) -> (batch_size,N,dim)\n",
    "        2) Using torch.Tensor.repeat to repeat the cls alone batch dimension.\n",
    "           Then, concatenate with cls token (batch_size,N,dim) -> (batch_size,N+1,dim)\n",
    "        3) Take sum of patch embedding and position embedding, then apply dropout. \n",
    "        4) Passing through all the transformer blocks (batch_size,N+1,dim) -> (batch_size,N+1,dim)\n",
    "        5) Use cls token or use pool method to get latent code of batched images \n",
    "            (batch_size,N+1,dim) -> (batch_size,dim)\n",
    "        6) Apply layerNorm to the output of last step\n",
    "        7) Passing though the final mlp layers: (batch_size,dim) -> (batch_size,num_class)\n",
    "        \n",
    "        '''\n",
    "        out = None\n",
    "        ################# Your Implementations #################################\n",
    "        \n",
    "        pass\n",
    "        \n",
    "        ################# End of your Implementations ##########################\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8oNroxoyG1r"
   },
   "source": [
    "Then let's train your ViT model with with cls token as pool policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tbp_mTZJF1rO"
   },
   "outputs": [],
   "source": [
    "seed_everything(0)\n",
    "\n",
    "#Define the model, optimizer, and criterion (loss_fn)\n",
    "model = ViT(image_size = 128,\n",
    "    patch_size = 16,\n",
    "    num_classes = 100,\n",
    "    dim = 192,\n",
    "    depth = 8,\n",
    "    heads = 4,\n",
    "    dim_head = 48,\n",
    "    mlp_dim = 768,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    "           )\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=1e-4,) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Define the dataset and data transform with flatten functions appended\n",
    "data_root = os.path.join(root_dir, 'data')\n",
    "train_dataset = MiniPlaces(\n",
    "    root_dir=data_root, split='train', \n",
    "    transform=data_transform)\n",
    "\n",
    "val_dataset = MiniPlaces(\n",
    "    root_dir=data_root, split='val', \n",
    "    transform=data_transform,\n",
    "    label_dict=train_dataset.label_dict)\n",
    "\n",
    "# Define the batch size and number of workers\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIF3rbTjxRqC"
   },
   "source": [
    "I got an accuracy of 14.43% using my own implementation. How about you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGXNfFX9zEXb"
   },
   "source": [
    "Then let's train your ViT model with with average pooling as pool policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWLFLdbwy3ko"
   },
   "outputs": [],
   "source": [
    "seed_everything(0)\n",
    "\n",
    "#Define the model, optimizer, and criterion (loss_fn)\n",
    "model = ViT(image_size = 128,\n",
    "    patch_size = 16,\n",
    "    num_classes = 100,\n",
    "    dim = 192,\n",
    "    depth = 8,\n",
    "    heads = 4,\n",
    "    pool = 'mean',\n",
    "    dim_head = 48,\n",
    "    mlp_dim = 768,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    "           )\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=1e-4,) \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Define the dataset and data transform with flatten functions appended\n",
    "data_root = os.path.join(root_dir, 'data')\n",
    "train_dataset = MiniPlaces(\n",
    "    root_dir=data_root, split='train', \n",
    "    transform=data_transform)\n",
    "\n",
    "val_dataset = MiniPlaces(\n",
    "    root_dir=data_root, split='val', \n",
    "    transform=data_transform,\n",
    "    label_dict=train_dataset.label_dict)\n",
    "\n",
    "# Define the batch size and number of workers\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIsqefHhZLUj"
   },
   "source": [
    "I got an accuracy of 14.11% using my own implementation. How about you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRYdlXK51eE6"
   },
   "source": [
    "### Q3.6 Comparision bewteen ResNet and ViT (5pts)\n",
    "Train your resnet18 model without pretrained weighted for 2 epoch, and comparing the accuracy with ViT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzBbgHQO2DF7"
   },
   "outputs": [],
   "source": [
    "# Train your resnet18 model without pretrained weighted for 2 epoch, and comparing the accuracy with ViT model.\n",
    "seed_everything(0)\n",
    "\n",
    "# Define the model, optimizer, and criterion (loss_fn)\n",
    "resnet = Resnet(pretrained=False)\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    resnet.parameters(), \n",
    "    lr=0.0001)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Define the dataset and data transform with flatten functions appended\n",
    "data_root = os.path.join(root_dir, 'data')\n",
    "train_dataset = MiniPlaces(\n",
    "    root_dir=data_root, split='train', \n",
    "    transform=data_transform)\n",
    "\n",
    "val_dataset = MiniPlaces(\n",
    "    root_dir=data_root, split='val', \n",
    "    transform=data_transform,\n",
    "    label_dict=train_dataset.label_dict)\n",
    "\n",
    "# Define the batch size and number of workers\n",
    "batch_size = 64\n",
    "num_workers = 2\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "\n",
    "# Train the model\n",
    "train(resnet, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmYxmnjA2WCv"
   },
   "source": [
    "Use 2-3 sentence, explain the why there is a performance gap between ResNet and ViT when they are trained with a short time. \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "X42ej8ZhF1q4"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
