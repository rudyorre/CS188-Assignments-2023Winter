{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDiXl4ay5UM8"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "Welcome to the first assignment of CS188 Computer Vision course! \n",
        "\n",
        "First of all, please type your information below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIeH92UsBmbO"
      },
      "outputs": [],
      "source": [
        "#@title Your Info { display-mode: \"form\" }\n",
        "\n",
        "Name = 'Jiawei Yang' #@param {type:\"string\"}\n",
        "UID = '401234567' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y9NzXD89qUJ"
      },
      "source": [
        "### Goals\n",
        "In the first assignment, you will learn:\n",
        "* How to prepare data for image classification\n",
        "* How to use Google Colab\n",
        "* How to implement a k-Nearest Neighbor (kNN) classifier\n",
        "* How to implement a linear/logistic/softmax regression classifier\n",
        "\n",
        "Free free to raise issues in the Piazza forum if you find any bugs or have any questions about the assignment. \n",
        "\n",
        "Please do not directly copy code from other sources.\n",
        "\n",
        "This assignment is due on **Sunday, Jan 29**. Good luck with your work!\n",
        "\n",
        "❗**Once you have finished all the questions, it will still take approximately *30 minutes* to re-run the entire notebook in order to prepare the submission version. Make sure to allocate enough time for this task and start early.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U62cf5KB5Xbp"
      },
      "source": [
        "## Getting Started\n",
        "\n",
        "\n",
        "### Overview\n",
        "\n",
        "In this assignment, you will be working with the [MiniPlaces dataset](https://github.com/CSAILVision/miniplaces), a small-scale version of the [Places2 dataset](http://places2.csail.mit.edu/), which is a large-scale dataset of scene images (10+ million images) with a wide variety of real-world environments (400+ unique scene categories). The MiniPlaces dataset is a subset of the [Places2 dataset](http://places2.csail.mit.edu/) and contains 100,000 images for training, 10,000 images for validation, and 10,000 images for testing, each of which has been annotated with one of 100 different scene categories. These images are divided into three folders: train, val, and test. \n",
        "\n",
        "The MiniPlaces dataset is a useful resource for developing and testing image classification models, particularly those that are designed to recognize different types of environments and scenes.\n",
        "\n",
        "Your task for this assignment is to create two new datasets called *TinyPlaces-Binary* and *TinyPlaces-Multiclass* from the MiniPlaces dataset. Both datasets should be much smaller in size than the original MiniPlaces dataset (more details later).\n",
        "\n",
        "In addition to creating the TinyPlaces datasets, you will also need to implement and evaluate the performance of the following classifiers on both datasets:\n",
        "* k-Nearest Neighbor (kNN) Classifier\n",
        "* Linear/logistic/sofrmax Regression Classifier\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SIGjL2GBmOw"
      },
      "source": [
        "##Q0: Understanding the storage of colab (0 pts)\n",
        "\n",
        "Colab is a cloud-based service that allows users to write and run code in a Jupyter notebook-style enviroment. Colab provides users with temporary virtual machines to run their code on. \n",
        "\n",
        "For file storage, Colab provides users with access to a variety of storage options, including temporary storage space and Google Drive storage.\n",
        "\n",
        "1. Temporary storage space, also known as \"local runtime storage,\" is a space on the virtual machine (VM) that is allocated to you when you open a Colab notebook. This space is temporary in the sense that it is wiped clean whenever you close the notebook or disconnect from the VM. However, it is much faster than Google Drive storage, since it is located on the same machine as the VM. This makes it ideal for storing and accessing large datasets or intermediate data that is being used frequently in your notebook.\n",
        "\n",
        "2. Google Drive storage is a more permanent storage option that is accessed through your Google account. It is ***much much*** slower than temporary storage space, since it requires data to be transferred over the internet to and from the VM. However, it is useful for storing and sharing notebooks and data that you want to keep long-term, or for collaborating with other users.\n",
        "\n",
        "As we are going to be working with a lot of images, and to make things run smoothly, we will be using Colab's temporary storage space to store our data. This means that every time you open up this notebook, we will need to re-download and process the dataset. Don't worry though - this shouldn't take long, usually just a minute or less. Okay, let's get started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol_RX3yIBi3w"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Let's make our assignment directory\n",
        "CS188_path = './'\n",
        "os.makedirs(os.path.join(CS188_path, 'Assignment1', 'data'), exist_ok=True)\n",
        "# os.makedirs will create directories recursively, i.e., it will create the \n",
        "# directories and any missing parent directories if they do not exist.\n",
        "\n",
        "# Now, let's specify the assignment path we will be working with as the root.\n",
        "root_dir = os.path.join(CS188_path, 'Assignment1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HVbkXsS5hiV"
      },
      "source": [
        "## Q1: Data preparation (25 pts)\n",
        "For Q1, you will need to create the *TinyPlaces-Binary* and *TinyPlaces-Multiclass* datasets. \n",
        "\n",
        "First, you will be creating two datasets - `tinyplaces_binary_train` and `tinyplaces_binary_val` - from the MiniPlaces dataset (https://github.com/CSAILVision/miniplaces). You will also be creating two additional datasets - `tinyplaces_multi_train` and `tinyplaces_multi_val` - that are similar to the first two datasets but with 20 categories instead of just 2.\n",
        "\n",
        "To do this, you should follow these steps:\n",
        "1. Follow the code cells below to download the MiniPlaces dataset from this link: https://drive.google.com/file/d/16GYHdSWS3iMYwMPv5FpeDZN2rH7PR0F2/view, and extract files using python. Don't worry, we have provided commands to do this.\n",
        "\n",
        "2. Use the file categories_tinyplaces.txt to select images from the target subcategories in the MiniPlaces dataset.\n",
        "\n",
        "3. Kindly follow the hints in the following codeblocks to implement different functions to finish this task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND3-KKZK8L0Q"
      },
      "source": [
        "### Q1.1: download and extract dataset (0 pts)\n",
        "We will use this sub-question as an example to show you how to download files in Colab in an effortless way.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhHE9G_pLKyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b28596-265a-4c0b-ee57-8d4daefb01d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=16GYHdSWS3iMYwMPv5FpeDZN2rH7PR0F2\n",
            "To: /content/data.tar.gz\n",
            "100% 460M/460M [00:07<00:00, 58.0MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Downloading this file takes about a few seconds.\n",
        "# Download the tar.gz file from google drive using its file ID.\n",
        "!pip3 install --upgrade gdown --quiet\n",
        "!gdown 16GYHdSWS3iMYwMPv5FpeDZN2rH7PR0F2 # this is the file ID of miniplaces dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkbRrFxRRRYE"
      },
      "source": [
        "This code block shows you how to extract files from the downloaded \"data.tar.gz\" zip file. \n",
        "\n",
        "It will takes more than 30 minutes to finish if you choose to extract this file in your google driver. However, it should take only about 10 seconds if you extract it in the colab temporary directory (our choice)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dun0w8Fm8K-q"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Open the tar.gz file\n",
        "tar = tarfile.open(\"data.tar.gz\", \"r:gz\")\n",
        "# Extract the file to the \"/content/drive/MyDrive/CS188-2023/Assignment1/data\" folder\n",
        "total_size = sum(f.size for f in tar.getmembers())\n",
        "with tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Extracting tar.gz file\") as pbar:\n",
        "    for member in tar.getmembers():\n",
        "        tar.extract(member, os.path.join(root_dir, 'data'))\n",
        "        pbar.update(member.size)\n",
        "# Close the tar.gz file\n",
        "tar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMPC-gGmReDu"
      },
      "source": [
        "If you are curious, however, here is an example output if you choose to extract the files to Google Drive:\n",
        "\n",
        "```\n",
        "Extracting tar.gz file: 100%|██████████| 566M/566M [21:49<00:00, 432kB/s] \n",
        "```\n",
        "\n",
        "In contrast, extracting files to temporary workspace only takes ~20-40 seconds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kEiCtvC5MeK"
      },
      "source": [
        "### Q1.2: Create subsets (15 pts)\n",
        "\n",
        "In this question, you will create the TinyPlaces datasets for both binary and multiclass classification tasks.\n",
        "\n",
        "Follow these steps to complete this question:\n",
        "1. Download `categories_tinyplaces.txt` from [this link](https://raw.githubusercontent.com/UCLAdeepvision/CS188-Assignments-2022Winter/master/Assignment1-Release/categories_tinyplaces.txt) and `val.txt` from [this link](https://github.com/CSAILVision/miniplaces/tree/master/data/val.txt.). You can download it directly to `/content/MyDrive/2023-CS188/Assignment1/data` in Colab. This file is also provided in the assignment repository on Github and can be directly uploaded.\n",
        "\n",
        "2. Use the file `categories_tinyplaces.txt` to select images from the target subcategories in the MiniPlaces dataset.\n",
        "  1. Load the target subcategories and their associated class IDs from the file `categories_tinyplaces.txt`. This file contains a list of subcategory names, in the format of directory paths, and class IDs, with one pair per line. For example, a line of `/b/bathroom 16` indicates a subcategory name \"bathroom\" with a class ID of \"16\". The directory of \"bathroom\" images is located at \"root/data/images/SPLIT/b/bathroom\", where \"SPLIT\" is either train, val, or test, and 'root' is specified as `/content/MyDrive/2023-CS188/Assignment1`. There are a total of 20 subcategories in this file. The first 10 subcategories are indoor scenes, and the last 10 are outdoor scenes.\n",
        "  2. Iterate through the images in the train and val folders and select only those that belong to one of the target subcategories in `categories_tinyplaces.txt`. For training data, take the **first** 500 images (`00000001.jpg` to `00000500.jpg`) of each subcategory in the root/data/images/train folder (500x20=10k images in total). For validation data, take the first 50 images of each subcategory in `val.txt`. The TinyPlaces-val dataset should contain a total of 1,000 images (50x20=1k images in total).\n",
        "  3. Resize all images to 32x32 and flatten them to create CIFAR-like 3072-dimensional vectors (3x32x32=3072).\n",
        "\n",
        "3. Assign labels to create the TinyPlaces datasets.\n",
        "  1. `tinyplaces_binary_train`: the training set for the binary classification task. Assign the images in the 10 indoor subcategories the label \"0\" and the images in the 10 outdoor categories the label \"1\".\n",
        "  2. `tinyplaces_binary_val`: the validation set for the binary classification task.\n",
        "  3. `tinyplaces_multi_train`: the training set for the multiclass classification task. Assign each subcategory a class ID (from 0 to 19) in the same order as `categories_tinyplaces.txt`. For example, the first class \"bathroom\" should have a class ID of \"0\", and the last class \"rainforest\" should have a class ID of \"19\".\n",
        "  4. `tinyplaces_multi_val`: the validation set for the multiclass classification task.\n",
        "\n",
        "4. Dump the generated datasets to `root/data/{dataset_name}.pkl` using the pickle package. You should have four pickle files: `tinyplaces_binary_train.pkl`, `tinyplaces_binary_val.pkl`, `tinyplaces_multi_train.pkl`, and `tinyplaces_multi_val.pkl`. Each pickle file should have two keys: \"data\" for a 10,000x3072 array and \"label\" for a 10,000x1 array or list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K_TRhZy8d0A"
      },
      "outputs": [],
      "source": [
        "# 1. Download categories_tinyplaces.txt to root/dataimport urllib.request\n",
        "import urllib.request\n",
        "def download_files():\n",
        "    \"\"\"\n",
        "    Downloads the required files from the given links and saves them to the \n",
        "      specified directory.\n",
        "    \"\"\"\n",
        "    # Download categories_tinyplaces.txt and save it to the data directory\n",
        "    categories_tinyplaces_url = \\\n",
        "      'https://raw.githubusercontent.com/UCLAdeepvision/CS188-Assignments-2023Winter/main/Assignment1/categories_tinyplaces.txt'\n",
        "    urllib.request.urlretrieve(categories_tinyplaces_url, \n",
        "                               f'{root_dir}/data/categories_tinyplaces.txt')\n",
        "\n",
        "    # Download val.txt and save it to the data directory\n",
        "    val_url = 'https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/val.txt'\n",
        "    urllib.request.urlretrieve(val_url, f'{root_dir}/data/val.txt')\n",
        "download_files()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJE9Wpx3gY5t"
      },
      "outputs": [],
      "source": [
        "# 2. Load the target subcategories and their associated class IDs.\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_subcategories(txt_path):\n",
        "    \"\"\"\n",
        "    Loads the target subcategories and their class IDs from the specified file.\n",
        "    Map the class IDs to sequential integers from 0 to 19 according to their order.\n",
        "\n",
        "    Parameters:\n",
        "        txt_path (str): The filepath of the categories_tinyplaces.txt file.\n",
        "        \n",
        "    Returns:\n",
        "        subcategories (dict): A dictionary mapping subcategory names to both \n",
        "          original class IDs and modified IDS.\n",
        "    Example output: \n",
        "      {'bathroom': {'ori_class_id': 16, 'class_id': 0}, \n",
        "       'bedroom': {'ori_class_id': 18, 'class_id': 1},\n",
        "       ...\n",
        "      }\n",
        "    \"\"\"\n",
        "    # TODO: Implement the function by following the steps below:\n",
        "    # 1. Open the categories_tinyplaces.txt file and read its contents into the 'lines' variable\n",
        "    # 2. Iterate through the lines in the file and split each line by space \n",
        "    #    character to separate the subcategory name and class ID. Note that you\n",
        "    #    need to further filter out the prefix in the txt file for category names.\n",
        "    # 3. Store the subcategory and its class ID in the dictionary, mapping the \n",
        "    #    class ID to a sequential integer from 0 to 19 according to its order.\n",
        "    # 4. Return the dictionary of subcategories\n",
        "\n",
        "    # Initialize an empty dictionary to store the subcategories\n",
        "    subcategories = {}\n",
        "    ################# Your Implementations #####################################\n",
        "    \n",
        "\n",
        "    ################# End of your Implementations ##############################\n",
        "    return subcategories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taKkUdakmVyI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c299e1af-0c85-4ca2-ff6a-943175625f92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n"
          ]
        }
      ],
      "source": [
        "# try to debug using this code segment\n",
        "# Specify file path\n",
        "txt_path = os.path.join(root_dir, 'data', 'categories_tinyplaces.txt')\n",
        "# Load the target subcategories and their class IDs\n",
        "subcategories = load_subcategories(txt_path)\n",
        "print(subcategories)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7VFoAsTnO3d"
      },
      "source": [
        "You should further parse the directory name to filter out the clas name. The output should be like:\n",
        "\n",
        "\n",
        "```\n",
        "{'bathroom': {'ori_class_id': 16, 'class_id': 0}, 'bedroom': {'ori_class_id': 18, 'class_id': 1}, 'bookstore': {'ori_class_id': 20, 'class_id': 2}, 'classroom': {'ori_class_id': 33, 'class_id': 3}, 'dining_room': {'ori_class_id': 45, 'class_id': 4}, 'food_court': {'ori_class_id': 48, 'class_id': 5}, 'kitchen': {'ori_class_id': 59, 'class_id': 6}, 'lobby': {'ori_class_id': 63, 'class_id': 7}, 'living_room': {'ori_class_id': 62, 'class_id': 8}, 'office': {'ori_class_id': 70, 'class_id': 9}, 'baseball_field': {'ori_class_id': 15, 'class_id': 10}, 'bridge': {'ori_class_id': 24, 'class_id': 11}, 'campsite': {'ori_class_id': 27, 'class_id': 12}, 'canyon': {'ori_class_id': 29, 'class_id': 13}, 'coast': {'ori_class_id': 35, 'class_id': 14}, 'fountain': {'ori_class_id': 49, 'class_id': 15}, 'highway': {'ori_class_id': 53, 'class_id': 16}, 'playground': {'ori_class_id': 74, 'class_id': 17}, 'mountain': {'ori_class_id': 68, 'class_id': 18}, 'rainforest': {'ori_class_id': 77, 'class_id': 19}}\n",
        "\n",
        " ```\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lc3fG7OmRKY"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange\n",
        "import cv2\n",
        "\n",
        "def select_samples(subcategories, root_dir, split, n_images_per_subcategory):\n",
        "    \"\"\"\n",
        "    Selects a subset of images from the specified folder based on the class IDs in the txt file.\n",
        "    Resizes the selected images to 32x32 and flattens them to create CIFAR-like 3072-d vectors.\n",
        "    \n",
        "    Parameters:\n",
        "        subcategories (dict): A dictionary mapping subcategory names to class IDs.\n",
        "        root_dir (str): The root directory of the MiniPlaces dataset.\n",
        "        split (str): The split (train or val) of the MiniPlaces dataset to select images from.\n",
        "        n_images_per_subcategory (int): The number of images to select from each subcategory.\n",
        "        \n",
        "    Returns:\n",
        "        samples (list): A list of tuples, where each tuple contains the image \n",
        "          data and class ID of an image.\n",
        "    \"\"\"\n",
        "    # TODO: Implement this function.\n",
        "    # Hints: \n",
        "    # 1. If the split is 'val', load the filepaths and class IDs from the 'val.txt' \n",
        "    #    file and store them in the 'val_data' list\n",
        "    # Note that val data and train data have different formats so pay attention.\n",
        "    # 2. Iterate through selected images\n",
        "    # 3. Load the image and resize it to 32x32\n",
        "    # 4. Construct a sample_pair as a tuple of the image data and class ID.\n",
        "    # 5. Add the sample_pair to the list\n",
        "    # 6. Return the list of samples\n",
        "\n",
        "    # Initialize an empty list to store the selected samples (images, labels)\n",
        "    samples = []\n",
        "    ################# Your Implementations #####################################\n",
        "    \n",
        "    ################# End of your Implementations ##############################\n",
        "    return samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igwHkTvGntzs"
      },
      "outputs": [],
      "source": [
        "# Try to debug using this code segment:\n",
        "tmp_train_samples = select_samples(subcategories, root_dir, 'train', 50) \n",
        "tmp_val_samples = select_samples(subcategories, root_dir, 'val', 5) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9JSWliRmQq_"
      },
      "outputs": [],
      "source": [
        "def create_tinyplaces(samples, binary=True):\n",
        "    \"\"\"\n",
        "    Creates a TinyPlaces dataset from the specified images.\n",
        "    \n",
        "    Parameters:\n",
        "        samples (list): A list of tuples, where each tuple contains the image data\n",
        "          and class ID of an image.\n",
        "        binary (bool, optional): If True, the dataset is for binary classification. \n",
        "          If False, the dataset is for multiclass classification.\n",
        "        \n",
        "    Returns:\n",
        "        dataset (dict): A dictionary with two keys: \"data\" and \"label\". The \"data\" \n",
        "          key contains a numpy array of image data, and the \"label\" key contains \n",
        "          a numpy array of class labels.\n",
        "     Example output shape: \n",
        "      {'data': (N, 3072), \n",
        "       'labels': (N,), # 0, 1 for binary, and 0 to 19 for multiclass\n",
        "      }\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize empty lists to store the image data and labels\n",
        "    data, labels = [], []\n",
        "    ################# Your Implementations #####################################\n",
        "    \n",
        "        \n",
        "    ################# End of your Implementations ##############################\n",
        "    \n",
        "    # Create the dataset dictionary\n",
        "    dataset = {\"data\": data, \"label\": labels}\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOXJIasPsriB"
      },
      "outputs": [],
      "source": [
        "# Try to debug using this code segment:\n",
        "tmp_binary_train = create_tinyplaces(tmp_train_samples, binary=True)\n",
        "tmp_multi_train = create_tinyplaces(tmp_train_samples, binary=False)\n",
        "\n",
        "print('binary:')\n",
        "print(tmp_binary_train['data'].shape, tmp_binary_train['label'].shape)\n",
        "print('unique class ids:', np.unique(tmp_binary_train['label']))\n",
        "\n",
        "print('multiclass:')\n",
        "print(tmp_multi_train['data'].shape, tmp_multi_train['label'].shape)\n",
        "print('unique class ids:', np.unique(tmp_multi_train['label']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOvNbuZRvGbk"
      },
      "source": [
        "The output should be like:\n",
        "```\n",
        "binary:\n",
        "(1000, 3072) (1000,)\n",
        "unique class ids: [0 1]\n",
        "multiclass:\n",
        "(1000, 3072) (1000,)\n",
        "unique class ids: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PjystwSvTfa"
      },
      "source": [
        "#### About pickle\n",
        "\n",
        "To dump a dictionary as a pickle file, follow this example:\n",
        "\n",
        "```\n",
        "import pickle\n",
        "\n",
        "# Create a dictionary to be dumped\n",
        "data = {'key1': [1, 2, 3], 'key2': 'string value'}\n",
        "\n",
        "# Open a file in binary mode\n",
        "with open('dict.pkl', 'wb') as f:\n",
        "  # Use pickle.dump to write the dictionary to the file\n",
        "  pickle.dump(data, f)\n",
        "```\n",
        "\n",
        "To load a dictionary from a pickle file, here is an example:\n",
        "```\n",
        "# Open the file in binary mode\n",
        "with open('dict.pkl', 'rb') as f:\n",
        "  # Use pickle.load to read the data from the file\n",
        "  data = pickle.load(f)\n",
        "\n",
        "# Print the loaded data\n",
        "print(data)  # Output: {'key1': [1, 2, 3], 'key2': 'string value'}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuT-gcPQvx57"
      },
      "source": [
        "Now, put everything together and make the required four datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jRV-V0zl96D"
      },
      "outputs": [],
      "source": [
        "# Set the root directory of the MiniPlaces dataset\n",
        "root_dir = './Assignment1'\n",
        "\n",
        "# Load the target subcategories and their class IDs\n",
        "subcategories = load_subcategories(os.path.join(root_dir, 'data', 'categories_tinyplaces.txt'))\n",
        "\n",
        "# Select the samples from the train split of the MiniPlaces dataset\n",
        "train_samples = select_samples(subcategories, root_dir, 'train', 500)\n",
        "\n",
        "# Create the TinyPlaces datasets for binary and multiclass classification\n",
        "tinyplaces_binary_train = create_tinyplaces(train_samples, binary=True)\n",
        "tinyplaces_multi_train = create_tinyplaces(train_samples, binary=False)\n",
        "\n",
        "# Select the samples from the val split of the MiniPlaces dataset\n",
        "val_samples = select_samples(subcategories, root_dir, 'val', 50)\n",
        "\n",
        "# Create the TinyPlaces datasets for binary and multiclass classification\n",
        "tinyplaces_binary_val = create_tinyplaces(val_samples, binary=True)\n",
        "tinyplaces_multi_val = create_tinyplaces(val_samples, binary=False)\n",
        "\n",
        "# Save the TinyPlaces datasets to the data directory\n",
        "data_dir = os.path.join(root_dir, 'data')\n",
        "\n",
        "with open(os.path.join(data_dir, 'tinyplaces_binary_train.pkl'), 'wb') as f:\n",
        "    pickle.dump(tinyplaces_binary_train, f)\n",
        "    \n",
        "with open(os.path.join(data_dir, 'tinyplaces_multi_train.pkl'), 'wb') as f:\n",
        "    pickle.dump(tinyplaces_multi_train, f)\n",
        "    \n",
        "with open(os.path.join(data_dir, 'tinyplaces_binary_val.pkl'), 'wb') as f:\n",
        "    pickle.dump(tinyplaces_binary_val, f)\n",
        "    \n",
        "with open(os.path.join(data_dir, 'tinyplaces_multi_val.pkl'), 'wb') as f:\n",
        "    pickle.dump(tinyplaces_multi_val, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S29_1u0xCWOJ"
      },
      "source": [
        "### Q1.3 Visualization (5pts)\n",
        "\n",
        "In this part, you will implement functions to visualize random samples from the created binary dataset and multiclass dataset.\n",
        "\n",
        "Here is a provided function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8798Db21Lciq"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_binary_samples(data_dict, samples_per_class, seed=None):\n",
        "    \"\"\"\n",
        "    Visualizes a random selection of samples from the TinyPlaces-binary dataset.\n",
        "    \n",
        "    Parameters:\n",
        "        data_dict (dict): A dictionary contains images and labels.\n",
        "        samples_per_class (int): The number of samples to display for each class.\n",
        "        \n",
        "    Returns:\n",
        "        None. Displays a grid of images.\n",
        "    \"\"\"\n",
        "    # if seeded.\n",
        "    np.random.seed(seed)\n",
        "    # Initialize a list to store the selected samples\n",
        "    selected_samples = []\n",
        "\n",
        "    # Initialize the figure\n",
        "    n_classes = 2\n",
        "    fig, axs = plt.subplots(n_classes, samples_per_class, \n",
        "                            figsize=(samples_per_class*1.5, n_classes*1.5))\n",
        "    axs = axs.flatten()\n",
        "    # Iterate through the classes\n",
        "    for y, cls in enumerate(['indoor', 'outdoor']):\n",
        "        # Find the indices of samples belonging to the current class\n",
        "        idxs = np.arange(len(data_dict['label']))[data_dict['label'] == y]\n",
        "        # Select a random subset of samples from the current class\n",
        "        idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
        "        for i, idx in enumerate(idxs):\n",
        "            axis = axs[i + y * samples_per_class]\n",
        "            if i == 0:\n",
        "              axis.set_title(cls, loc='left')\n",
        "            image = data_dict['data'][idx].reshape(32, 32, 3)\n",
        "            axis.imshow(image)\n",
        "            axis.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pQuIV8F-NWll"
      },
      "outputs": [],
      "source": [
        "visualize_binary_samples(tinyplaces_binary_train, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvUoGJVuWuGp"
      },
      "source": [
        "Now, modify the `visualize_binary_samples` function to create a new function `visualize_multiclass_samples` to support visualizing multiclass samples. The `subcategories` dict should be also provided as input to retrieve the class names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJB03m5hWtOC"
      },
      "outputs": [],
      "source": [
        "def visualize_multiclass_samples(data_dict, subcategories, samples_per_class, seed=None):\n",
        "    \"\"\"\n",
        "    Visualizes a random selection of samples from the TinyPlaces-multiclass dataset.\n",
        "    \n",
        "    Parameters:\n",
        "        data_dict (dict): A dictionary contains images and labels.\n",
        "        subcategories (dict): A dictionary mapping subcategory names to class IDs.\n",
        "        samples_per_class (int): The number of samples to display for each class.\n",
        "        \n",
        "    Returns:\n",
        "        None. Displays a grid of images.\n",
        "    \"\"\"\n",
        "    # if seeded.\n",
        "    np.random.seed(seed)\n",
        "    ################# Your Implementations #####################################\n",
        "    \n",
        "    ################# End of your Implementations ##############################\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4g1b4CgVk_JE"
      },
      "outputs": [],
      "source": [
        "visualize_multiclass_samples(tinyplaces_multi_train, subcategories, 5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_multiclass_samples(tinyplaces_multi_val, subcategories, 5)"
      ],
      "metadata": {
        "id": "GWUF3LfkscRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtnWxob2YCst"
      },
      "source": [
        "### Q1.4 Data Class (5pts)\n",
        "\n",
        "When implementing machine learning algorithms, it's usually a good idea to use a small subset of the full dataset. In this way, your code will run much faster, allowing for more interactive and efficient development and debugging. Once you are satisfied that you have correctly implemented the algorithm, you can then rerun with the entire dataset.\n",
        "\n",
        "Our next task is to implement a subsampling function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34M1D4IfYCYp"
      },
      "outputs": [],
      "source": [
        "class TinyPlacesDataset(object):\n",
        "    def __init__(self, data_dict):\n",
        "        self.dataset = data_dict\n",
        "        self.num_samples = len(data_dict['data'])\n",
        "\n",
        "    def subsample(self, ratio=0.1, seed=None):\n",
        "        \"\"\"\n",
        "        Subsamples the TinyPlaces dataset.\n",
        "        \n",
        "        Parameters:\n",
        "            ratio (float, optional): The ratio of the subsampled dataset to the original dataset.\n",
        "              Default is 0.1.\n",
        "        \n",
        "        Returns:\n",
        "            subsampled_dataset (TinyPlacesDataset): A new TinyPlacesDataset object\n",
        "              containing the subsampled data.\n",
        "        \"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        ################# Your Implementations ################################\n",
        "        # Randomly select a subset of the data\n",
        "        \n",
        "        \n",
        "\n",
        "        # Create a new TinyPlacesDataset object using the subsampled data\n",
        "\n",
        "        # Return the new TinyPlacesDataset object\n",
        "        ################# End of your Implementations ##########################\n",
        "        return subsampled_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aum0M2TqwHmm"
      },
      "outputs": [],
      "source": [
        "# Try to debug using this code segment\n",
        "# seed your results\n",
        "np.random.seed(0)\n",
        "\n",
        "tinyplaces_binary_train = TinyPlacesDataset(tinyplaces_binary_train)\n",
        "tinyplace_binary_subsampled = tinyplaces_binary_train.subsample(0.1)\n",
        "print('before:')\n",
        "print('num_samples:', tinyplaces_binary_train.num_samples)\n",
        "print('after:')\n",
        "print('num_samples:', tinyplace_binary_subsampled.num_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1.5: Construct a complete dataset class (optional bonus 5 pts)\n",
        "\n",
        "Construct a complete dataset class that can builds TinyPlacesDataset directly from `root_dir`, `split`, `n_images_per_class` and `binary` arguments using the functions we have implemented above. Include the subsample and visualization codes in this class as well."
      ],
      "metadata": {
        "id": "1Q5266O7x0jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################# Your Implementations #####################################\n",
        "class TinyPlacesDatasetV2(object):\n",
        "    def __init__(self, root_dir, split, n_images_per_class, binary=True):\n",
        "      \"\"\"\n",
        "      Initializes a TinyPlaces dataset from the specified root directory, split, \n",
        "        and number of images per class.\n",
        "      Parameters:\n",
        "          root_dir (str): The root directory of the MiniPlaces dataset.\n",
        "          split (str): The split to select images from ('train' or 'val').\n",
        "          n_images_per_class (int): The number of images to select per class.\n",
        "          binary (bool, optional): If True, the dataset is for binary classification. \n",
        "            If False, the dataset is for multiclass classification.\n",
        "      \n",
        "      Returns:\n",
        "          None.\n",
        "      \"\"\"\n",
        "    def subsample(self, ratio=0.1):\n",
        "      \"\"\"\n",
        "      Subsamples the TinyPlaces dataset.\n",
        "      \n",
        "      Parameters:\n",
        "          ratio (float, optional): The ratio of the subsampled dataset to the original dataset.\n",
        "            Default is 0.1.\n",
        "      \n",
        "      Returns:\n",
        "          subsampled_dataset (TinyPlacesDataset): A new TinyPlacesDataset object\n",
        "             containing the subsampled data.\n",
        "      \"\"\"\n",
        "      pass\n",
        "    \n",
        "    def visualize_samples(self, samples_per_class, seed=None):\n",
        "      \"\"\"\n",
        "      Visualizes a random selection of samples from the TinyPlaces dataset.\n",
        "      \n",
        "      Parameters:\n",
        "          samples_per_class (int): The number of samples to display for each class.\n",
        "          seed (int, optional): The seed for the random number generator. Default is None.\n",
        "      \n",
        "      Returns:\n",
        "          None. Displays a grid of images.\n",
        "        \n",
        "      Hints: use self.binary information.\n",
        "      \"\"\"\n",
        "      pass\n",
        "\n",
        "################# End of your Implementations ##############################\n"
      ],
      "metadata": {
        "id": "2FLhiH-h4cSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can delete this code cell if you skip this question.\n",
        "\n",
        "# Create a TinyPlaces-binary dataset\n",
        "tinyplaces_train_b = TinyPlacesDatasetV2(root_dir, 'train', 500, binary=True)\n",
        "tinyplaces_val_b = TinyPlacesDatasetV2(root_dir, 'val', 50, binary=True)\n",
        "# Create a TinyPlaces-multiclass dataset\n",
        "tinyplaces_train_m = TinyPlacesDatasetV2(root_dir, 'train', 500, binary=False)\n",
        "tinyplaces_val_m = TinyPlacesDatasetV2(root_dir, 'val', 50, binary=False)"
      ],
      "metadata": {
        "id": "VJVMkC931163"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can delete this code cell if you skip this question.\n",
        "tinyplaces_train_b.visualize_samples(5)\n",
        "tinyplaces_train_m.visualize_samples(5)"
      ],
      "metadata": {
        "id": "1vgDgPto3E6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2: Regression (40 pts)\n",
        "\n",
        "For this question, you will be implementing and evaluating the performance of linear and logistic regression classifiers on the TinyPlaces-Binary dataset and softmax regression classifier on the TinyPlaces-Multiclass dataset.\n",
        "\n",
        "Recall some details:\n",
        " 1. Linear Regression models the relationship between an input and an output by fitting a linear function to the data. In this assignemnt, we will use the Mean Saquare Error (MSE) loss function to minimize the weights and biases of the linear function.\n",
        " 2. Logistic Regression is commonly used for binary classification tasks. It is an extension of linear regression that adds a logistic function to the linear output. We will use the Cross-Entropy Loss to optimize the logistic regression model.\n",
        " 3. Softmax regression is usually used for multiclass classification tasks. It is an extension of linear regression that adds a softmax function to the linear output and uses the Negative Log Likelihood (NLL) loss to optimize the model.\n",
        "\n",
        "\n",
        "\n",
        "The goal of this exercise is to go through a simple example of the data-driven image classification pipeline, and also to practice writing efficient, vectorized code in PyTorch. \n",
        "\n",
        "❗\n",
        "You may not use any functions from torch.nn or torch.nn.functional in your implementation.\n",
        "\n",
        "\n",
        "The goals of this exercise are to go through a simple example of the data-driven image classification pipeline, and also to practice writing efficient, vectorized code in [PyTorch](https://pytorch.org/).\n",
        "\n",
        "You may not use any functions from torch.nn or torch.nn.functional."
      ],
      "metadata": {
        "id": "obpZTVJ26hyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.0: Setup code (0 pts)\n",
        "\n",
        "We will run the codes on GPU. Go to:\n",
        "Runtime->Change Runtime Type->Hardware Accelerator->GPU"
      ],
      "metadata": {
        "id": "OfvRB8rdC1RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print('Good to go!')\n",
        "else:\n",
        "  print('Please set GPU via Edit -> Notebook Settings.')"
      ],
      "metadata": {
        "id": "HE2Hg8wFC0Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This line of code gives you the information about GPU\n",
        "! nvidia-smi"
      ],
      "metadata": {
        "id": "A2sV9w1q3Pek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.1 Linear Regression (15 pts)\n",
        "\n",
        "In this part of the question, you will implement linear regression for image classification on the TinyPlaces dataset. Your goal is to predict the category (indoor or outdoor) of an image based on its pixel values.\n",
        "\n",
        "To do this, you will need to complete the following steps:\n",
        "\n",
        "1. Initialize the weights and biases of the linear regression model and implement the linear function and the predict function in the LinearRegression class. If the prediction score is greater than 0.5, we consider the image to be of the outdoor category. Otherwise, we consider it to be of the indoor category.\n",
        "\n",
        "2. Use the fit function in the LinearRegression class to fit the linear regression model to the training data using gradient descent. You will need to set the learning rate and the number of epochs for the gradient descent algorithm.\n",
        "\n",
        "3. Evaluate the performance of the linear regression model on the training and validation datasets using the evaluate function in the LinearRegression class. This function should calculate the accuracy of the model on the dataset."
      ],
      "metadata": {
        "id": "ZcS_PwFfEwRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we dive into step by step implementations, we need to further organize our data."
      ],
      "metadata": {
        "id": "0RjvBauVTm9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(root_dir, 'data', 'tinyplaces_binary_train.pkl'), 'rb') as f:\n",
        "    binary_train = TinyPlacesDataset(pickle.load(f))\n",
        "with open(os.path.join(root_dir, 'data', 'tinyplaces_binary_val.pkl'), 'rb') as f:\n",
        "    binary_val = TinyPlacesDataset(pickle.load(f))\n",
        "with open(os.path.join(root_dir, 'data', 'tinyplaces_multi_train.pkl'), 'rb') as f:\n",
        "    multi_train = TinyPlacesDataset(pickle.load(f))\n",
        "with open(os.path.join(root_dir, 'data', 'tinyplaces_multi_val.pkl'), 'rb') as f:\n",
        "    multi_val = TinyPlacesDataset(pickle.load(f))\n",
        "\n",
        "# Convert everything from numpy arrays to tensors and move them to GPU.\n",
        "for dataset in [binary_train, binary_val, multi_train, multi_val]:\n",
        "  for k in ['data', 'label']:\n",
        "    dataset.dataset[k] = torch.tensor(dataset.dataset[k]).float().cuda()"
      ],
      "metadata": {
        "id": "977hhjjgTmSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Also, seed everything for reproducibility\n",
        "# code from https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964#file-seed_everything-py\n",
        "def seed_everything(seed: int):\n",
        "    import random, os\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True"
      ],
      "metadata": {
        "id": "SxiHmwV2ZX9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "We will implement linear regression step by step. \n",
        "\n",
        "Remember that for linear regression:\n",
        "\n",
        "$$y = WX+b$$\n",
        "\n",
        "In our case, $X$ is the 3072-dim image pixels. Thus, the weight $W$ should be compatible with the dim of $X$.\n",
        "\n",
        "If the prediction score > 0.5, we consider the image to be of outdoor category. Otherwise, we consider it to be indoor category.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Below is the class you need to implement. Don't be scared. Each code block requires a very few lines of implementation. \n",
        "\n",
        "You do not need to finish all the class functions at once. Instead, you should finish them one by one and try to debug them."
      ],
      "metadata": {
        "id": "sbXT1RrtTyDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "class LinearRegression(object):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        \"\"\"\n",
        "          Initialize the weights and biases using zeros distribution\n",
        "          \n",
        "          Parameters:\n",
        "              input_size (int): The input size (dimension of feature vectors)\n",
        "              output_size (int): The output size (dimension of output logits)\n",
        "          \n",
        "          Returns:\n",
        "              None.\n",
        "        \"\"\"\n",
        "        # Initialize the weights and biases using zeros\n",
        "        # Move the parameters to GPU (cuda).\n",
        "        ################# Your Implementations ################################\n",
        "\n",
        "        \n",
        "        ################# End of your Implementations ##########################\n",
        "        \n",
        "    def linear(self, x):\n",
        "        # Implement the linear function using y = Wx + b\n",
        "        ################# Your Implementations ################################\n",
        "        \n",
        "        ################# End of your Implementations ##########################\n",
        "        return  output\n",
        "\n",
        "    def forward(self, x):\n",
        "        # To make the output shape compact.\n",
        "        return self.linear(x).squeeze()\n",
        "\n",
        "    def get_loss(self, pred_logits, targets):\n",
        "        # Calculate the mean squared error between the predicted labels and the ground-truth labels\n",
        "        loss = None\n",
        "        ################# Your Implementations ################################\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return loss\n",
        "\n",
        "    def fit(self, x, y, x_val, y_val, lr, epochs=500000, print_freq=1000):\n",
        "        # Fit the linear regression model to the training data using gradient descent\n",
        "        # x is the input data, y is the ground-truth labels\n",
        "        # lr is the learning rate, epochs is the number of epochs\n",
        "        \n",
        "        # To store validation accuracy\n",
        "        val_accs = []\n",
        "        # Create a progress bar using tqdm\n",
        "        pbar = tqdm(range(epochs))\n",
        "        for epoch in pbar:\n",
        "            # Calculate the loss\n",
        "            y_pred_logits = self.forward(x) \n",
        "            loss = self.get_loss(y_pred_logits, y)\n",
        "            # Backpropagate the loss to compute the gradients\n",
        "            loss.backward()\n",
        "            # Update the weights and biases using gradient descent\n",
        "            with torch.no_grad():\n",
        "                self.W -= lr * self.W.grad\n",
        "                self.b -= lr * self.b.grad\n",
        "                # Reset the gradients\n",
        "                self.W.grad.zero_()\n",
        "                self.b.grad.zero_()\n",
        "                \n",
        "            if epoch % print_freq == 0:\n",
        "                # Calculate the validation accuracy\n",
        "                val_acc = self.evaluate(x_val, y_val)\n",
        "                val_accs.append(val_acc)\n",
        "                # Update the progress bar with the validation accuracy and training loss\n",
        "                pbar.set_description(f'val_acc: {val_acc:.3f}')\n",
        "        return val_accs\n",
        "                \n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        # Evaluate the performance of the linear regression model on the dataset\n",
        "        # x is the input data, y is the ground-truth labels\n",
        "        # Calculate the predicted labels\n",
        "        y_pred = self.forward(x) > 0.5\n",
        "        return (y_pred == y).float().mean().item()\n"
      ],
      "metadata": {
        "id": "eKbDCvNAEp4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Normalization (0 pts)\n",
        "It is usually important to normalize the inputs to a linear model. Let's define a normalziation function."
      ],
      "metadata": {
        "id": "b6CHxjD5yU38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(x):\n",
        "    # We can simply divide x by 255 since its range is (0,255)\n",
        "    return x / 255."
      ],
      "metadata": {
        "id": "YrfEfC8HySV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's prepare our data."
      ],
      "metadata": {
        "id": "gOdvGMDu3DsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here \"ori\" indicates \"original\"\n",
        "X_train_ori, y_train = binary_train.dataset['data'], binary_train.dataset['label']\n",
        "X_val_ori, y_val = binary_val.dataset['data'], binary_val.dataset['label']\n",
        "\n",
        "# Normalization\n",
        "X_train = normalize(X_train_ori)\n",
        "X_val = normalize(X_val_ori)"
      ],
      "metadata": {
        "id": "62hOHXjJS4ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model initialization and linear forward (5 pts)\n",
        "\n",
        "Let's test the performance of zeros-initialized model.\n",
        "\n",
        "You should finish two functions: `__init__` and `linear` in the `LinearRegression` class to run the below cell."
      ],
      "metadata": {
        "id": "vG0lialr3jBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(0)\n",
        "\n",
        "# Try to debug using this cell\n",
        "linear_model = LinearRegression(3072, 1)\n",
        "train_acc = linear_model.evaluate(X_train, y_train)\n",
        "val_acc = linear_model.evaluate(X_val, y_val)\n",
        "print('train accuracy:', train_acc)\n",
        "print('val accuracy:', val_acc)"
      ],
      "metadata": {
        "id": "J0A6AHca2-TG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above code should give you an accuracy of exactly 0.5, since we're using zeros-initialization, therefore all the outputs will be 0. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### Gradient descent (5 pts)\n",
        "\n",
        "Now, let's consider optimizing the linear model by gradient descent. \n",
        "\n",
        "Recall that to optimize the linear model using gradient descent, you need to follow these steps:\n",
        "1. Compute the losses between the predicted labels and ground-truth labels using the mean squared error (MSE) loss.\n",
        "\n",
        "2. Back-propagate the losses to get the gradients of the parameters. This is done automatically in PyTorch when you call the `loss.backward()` method.\n",
        "\n",
        "3. Get the parameters for the next iteration by subtracting the gradient multiplied by the learning rate. You can do this by updating the `self.W` and `self.b` parameters in the `fit()` function.\n",
        "\n",
        "\n",
        "You will only need to implement the first step as we have implemented the rest of the steps for you. Please finish the ``get_loss()`` in order to continue."
      ],
      "metadata": {
        "id": "BTS_hvlc2pW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to debug using this cell\n",
        "linear_model = LinearRegression(3072, 1)\n",
        "# We refer the raw outputs from a model to as \"logits\",\n",
        "# i.e., we haven't transformed the results to binary labels.\n",
        "y_pred_logits = linear_model.forward(X_train)\n",
        "loss = linear_model.get_loss(y_pred_logits, y_train)\n",
        "print(\"loss:\", loss.item())"
      ],
      "metadata": {
        "id": "KLKHdXqC2nL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting number is called `loss`, because it indicates how bad the model is at predicting the target variables. Lower the loss, better the model.\n",
        "\n",
        "Since we initialize the weight as zero, all outputs are zero. Therefore, the loss is 0.5 here. (Because it is the mean of the squared loss of the differences between preds and targets. )\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Next we will compute the gradients. With PyTorch, we can automatically compute the gradient or derivative of the loss w.r.t. to the weights and biases, because they have `requires_grad` set to True."
      ],
      "metadata": {
        "id": "x8HloDEO7CwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute gradients\n",
        "loss.backward()\n",
        "\n",
        "# check the gradients\n",
        "print(linear_model.W.grad)\n",
        "print(linear_model.b.grad)"
      ],
      "metadata": {
        "id": "vXz4DNg_7QwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to adjust the weights and biases using gradient descent in order to reduce the loss and improve the model. We do this by updating the weights by subtracting a small quantity proportional to the gradient, which is calculated during the backpropagation step."
      ],
      "metadata": {
        "id": "9WW4FDs27d2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a learning rate (lr)\n",
        "lr = 1e-4\n",
        "\n",
        "# Adjust weights & reset gradients\n",
        "with torch.no_grad():\n",
        "    linear_model.W -= lr * linear_model.W.grad\n",
        "    linear_model.b -= lr * linear_model.b.grad\n",
        "    # It's important to zero the gradients after update.\n",
        "    linear_model.W.grad.zero_()\n",
        "    linear_model.b.grad.zero_()"
      ],
      "metadata": {
        "id": "_OwfcI6g7oKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: Why should we zero the gradients after update?\n",
        "\n",
        "A: The gradients of the model's parameters are accumulated during the backward pass.  If you don't zero the gradients, they will be added to the gradients of the next iteration and the model's parameters will be updated using the accumulated gradients, which may not be what you want.\n",
        "\n",
        "You can also use the `optimizer.zero_grad()` method to zero the gradients of the model's parameters. This can be convenient if you are using an optimizer from PyTorch's `torch.optim` module. We will use this method in the future assignments.\n",
        "\n",
        "---\n",
        "We now calculate the new loss in the next iteration. You should see that the loss is reduced compared to the old loss (0.5)."
      ],
      "metadata": {
        "id": "Vjxq6MlT81ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate new loss\n",
        "y_pred_logits = linear_model.forward(X_train)\n",
        "loss = linear_model.get_loss(y_pred_logits, y_train)\n",
        "print (\"new loss is %f\"%loss.item())"
      ],
      "metadata": {
        "id": "5L6yhlLN7-ie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training for multiple epochs.\n",
        "\n",
        "To further reduce the loss, we can repeat the process of adjusting the weights and biases using the gradients multiple times."
      ],
      "metadata": {
        "id": "ii17wRAF9gp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this\n",
        "linear_model = LinearRegression(3072, 1)\n",
        "# You can reduce the number of training epochs to debug. Running 500000 takes me 8 mins.\n",
        "lin_val_accs = linear_model.fit(X_train, y_train, X_val, y_val, 1e-4, 500000, 1000)"
      ],
      "metadata": {
        "id": "FRzp8zPu8gyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Iteration v.s. Epoch (5 pts)\n",
        "\n",
        "Use your own words, explain the difference between training iterations and training epochs."
      ],
      "metadata": {
        "id": "T6zQ2My0Eeme"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Double click to edit)\n",
        "\n",
        "Your Answer:\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "LGvOZjWYEsHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.2 Logistic Regression (10pts)\n",
        "\n",
        "Logistic regression is a classification algorithm that extends Linear Regression by applying a sigmoid function to the output of the linear model:\n",
        "\n",
        "$$\\frac{1}{1+e^{-x}}$$\n",
        "\n",
        "The common loss function used in logistic regression is the cross-entropy loss, which measures the difference between the predicted probability distribution and the true probability distribution of the target classes:\n",
        "\n",
        "$$\\mathrm{CE_Loss}(p, y) = -{(y\\log(p) + (1-y)\\log(1-p))}$$\n",
        "\n",
        "You need to implement the sigmoid function (5 pts) and the cross entropy loss (5pts)."
      ],
      "metadata": {
        "id": "4DGOh6Fm_bMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "  ################# Your Implementations ################################\n",
        "  \n",
        "  ################# End of your Implementations ##########################\n",
        "  return  output\n",
        "\n",
        "\n",
        "def cross_entropy_loss(p, y):\n",
        "  ################# Your Implementations ################################\n",
        "\n",
        "  ################# End of your Implementations ##########################\n",
        "  return  output\n"
      ],
      "metadata": {
        "id": "uyJW0sfdiMv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that Logistic Regression is an extension of Linear Regression, so we can implement it by inheriting from Linear Regression:"
      ],
      "metadata": {
        "id": "p7GxPJ-yBSbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression(LinearRegression):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LogisticRegression, self).__init__(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the sigmoid function to the linear output\n",
        "        ################# Your Implementations ################################\n",
        "        \n",
        "        ################# End of your Implementations ##########################\n",
        "        return  output\n",
        "\n",
        "    def get_loss(self, pred_logits, targets):\n",
        "        # Calculate the cross-entropy loss\n",
        "        ################# Your Implementations ################################\n",
        "        \n",
        "        ################# End of your Implementations ##########################\n",
        "        return loss"
      ],
      "metadata": {
        "id": "3NrBfgSuBQ3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_model = LogisticRegression(3072, 1)\n",
        "logi_val_accs = logistic_model.fit(X_train, y_train, X_val, y_val, lr, 500000, 1000)"
      ],
      "metadata": {
        "id": "ZkXUCz6zBxOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize the validation accuracy"
      ],
      "metadata": {
        "id": "q8u9PjZ8DZHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(lin_val_accs, label='LinearRegression')\n",
        "plt.plot(logi_val_accs, label='LogisticRegression')\n",
        "plt.legend()\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Validation Accuracy (%)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "28G1Z5_zCIva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize the weights\n",
        "\n",
        "Visualize the learned weights for each class. If your implementation is right, the visualization should be close to the average of the images."
      ],
      "metadata": {
        "id": "888iMj4qFJes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_model_weights(model, model_name):\n",
        "    w = model.W.reshape(32, 32, 3)\n",
        "    w_min, w_max = torch.min(w), torch.max(w)\n",
        "    wimg = 255.0 * (w - w_min) / (w_max - w_min)\n",
        "\n",
        "    plt.imshow(wimg.type(torch.uint8).cpu())\n",
        "    plt.axis('off')\n",
        "    plt.title(model_name)\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "visualize_model_weights(linear_model, 'Linear Model')\n",
        "plt.subplot(1,2,2)\n",
        "visualize_model_weights(logistic_model, 'Logistic Model')\n"
      ],
      "metadata": {
        "id": "jj5MAKjAFPf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.3 Softmax Regression (10 pts)\n",
        "\n",
        "Softmax regression is a classification algorithm that extends logistic regression. It is used to predict the probability of a data point belonging to each of the K classes in a multi-class classification problem. In contrast to logistic regression, which only handles binary classification problems, softmax regression allows us to predict the probability of a data point belonging to any of K classes.\n",
        "\n",
        "The softmax function is a generalization of the sigmoid function, which is used in logistic regression. It maps the output of a linear model to a probability distribution over the K classes. The softmax function is defined as:\n",
        "\n",
        "$$\\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}} \\ \\ \\ for\\ i=1,2,\\dots,K$$\n",
        "\n",
        "where z is the output of the linear model, and K is the number of classes.\n",
        "\n",
        "The loss function for softmax regression is the negative log likelihood (NLL) loss. It measures the difference between the predicted probability distribution and the true probability distribution of the target class. The NLL loss is defined as:\n",
        "\n",
        "$$-{\\log(p(y))}$$\n",
        "\n",
        "where p(y) is the predicted probability of the data point belonging to the true class y.\n",
        "\n",
        "In the Softmax Regression class, you will need to implement:\n",
        "1. The softmax function that maps the output of the linear model to a probability distribution over the K classes.\n",
        "2. The NLL loss function that measures the difference between the predicted probability distribution and the true probability distribution of the target class.\n",
        "3. The predict function that predicts the class of a data point based on the predicted probability distribution."
      ],
      "metadata": {
        "id": "dz1F6flbHBMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    # Implement the softmax function\n",
        "    # x is the input data with shape (batch_size, input_size)\n",
        "    # Return the softmax output with shape (batch_size, output_size)\n",
        "    ################# Your Implementations ################################\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "    return  output\n",
        "  \n",
        "\n",
        "\n",
        "def nll_loss(pred_probs, targets):\n",
        "    # Calculate the negative log likelihood loss\n",
        "    # pred_probs is the predicted probability distribution with shape (batch_size, output_size)\n",
        "    # targets is the ground-truth labels with shape (batch_size,)\n",
        "    # Return the negative log likelihood loss with shape (batch_size,)\n",
        "\n",
        "    # Hine: Convert the ground-truth labels to one-hot encoding using torch.eye()\n",
        "    ################# Your Implementations ################################\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    ################# End of your Implementations #################\n",
        "    return loss"
      ],
      "metadata": {
        "id": "vUNVJMx2LoTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxRegression(LinearRegression):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(SoftmaxRegression, self).__init__(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the softmax function to the linear output\n",
        "        ################# Your Implementations ################################\n",
        "        \n",
        "        ################# End of your Implementations ##########################\n",
        "        return output\n",
        "\n",
        "    def get_loss(self, pred_logits, targets):\n",
        "        # Calculate the cross-entropy loss\n",
        "        ################# Your Implementations ################################\n",
        "        \n",
        "        ################# End of your Implementations ##########################\n",
        "        return loss\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        # Evaluate the performance of the linear regression model on the dataset\n",
        "        # x is the input data, y is the ground-truth labels\n",
        "        # Calculate the predicted labels\n",
        "        y_pred = self.forward(x)\n",
        "        y_pred = y_pred.argmax(dim=1)\n",
        "        return (y_pred == y).float().mean().item()\n"
      ],
      "metadata": {
        "id": "BzYXxe6TGTjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare multi-class dataset\n",
        "# Here \"ori\" indicates \"original\"\n",
        "X_train_ori_multi, y_train_multi = multi_train.dataset['data'], multi_train.dataset['label']\n",
        "X_val_ori_multi, y_val_multi = multi_val.dataset['data'], multi_val.dataset['label']\n",
        "\n",
        "# Normalization\n",
        "X_train_multi = normalize(X_train_ori_multi)\n",
        "X_val_multi = normalize(X_val_ori_multi)"
      ],
      "metadata": {
        "id": "BcI45vFhMc32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sfm_model = SoftmaxRegression(3072, 20)\n",
        "sfm_val_accs = sfm_model.fit(X_train_multi, y_train_multi, X_val_multi, y_val_multi, lr, 100000, 1000)"
      ],
      "metadata": {
        "id": "YBlVP4RIMQ4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I can get a validation accuracy around 21%. How about you?"
      ],
      "metadata": {
        "id": "uWNMas5Wjbvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visualize Weight\n",
        "\n",
        "Now, visualize the weight for each class. If your implementation is right, the weight of each class should be equivalent to the average of the images of that class."
      ],
      "metadata": {
        "id": "JrymHivkRaKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = sfm_model.W.reshape(32, 32, 3, 20)\n",
        "w_min, w_max = torch.min(w), torch.max(w)\n",
        "plt.figure(figsize=(10,10))\n",
        "classes = ['bathroom', 'bedroom', 'bookstore', 'classroom', 'dining_room', 'food_court', 'kitchen', 'lobby', 'living_room', 'office', 'baseball_field', 'bridge', 'campsite', 'canyon', 'coast', 'fountain', 'highway', 'playground', 'mountain', 'rainforest']\n",
        "for i in range(20):\n",
        "  plt.subplot(4,5, i + 1)\n",
        "  # Rescale the weights to be between 0 and 255\n",
        "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
        "  plt.imshow(wimg.type(torch.uint8).cpu())\n",
        "  plt.axis('off')\n",
        "  plt.title(classes[i])"
      ],
      "metadata": {
        "id": "0BRvd-G2MXR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.4: Comparison (5 pts)\n",
        "\n",
        "Use your own words, explain what's the difference between Linear Regression, Logistic Regression and Softmax Regression? (Write down your answer below)"
      ],
      "metadata": {
        "id": "rndJ7OjaRHef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Answer**: [fill in the answer here]\n",
        "\n",
        "-----"
      ],
      "metadata": {
        "id": "I12Gpv5nROiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3: KNN Classification (35 pts)\n",
        "\n",
        "K-Nearest Neighbors (KNN) is a simple and powerful classification algorithm that is based on the idea of finding the k nearest data points (neighbors) to a given data point, and predicting the class of the data point based on the majority class of its neighbors.\n",
        "\n",
        "You will implement a KNN classifier on the TinyPlaces dataset you created.\n",
        "\n",
        "To implement KNN for binary classification, you need to follow these steps:\n",
        "\n",
        "1. Define a distance function to measure the similarity between data points. The most common distance function used in KNN is the Euclidean distance.\n",
        "2. Define the predict function that takes in a test data point and the number of neighbors (k) as input, and returns the predicted class. The predict function should perform the following steps:\n",
        "  1. Calculate the distance between the test data point and each training data point.\n",
        "  2. Sort the distances in ascending order and select the k nearest neighbors.\n",
        "  3. Predict the class of the test data point based on the majority class of its k nearest neighbors.\n",
        "3. Define the evaluate function that takes in the test data and ground-truth labels as input, and returns the accuracy of the KNN model on the test data.\n",
        "4. Train and evaluate the KNN model on the training and test data."
      ],
      "metadata": {
        "id": "3_hYlYIrRoJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "### Q3.1 Three ways to compute distances (10pts)\n",
        "\n",
        "In this question, you will be implementing three different ways to compute the distance matrix between all pairs of training and test examples. \n",
        "\n",
        "The distance matrix is an important component in many machine learning algorithms, as it allows us to measure the similarity between examples. In this case, we will be using the squared Euclidean distance as our measure of similarity. \n",
        "\n",
        "1. The first method you will implement uses explicit loops over the training and test sets, which can be computationally expensive for large datasets. \n",
        "\n",
        "2. The second method will use a single loop over the training set and vectorized operations to compute the distances. \n",
        "\n",
        "3. The third method will use no loops and only vectorized operations to compute the distances. These three methods will be compared in terms of efficiency and accuracy."
      ],
      "metadata": {
        "id": "15h7q56_ZJQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We subsample a dataset for faster developement\n",
        "binary_train_subsampled = binary_train.subsample(0.2)\n",
        "binary_val_subsampled = binary_val.subsample(0.2)\n",
        "\n",
        "\n",
        "# Here \"ori\" indicates \"original\n",
        "X_train, y_train = binary_train_subsampled.dataset['data'], binary_train_subsampled.dataset['label']\n",
        "X_val, y_val = binary_val_subsampled.dataset['data'], binary_val_subsampled.dataset['label']\n",
        "\n",
        "# Normalization\n",
        "X_train = normalize(X_train)\n",
        "X_val = normalize(X_val)"
      ],
      "metadata": {
        "id": "ail027RcVPOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Explict but expensive two-loops\n",
        "\n",
        "Lets begin with computing the distance matrix between all training and test examples. First we will implement a naive version of the distance computation, using explicit loops over the training and test sets.\n",
        "\n",
        "**NOTE: When implementing distance functions for this assignment, you may not use functions `torch.norm` or `torch.dist` (or their instance method variants `x.norm` / `x.dist`); you may not use any functions from `torch.nn` or `torch.nn.functional`.**"
      ],
      "metadata": {
        "id": "W4f-ILoMaWSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_distances_two_loops(x_train, x_test):\n",
        "  \"\"\"\n",
        "  Computes the squared Euclidean distance between each element of the training\n",
        "  set and each element of the test set. Images should be flattened and treated\n",
        "  as vectors. This implementation uses a naive set of nested loops over the training and\n",
        "  test data.\n",
        "\n",
        "  The input data may have any number of dimensions -- for example this function\n",
        "  should be able to compute nearest neighbor between vectors, in which case\n",
        "  the inputs will have shape (num_{train, test}, D); it should alse be able to\n",
        "  compute nearest neighbors between images, where the inputs will have shape\n",
        "  (num_{train, test}, C, H, W). More generally, the inputs will have shape\n",
        "  (num_{train, test}, D1, D2, ..., Dn); you should flatten each element\n",
        "  of shape (D1, D2, ..., Dn) into a vector of shape (D1 * D2 * ... * Dn) before\n",
        "  computing distances.\n",
        "\n",
        "  Parameters:\n",
        "    - x_train: Torch tensor of shape (num_train, D1, D2, ...)\n",
        "    - x_test: Torch tensor of shape (num_test, D1, D2, ...)\n",
        "\n",
        "Returns:\n",
        "    - dists: Torch tensor of shape (num_train, num_test) where dists[i, j] is the\n",
        "      squared Euclidean distance between the ith training point and the jth test\n",
        "      point. It has the same dtype as x_train.\n",
        "\"\"\"\n",
        "  # Initialize dists to be a tensor of shape (num_train, num_test) with the\n",
        "  # same datatype and device as x_train\n",
        "  num_train = x_train.shape[0]\n",
        "  num_test = x_test.shape[0]\n",
        "  dists = torch.zeros(num_train, num_test, dtype=torch.float64)\n",
        "  ##############################################################################\n",
        "  # TODO: Implement this function using a pair of nested loops over the        #\n",
        "  # training data and the test data.                                           #\n",
        "  ##############################################################################\n",
        "  \n",
        "\n",
        "  \n",
        "  ##############################################################################\n",
        "  #                             END OF YOUR CODE                               #\n",
        "  ##############################################################################\n",
        "  return dists\n",
        "\n"
      ],
      "metadata": {
        "id": "8pfgUFaTOgeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell should take a relative long time to run, as two-loop implementation is slow.\n",
        "dists = compute_distances_two_loops(X_train, X_val)\n",
        "print('dists has shape: ', dists.shape)"
      ],
      "metadata": {
        "id": "FIYcxMrjVMic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Efficient one-loop vectorization\n",
        "Our implementation of the distance computation above is fairly inefficient since it uses nested Python loops over the training and test sets.\n",
        "\n",
        "When implementing algorithms in PyTorch, it's best to avoid loops in Python if possible. Instead it is preferable to implement your computation so that all loops happen inside PyTorch functions. This will usually be much faster than writing your own loops in Python, since PyTorch functions can be internally optimized to iterate efficiently, possibly using multiple threads. This is especially important when using a GPU to accelerate your code.\n",
        "\n",
        "The process of eliminating explict loops from your code is called **vectorization**. Sometimes it is straighforward to vectorize code originally written with loops; other times vectorizing requires thinking about the problem in a new way. We will use vectorization to improve the speed of our distance computation function.\n",
        "\n",
        "As a first step toward vectorizing our distance computation, you will implement a version that uses only a single Python loop over the training data.\n",
        "\n",
        "We can check the correctness of our one-loop implementation by comparing it with our two-loop implementation on some randomly generated data.\n",
        "\n",
        "Note that we do the comparison with 64-bit floating points for increased numeric precision."
      ],
      "metadata": {
        "id": "wrj3gwcKWJl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_distances_one_loop(x_train, x_test):\n",
        "  \"\"\"\n",
        "  Computes the squared Euclidean distance between each element of the training\n",
        "  set and each element of the test set. Images should be flattened and treated\n",
        "  as vectors.\n",
        "\n",
        "  This implementation uses only a single loop over the training data.\n",
        "\n",
        "  Similar to compute_distances_two_loops, this should be able to handle inputs\n",
        "  with any number of dimensions. The inputs should not be modified.\n",
        "\n",
        "  Inputs:\n",
        "  - x_train: Torch tensor of shape (num_train, D1, D2, ...)\n",
        "  - x_test: Torch tensor of shape (num_test, D1, D2, ...)\n",
        "\n",
        "  Returns:\n",
        "  - dists: Torch tensor of shape (num_train, num_test) where dists[i, j] is the\n",
        "    squared Euclidean distance between the ith training point and the jth test\n",
        "    point.\n",
        "  \"\"\"\n",
        "  # Initialize dists to be a tensor of shape (num_train, num_test) with the\n",
        "  # same datatype and device as x_train\n",
        "  num_train = x_train.shape[0]\n",
        "  num_test = x_test.shape[0]\n",
        "  dists = torch.zeros(num_train, num_test, dtype=torch.float64)\n",
        "  ##############################################################################\n",
        "  # TODO: Implement this function using only a single loop over x_train.       #\n",
        "  ##############################################################################\n",
        "  \n",
        "  \n",
        "  \n",
        "  ##############################################################################\n",
        "  ##############################################################################\n",
        "  #                             END OF YOUR CODE                               #\n",
        "  ##############################################################################\n",
        "  return dists"
      ],
      "metadata": {
        "id": "Rw4CTJftWQ0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "x_train_rand = torch.randn(100, 3, 16, 16, dtype=torch.float64)\n",
        "x_test_rand = torch.randn(100, 3, 16, 16, dtype=torch.float64)\n",
        "\n",
        "dists_one = compute_distances_one_loop(x_train_rand, x_test_rand)\n",
        "dists_two = compute_distances_two_loops(x_train_rand, x_test_rand)\n",
        "difference = (dists_one - dists_two).pow(2).sum().sqrt().item()\n",
        "print('Difference: ', difference)\n",
        "if difference < 1e-4:\n",
        "    print('Good! The distance matrices match')\n",
        "else:\n",
        "    print('Uh-oh! The distance matrices are different')"
      ],
      "metadata": {
        "id": "gFAN63OIWJLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Efficient no-loop vectorization\n",
        "\n",
        "You will now implement a fully vectorized version of the distance computation function\n",
        "that does not use any Python loops."
      ],
      "metadata": {
        "id": "TZaJOZfIanX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def compute_distances_no_loops(x_train, x_test):\n",
        "  \"\"\"\n",
        "  Computes the squared Euclidean distance between each element of the training\n",
        "  set and each element of the test set. Images should be flattened and treated\n",
        "  as vectors.\n",
        "\n",
        "  This implementation should not use any Python loops. For memory-efficiency,\n",
        "  it also should not create any large intermediate tensors; in particular you\n",
        "  should not create any intermediate tensors with O(num_train*num_test)\n",
        "  elements.\n",
        "\n",
        "  Similar to compute_distances_two_loops, this should be able to handle inputs\n",
        "  with any number of dimensions. The inputs should not be modified.\n",
        "\n",
        "  NOTE: Your implementation may not use `torch.norm`, `torch.dist`,\n",
        "  `torch.cdist`, or their instance method variants x.norm / x.dist / x.cdist.\n",
        "  You may not use any functions from torch.nn or torch.nn.functional.\n",
        "  Inputs:\n",
        "  - x_train: Torch tensor of shape (num_train, C, H, W)\n",
        "  - x_test: Torch tensor of shape (num_test, C, H, W)\n",
        "\n",
        "  Returns:\n",
        "  - dists: Torch tensor of shape (num_train, num_test) where dists[i, j] is the\n",
        "    squared Euclidean distance between the ith training point and the jth test\n",
        "    point.\n",
        "  \"\"\"\n",
        "  # Initialize dists to be a tensor of shape (num_train, num_test) with the\n",
        "  # same datatype and device as x_train\n",
        "  num_train = x_train.shape[0]\n",
        "  num_test = x_test.shape[0]\n",
        "  dists = torch.zeros(num_train, num_test, dtype=torch.float64)\n",
        "  ##############################################################################\n",
        "  # TODO: Implement this function without using any explicit loops and without #\n",
        "  # creating any intermediate tensors with O(num_train * num_test) elements.   #\n",
        "  #                                                                            #\n",
        "  # You may not use torch.norm (or its instance method variant), nor any       #\n",
        "  # functions from torch.nn or torch.nn.functional.                            #\n",
        "  #                                                                            #\n",
        "  # HINT: Try to formulate the Euclidean distance using two broadcast sums     #\n",
        "  #       and a matrix multiply.                                               #\n",
        "  ##############################################################################\n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "  ##############################################################################\n",
        "  #                             END OF YOUR CODE                               #\n",
        "  ##############################################################################\n",
        "  return dists\n"
      ],
      "metadata": {
        "id": "L_FvfH_AVe8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we can check the correctness of our implementation by comparing the fully vectorized version against the original naive version:"
      ],
      "metadata": {
        "id": "aPFPYiJSedSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "x_train_rand = torch.randn(100, 3, 16, 16, dtype=torch.float64)\n",
        "x_test_rand = torch.randn(100, 3, 16, 16, dtype=torch.float64)\n",
        "\n",
        "dists_two = compute_distances_two_loops(x_train_rand, x_test_rand)\n",
        "dists_none = compute_distances_no_loops(x_train_rand, x_test_rand)\n",
        "difference = (dists_two - dists_none).pow(2).sum().sqrt().item()\n",
        "print('Difference: ', difference)\n",
        "if difference < 1e-4:\n",
        "  print('Good! The distance matrices match')\n",
        "else:\n",
        "  print('Uh-oh! The distance matrices are different')"
      ],
      "metadata": {
        "id": "iGUrEeOSecp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now compare the speed of our three implementations. If you've implemented everything properly, the one-loop implementation should take less than 4 seconds to run, and the fully vectorized implementation should take less than 0.1 seconds to run."
      ],
      "metadata": {
        "id": "KZiE41lsWiVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def timeit(f, *args):\n",
        "    tic = time.time()\n",
        "    f(*args) \n",
        "    toc = time.time()\n",
        "    return toc - tic\n",
        "\n",
        "torch.manual_seed(0)\n",
        "x_train_rand = torch.randn(500, 3, 32, 32)\n",
        "x_test_rand = torch.randn(500, 3, 32, 32)\n",
        "\n",
        "two_loop_time = timeit(compute_distances_two_loops, x_train_rand, x_test_rand)\n",
        "print('Two loop version took %.2f seconds' % two_loop_time)\n",
        "\n",
        "one_loop_time = timeit(compute_distances_one_loop, x_train_rand, x_test_rand)\n",
        "speedup = two_loop_time / one_loop_time\n",
        "print('One loop version took %.2f seconds (%.1fX speedup)'\n",
        "      % (one_loop_time, speedup))\n",
        "\n",
        "no_loop_time = timeit(compute_distances_no_loops, x_train_rand, x_test_rand)\n",
        "speedup = two_loop_time / no_loop_time\n",
        "print('No loop version took %.2f seconds (%.1fX speedup)'\n",
        "      % (no_loop_time, speedup))"
      ],
      "metadata": {
        "id": "iDY_FqDBWhxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3.2 Prediction function (10 pts)\n",
        "\n",
        "Define the predict function that takes in a distance matrix, a set of memory labels (training labels), and the number of neighbors (k) as input, and returns the top-k indices and the predicted classes. The predict function should perform the following steps:\n",
        "\n",
        "1. Sort the distance matrix in ascending order and select the top-k indices for each test sample.\n",
        "2. Predict the class of each test sample based on the majority class among its k nearest neighbors in the training set.\n",
        "3. Return the top-k indices and the predicted classes for each test sample.\n",
        "\n",
        "Now, let's implement the `predict_labels` function to continue"
      ],
      "metadata": {
        "id": "4xw-dwdegg44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_labels(distance_matrix, memory_labels, k=1):\n",
        "  \"\"\"\n",
        "  Given distances between all pairs of training and test samples, predict a\n",
        "  label for each test sample by taking a **majority vote** among its k nearest\n",
        "  neighbors in the training set.\n",
        "\n",
        "  In the event of a tie, this function **should** return the smallest label. For\n",
        "  example, if k=5 and the 5 nearest neighbors to a test example have labels\n",
        "  [1, 2, 1, 2, 3] then there is a tie between 1 and 2 (each have 2 votes), so\n",
        "  we should return 1 since it is the smallest label.\n",
        "s\n",
        "  This function should not modify any of its inputs.\n",
        "\n",
        "  Inputs:\n",
        "  - distance_matrix: Torch tensor of shape (num_train, num_test) where dists[i, j] is the\n",
        "    squared Euclidean distance between the ith training point and the jth test\n",
        "    point.\n",
        "  - memory_labels: Torch tensor of shape (num_train,) giving labels for all training\n",
        "    samples. Each label is an integer in the range [0, num_classes - 1]\n",
        "  - k: The number of nearest neighbors to use for classification.\n",
        "\n",
        "  Returns:\n",
        "  - indices: top k indices of the most relevant images\n",
        "  - y_pred: A torch int64 tensor of shape (num_test,) giving predicted labels\n",
        "    for the test data, where y_pred[j] is the predicted label for the jth test\n",
        "    example. Each label should be an integer in the range [0, num_classes - 1].\n",
        "  \"\"\"\n",
        "  y_pred = torch.zeros(distance_matrix.shape[1], dtype=torch.int64)\n",
        "  memory_labels = memory_labels.int()\n",
        "  ##############################################################################\n",
        "  # TODO: Implement this function. You may use an explicit loop over the test  #\n",
        "  # samples. Hint: Look up the function torch.topk                             #\n",
        "  ##############################################################################\n",
        "  \n",
        "\n",
        "\n",
        "  \n",
        "  ##############################################################################\n",
        "  #                             END OF YOUR CODE                               #\n",
        "  ##############################################################################\n",
        "  return indices, y_pred"
      ],
      "metadata": {
        "id": "1JEV54_khSgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Image Retrieval\n",
        "We then work on a toy example of 1D data to test `predict_labels` to output the correct indices. If your implementation is correct, should output tensor([[1, 2, 0]])."
      ],
      "metadata": {
        "id": "acDLoZXckXhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "dists = torch.tensor([\n",
        "    [0.3, 0.4, 0.1],\n",
        "    [0.1, 0.5, 0.5],\n",
        "    [0.4, 0.1, 0.2],\n",
        "    [0.2, 0.2, 0.4],\n",
        "    [0.5, 0.3, 0.3],\n",
        "])\n",
        "y_train = torch.tensor([0, 1, 0, 1, 2])\n",
        "indices, _ = predict_labels(dists, y_train, k=1)\n",
        "print (indices)"
      ],
      "metadata": {
        "id": "lovykVaJkTw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4DOr9xObwm02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we implement the `retrieval` function in `KNNClassifier` to retrieve 2D images. We use an example from the TinyPlaces dataset. We visualize the 5 most relevant images from the training dataset giving a sample from the test set."
      ],
      "metadata": {
        "id": "4MxW_G1BlC4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KNNClassifier():\n",
        "    def __init__(self, x_train, y_train):\n",
        "        \"\"\"\n",
        "        Create a new K-Nearest Neighbor classifier with the specified training data.\n",
        "        In the initializer we simply memorize the provided training data.\n",
        "\n",
        "        Inputs:\n",
        "        - x_train: Torch tensor of shape (num_train, C, H, W) giving training data\n",
        "        - y_train: int64 torch tensor of shape (num_train,) giving training labels\n",
        "        \"\"\"\n",
        "        ###########################################################################\n",
        "        # TODO: Implement the initializer for this class. It should perform no    #\n",
        "        # computation and simply memorize the training data.                      #\n",
        "        ###########################################################################\n",
        "        \n",
        "\n",
        "\n",
        "        ###########################################################################\n",
        "        #                           END OF YOUR CODE                              #\n",
        "        ###########################################################################\n",
        "\n",
        "    def retrieval(self, x_test, k=1):\n",
        "        \"\"\"\n",
        "        Retrieve the most relevant images from the training set\n",
        "\n",
        "        Inputs:\n",
        "        - x_test: Torch tensor of shape (num_test, C, H, W) giving test samples\n",
        "        - k: The number of neighbors to use for predictions\n",
        "\n",
        "        Returns:\n",
        "        - indices: the indices of the top k most relevant images\n",
        "        \"\"\"\n",
        "        ###########################################################################\n",
        "        # TODO: Implement this method. You should use the functions you wrote     #\n",
        "        # above for computing distances (use the no-loop variant) and to retrieve #\n",
        "        # images                                                                  #\n",
        "        ###########################################################################\n",
        "        \n",
        "\n",
        "\n",
        "        ###########################################################################\n",
        "        #                           END OF YOUR CODE                              #\n",
        "        ########################################################################### \n",
        "        return indices\n",
        "\n",
        "    def predict(self, x_test, k=1):\n",
        "        \"\"\"\n",
        "        Make predictions using the classifier.\n",
        "\n",
        "        Inputs:\n",
        "        - x_test: Torch tensor of shape (num_test, C, H, W) giving test samples\n",
        "        - k: The number of neighbors to use for predictions\n",
        "\n",
        "        Returns:\n",
        "        - y_test_pred: Torch tensor of shape (num_test,) giving predicted labels\n",
        "          for the test samples.\n",
        "        \"\"\"\n",
        "        y_test_pred = None\n",
        "        ###########################################################################\n",
        "        # TODO: Implement this method. You should use the functions you wrote     #\n",
        "        # above for computing distances (use the no-loop variant) and to predict  #\n",
        "        # output labels.\n",
        "        ###########################################################################\n",
        "        \n",
        "\n",
        "        \n",
        "        ###########################################################################\n",
        "        #                           END OF YOUR CODE                              #\n",
        "        ###########################################################################\n",
        "        return y_test_pred\n",
        "\n",
        "    def check_accuracy(self, x_test, y_test, k=1, quiet=False):\n",
        "        \"\"\"\n",
        "        Utility method for checking the accuracy of this classifier on test data.\n",
        "        Returns the accuracy of the classifier on the test data, and also prints a\n",
        "        message giving the accuracy.\n",
        "\n",
        "        Inputs:\n",
        "        - x_test: Torch tensor of shape (num_test, C, H, W) giving test samples\n",
        "        - y_test: int64 torch tensor of shape (num_test,) giving test labels\n",
        "        - k: The number of neighbors to use for prediction\n",
        "        - quiet: If True, don't print a message.\n",
        "\n",
        "        Returns:\n",
        "        - accuracy: Accuracy of this classifier on the test data, as a percent.\n",
        "          Python float in the range [0, 100].\n",
        "        \"\"\"\n",
        "        y_test_pred = self.predict(x_test, k)\n",
        "        y_test_pred = y_test_pred.to(y_test.device)\n",
        "        num_correct = (y_test_pred == y_test).sum().item()\n",
        "        accuracy = float(num_correct) / y_test.shape[0] * 100\n",
        "        if not quiet:\n",
        "            print(f'Got {num_correct} / {y_test.shape[0]} correct => accuracy: {accuracy:.2f}')\n",
        "        return accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "ncnAN3UClCBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we implement the `retrieval` function in `KnnClassifier` to retrieve 2D images. We use an example from the TinyPlaces dataset. We visualize the 5 most relevant images from the training dataset giving a sample from the test set."
      ],
      "metadata": {
        "id": "PYz8JT19oemI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "num_classes = 2\n",
        "classes_list = ['indoor', 'outdoor']\n",
        "\n",
        "# We subsample a dataset for faster developement\n",
        "seed = None # You can fix a seed for deterministic results\n",
        "binary_train_subsampled = binary_train.subsample(0.2, seed)\n",
        "binary_val_subsampled = binary_val.subsample(0.2, seed)\n",
        "\n",
        "\n",
        "# Here \"ori\" indicates \"original\n",
        "X_train, y_train = binary_train_subsampled.dataset['data'], binary_train_subsampled.dataset['label']\n",
        "X_val, y_val = binary_val_subsampled.dataset['data'], binary_val_subsampled.dataset['label']\n",
        "\n",
        "# Normalization\n",
        "X_train = normalize(X_train)\n",
        "X_val = normalize(X_val)\n",
        "\n",
        "classifier = KNNClassifier(X_train, y_train)\n",
        "\n",
        "indices = classifier.retrieval(X_val[:1], k=5)\n",
        "\n",
        "plt.figure(figsize=(20,5))\n",
        "plt.subplot(1,6,1)\n",
        "plt.imshow(X_val[0].reshape(32,32,3).cpu().numpy())\n",
        "plt.title(f'query:{classes_list[int(y_val[0])]}')\n",
        "plt.axis('off')\n",
        "\n",
        "for i, idx in enumerate(indices):\n",
        "  plt.subplot(1,6,i+2)\n",
        "  plt.imshow(X_train[idx].cpu().numpy().reshape(32, 32, 3))\n",
        "  plt.axis('off')\n",
        "  plt.title(f'retrieved: {classes_list[int(y_train[idx])]}')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ji8Gq_kuhpMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict Labels\n",
        "The next step is to output the labels via retrieved indices. We further implement `predict_labels` to output the labels for an example in the test set. Here we use a toy test. "
      ],
      "metadata": {
        "id": "cDhsxsLNXo_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "dists = torch.tensor([\n",
        "    [0.3, 0.4, 0.1],\n",
        "    [0.1, 0.5, 0.5],\n",
        "    [0.4, 0.1, 0.2],\n",
        "    [0.2, 0.2, 0.4],\n",
        "    [0.5, 0.3, 0.3],\n",
        "])\n",
        "y_train = torch.tensor([0, 1, 0, 1, 2])\n",
        "y_pred_expected = torch.tensor([1, 0, 0])\n",
        "_, y_pred = predict_labels(dists, y_train, k=3)\n",
        "correct = y_pred.tolist() == y_pred_expected.tolist()\n",
        "print('Correct: ', correct)"
      ],
      "metadata": {
        "id": "Bhk0IUI2rdyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we use an example from the TinyPlaces dataset. "
      ],
      "metadata": {
        "id": "8fKHVlCgriL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We subsample a dataset for faster developement\n",
        "seed = None # You can fix a seed for deterministic results\n",
        "binary_train_subsampled = binary_train.subsample(0.2, seed)\n",
        "binary_val_subsampled = binary_val.subsample(0.2, seed)\n",
        "\n",
        "\n",
        "# Here \"ori\" indicates \"original\n",
        "X_train, y_train = binary_train_subsampled.dataset['data'], binary_train_subsampled.dataset['label']\n",
        "X_val, y_val = binary_val_subsampled.dataset['data'], binary_val_subsampled.dataset['label']\n",
        "\n",
        "# Normalization\n",
        "X_train = normalize(X_train)\n",
        "X_val = normalize(X_val)\n",
        "\n",
        "classifier = KNNClassifier(X_train, y_train)\n",
        "\n",
        "y_pred_test = classifier.predict(X_val[:1], k=10).squeeze()\n",
        "\n",
        "correct = y_pred_test == y_val[0]\n",
        "print('Correct: ', correct)"
      ],
      "metadata": {
        "id": "3hPzbXvsrSpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Image Classification\n",
        "We can now use the exact same KNN code to perform image classification on TinyPlaces!\n",
        "\n",
        "Now lets put everything together and test our K-NN clasifier on the full TinyPlaces, using k=1:"
      ],
      "metadata": {
        "id": "USmaiBzdr0hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = binary_train.dataset['data'], binary_train.dataset['label']\n",
        "X_val, y_val = binary_val.dataset['data'], binary_val.dataset['label']\n",
        "\n",
        "# Normalization\n",
        "X_train = normalize(X_train)\n",
        "X_val = normalize(X_val)\n",
        "\n",
        "classifier = KNNClassifier(X_train, y_train)\n",
        "classifier.check_accuracy(X_val, y_val, k=1)"
      ],
      "metadata": {
        "id": "y1j9Z3iFsVge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets increase to k=10. You should see a slightly higher accuracy than k=1:"
      ],
      "metadata": {
        "id": "GuE4aOzMsibr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "classifier.check_accuracy(X_val, y_val, k=10)"
      ],
      "metadata": {
        "id": "sE6uyBOkskJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3.3 Cross-validation (5 pts)\n",
        "We have not implemented the full k-Nearest Neighbor classifier. We will use **cross-validation** to set this hyperparameter in a more principled manner.\n",
        "\n",
        "Now, implement the function `knn_cross_validate` to perform cross-validation on k."
      ],
      "metadata": {
        "id": "zxPB6BiqtcQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def knn_cross_validate(x_train, y_train, num_folds=5, k_choices=None):\n",
        "  \"\"\"\n",
        "  Perform cross-validation for KnnClassifier.\n",
        "\n",
        "  Inputs:\n",
        "  - x_train: Tensor of shape (num_train, C, H, W) giving all training data\n",
        "  - y_train: int64 tensor of shape (num_train,) giving labels for training data\n",
        "  - num_folds: Integer giving the number of folds to use\n",
        "  - k_choices: List of integers giving the values of k to try\n",
        "\n",
        "  Returns:\n",
        "  - k_to_accuracies: Dictionary mapping values of k to lists, where\n",
        "    k_to_accuracies[k][i] is the accuracy on the ith fold of a KnnClassifier\n",
        "    that uses k nearest neighbors.\n",
        "  \"\"\"\n",
        "  if k_choices is None:\n",
        "    # Use default values\n",
        "    k_choices = [1, 3, 5, 8, 10, 12, 15, 20, 50, 100, 150, 200]\n",
        "\n",
        "  # First we divide the training data into num_folds equally-sized folds.\n",
        "  x_train_folds = []\n",
        "  y_train_folds = []\n",
        "  ##############################################################################\n",
        "  # TODO: Split the training data and images into folds. After splitting,      #\n",
        "  # x_train_folds and y_train_folds should be lists of length num_folds, where #\n",
        "  # y_train_folds[i] is the label vector for images in x_train_folds[i].       #\n",
        "  # Hint: torch.chunk                                                          #\n",
        "  ##############################################################################\n",
        "  \n",
        "\n",
        "\n",
        "  ##############################################################################\n",
        "  #                            END OF YOUR CODE                                #\n",
        "  ##############################################################################\n",
        "\n",
        "  # A dictionary holding the accuracies for different values of k that we find\n",
        "  # when running cross-validation. After running cross-validation,\n",
        "  # k_to_accuracies[k] should be a list of length num_folds giving the different\n",
        "  # accuracies we found when trying KnnClassifiers that use k neighbors.\n",
        "  k_to_accuracies = {}\n",
        "\n",
        "  ##############################################################################\n",
        "  # TODO: Perform cross-validation to find the best value of k. For each value #\n",
        "  # of k in k_choices, run the k-nearest-neighbor algorithm num_folds times;   #\n",
        "  # in each case you'll use all but one fold as training data, and use the     #\n",
        "  # last fold as a validation set. Store the accuracies for all folds and all  #\n",
        "  # values in k in k_to_accuracies.   HINT: torch.cat                          #\n",
        "  ##############################################################################\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "  ##############################################################################\n",
        "  #                            END OF YOUR CODE                                #\n",
        "  ##############################################################################\n",
        "\n",
        "  return k_to_accuracies"
      ],
      "metadata": {
        "id": "mx41812Mtkq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "X_train, y_train = binary_train.dataset['data'], binary_train.dataset['label']\n",
        "# Normalization\n",
        "X_train = normalize(X_train)\n",
        "\n",
        "# Shuffle the data.\n",
        "np.random.seed(0)\n",
        "random_perm = np.arange(len(X_train))\n",
        "np.random.shuffle(random_perm)\n",
        "X_train, y_train = X_train[random_perm], y_train[random_perm]\n",
        "\n",
        "k_to_accuracies = knn_cross_validate(X_train, y_train, num_folds=5)\n",
        "\n",
        "for k, accs in sorted(k_to_accuracies.items()):\n",
        "  print('k = %d got accuracies: %r' % (k, accs))"
      ],
      "metadata": {
        "id": "TkBQpPJotZrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ks, means, stds = [], [], []\n",
        "torch.manual_seed(0)\n",
        "for k, accs in sorted(k_to_accuracies.items()):\n",
        "  plt.scatter([k] * len(accs), accs, color='g')\n",
        "  ks.append(k)\n",
        "  means.append(np.mean(accs))\n",
        "  stds.append(np.std(accs))\n",
        "plt.errorbar(ks, means, yerr=stds)\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Cross-validation accuracy')\n",
        "plt.title('Cross-validation on k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fdTxqt74vk4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, implement a greedy policy to find the best k value for the KNN classifier in our case."
      ],
      "metadata": {
        "id": "sKacRhwdzNYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def knn_get_best_k(k_to_accuracies):\n",
        "  \"\"\"\n",
        "  Select the best value for k, from the cross-validation result from\n",
        "  knn_cross_validate. If there are multiple k's available, then you SHOULD\n",
        "  choose the smallest k among all possible answer.\n",
        "\n",
        "  Inputs:\n",
        "  - k_to_accuracies: Dictionary mapping values of k to lists, where\n",
        "    k_to_accuracies[k][i] is the accuracy on the ith fold of a KnnClassifier\n",
        "    that uses k nearest neighbors.\n",
        "\n",
        "  Returns:\n",
        "  - best_k: best (and smallest if there is a conflict) k value based on\n",
        "            the k_to_accuracies info\n",
        "  \"\"\"\n",
        "  best_k = 0\n",
        "  ##############################################################################\n",
        "  # TODO: Use the results of cross-validation stored in k_to_accuracies to     #\n",
        "  # choose the value of k, and store the result in best_k. You should choose   #\n",
        "  # the value of k that has the highest mean accuracy accross all folds.       #\n",
        "  ##############################################################################\n",
        "  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  ##############################################################################\n",
        "  #                            END OF YOUR CODE                                #\n",
        "  ##############################################################################\n",
        "  return best_k"
      ],
      "metadata": {
        "id": "oYSBOZZtzJQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use the results of cross-validation to select the best value for k, and rerun the classifier on our full set of training examples. \n",
        "(I can get an accuracy of 69.4%. Try to beat me!)"
      ],
      "metadata": {
        "id": "qdiEMtnxzEvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "X_train, y_train = binary_train.dataset['data'], binary_train.dataset['label']\n",
        "X_val, y_val = binary_val.dataset['data'], binary_val.dataset['label']\n",
        "# Normalization\n",
        "X_train = normalize(X_train)\n",
        "X_val = normalize(X_val)\n",
        "\n",
        "best_k = knn_get_best_k(k_to_accuracies)    \n",
        "print('Best k is ', best_k)\n",
        "\n",
        "\n",
        "classifier = KNNClassifier(X_train, y_train)\n",
        "classifier.check_accuracy(X_val, y_val, k=best_k)"
      ],
      "metadata": {
        "id": "HxSFqDlQzETX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3.4 Feature Extraction (5 pts)\n",
        "\n",
        "Now, instead of using raw pixel values as input, we will extract the features from the images. We use histograms of colors as features. \n",
        "\n",
        "To do that, we first define a function to extract the color histogram feature from a 3072-d vector, which is reshaped from an RGB image of the shape (32, 32,3).\n",
        "\n",
        "We take the following steps to implement:\n",
        "\n",
        "1. Reshape the 3072-d vector into an image of shape (32, 32, 3).\n",
        "2. Compute the histogram of the image using the `cv2.calcHist` function from the OpenCV library. This function takes in the image and a list of channels, and returns a histogram for each channel.\n",
        "3. Normalize the histogran for each channel.\n",
        "4. Concatenate the histograms of all channels into a single feature vector. \n",
        "5. Return the concatenated histogram as the color histogram feature.\n",
        "\n",
        "Here is an example implementation:"
      ],
      "metadata": {
        "id": "-p13R_S50IyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def extract_color_histogram_feature(image_vector):\n",
        "    # Assume the input vector is torch.tensor in cuda.\n",
        "    image_vector = image_vector.cpu().numpy()\n",
        "    # Reshape the image vector into an image of shape (32, 32, 3)\n",
        "    image = np.reshape(image_vector, (32, 32, 3))\n",
        "    # Compute the histogram for each channel\n",
        "    histograms = []\n",
        "    for channel in range(3):\n",
        "        histogram = cv2.calcHist([image], [channel], None, [256], [0, 256])\n",
        "        histograms.append(histogram/histogram.max())\n",
        "    # Concatenate the histograms of all channels into a single feature vector\n",
        "    histogram_feature = np.concatenate(histograms)\n",
        "    \n",
        "    return histogram_feature\n"
      ],
      "metadata": {
        "id": "STfwGrM-WgTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Likewise, we will first perform cross validation"
      ],
      "metadata": {
        "id": "Sjc0UXVR4aUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "X_train, y_train = binary_train.dataset['data'], binary_train.dataset['label']\n",
        "X_train = torch.tensor([extract_color_histogram_feature(x) for x in X_train])\n",
        "\n",
        "# Shuffle the data.\n",
        "np.random.seed(0)\n",
        "random_perm = np.arange(len(X_train))\n",
        "np.random.shuffle(random_perm)\n",
        "X_train, y_train = X_train[random_perm], y_train[random_perm]\n",
        "\n",
        "k_to_accuracies = knn_cross_validate(X_train, y_train, num_folds=5)\n",
        "\n",
        "for k, accs in sorted(k_to_accuracies.items()):\n",
        "  print('k = %d got accuracies: %r' % (k, accs))"
      ],
      "metadata": {
        "id": "63L44b7M4VW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using features, I can achieve 72.4% accuracy. What about you?"
      ],
      "metadata": {
        "id": "YoB-swL75uN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "X_train, y_train = binary_train.dataset['data'], binary_train.dataset['label']\n",
        "X_train = torch.tensor([extract_color_histogram_feature(x) for x in X_train])\n",
        "\n",
        "X_val, y_val = binary_val.dataset['data'], binary_val.dataset['label']\n",
        "X_val = torch.tensor([extract_color_histogram_feature(x) for x in X_val])\n",
        "\n",
        "best_k = knn_get_best_k(k_to_accuracies)    \n",
        "print('Best k is ', best_k)\n",
        "\n",
        "\n",
        "classifier = KNNClassifier(X_train, y_train)\n",
        "classifier.check_accuracy(X_val, y_val, k=best_k)"
      ],
      "metadata": {
        "id": "3Iz6FuTz5cpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Write down your understanding of feature extraction, and how it affects the performances (5 pts)"
      ],
      "metadata": {
        "id": "AdVe7YDh6o1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Answer**: [Write down your answer here]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yzdEu2766v39"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3.5 KNN (Multi-Class Classification)\n",
        "\n",
        "Now we we will switch to multi-class classification. Instead of classifying indoor-outdoor scenes, we will have all the 20 sub-categories for classification.\n",
        "\n",
        "You do not need to change your implementations here. Just set ``binary`` to ``False`` when calling tinyplaces loader. \n",
        "\n",
        "We will also go through cross validation first."
      ],
      "metadata": {
        "id": "WlP1ESy_6xpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "X_train, y_train = multi_train.dataset['data'], multi_train.dataset['label']\n",
        "X_val, y_val = multi_val.dataset['data'], multi_val.dataset['label']\n",
        "# Normalization\n",
        "X_train = normalize(X_train)\n",
        "X_val = normalize(X_val)\n",
        "\n",
        "# Shuffle the data.\n",
        "np.random.seed(0)\n",
        "random_perm = np.arange(len(X_train))\n",
        "np.random.shuffle(random_perm)\n",
        "X_train, y_train = X_train[random_perm], y_train[random_perm]\n",
        "\n",
        "k_to_accuracies = knn_cross_validate(X_train, y_train, num_folds=5)\n",
        "\n",
        "for k, accs in sorted(k_to_accuracies.items()):\n",
        "  print('k = %d got accuracies: %r' % (k, accs))"
      ],
      "metadata": {
        "id": "AByNzjGD56nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_k = knn_get_best_k(k_to_accuracies)    \n",
        "print('Best k is ', best_k)\n",
        "\n",
        "classifier = KNNClassifier(X_train, y_train)\n",
        "classifier.check_accuracy(X_val, y_val, k=best_k)"
      ],
      "metadata": {
        "id": "MDESnaGj7VA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I get only around 14.7% accuracy based on the naive implementation. How about you?"
      ],
      "metadata": {
        "id": "Ns0IWZpQkXVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bonus (Optional, 10 pts)\n",
        "\n",
        "Try to reach 20% accuracy for multi-class KNN classification.\n",
        "\n",
        "Explain your solution and present your code in the following cells."
      ],
      "metadata": {
        "id": "bqWtkYe8kvi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your answer:"
      ],
      "metadata": {
        "id": "hgVvdICw-Q2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mK3aPIDvkU4b"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
