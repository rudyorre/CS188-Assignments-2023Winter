{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb2eovi0nfh9"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "Welcome to the second assignment! We're excited to see what you'll create using the techniques you've learned in this course.\n",
        "\n",
        "First of all, please type your name and UID in the following format:\n",
        "\n",
        "Firstname Lastname, #UID\n",
        "\n",
        "For example: Sicheng Mo, #401234567"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPW1ZnKxnkua"
      },
      "outputs": [],
      "source": [
        "#@title Your Info { display-mode: \"form\" }\n",
        "\n",
        "Name = '' #@param {type:\"string\"}\n",
        "UID = '' #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbqmTtixn70A"
      },
      "source": [
        "### Goals\n",
        "The goals of this assignment are to:\n",
        "\n",
        "1. Implement a PyTorch data loader to load and preprocess the MiniPlaces dataset.\n",
        "2. Build and train a fully-connected neural network from scratch and using built-in PyTorch modules.\n",
        "3. Build and train a convolutional neural network from scratch and using built-in PyTorch modules.\n",
        "4. Come up with your own model and compete on a test leaderboard for extra credit.\n",
        "\n",
        "By the end of this assignment, you will have gained experience with:\n",
        "\n",
        "- Working with PyTorch and the MiniPlaces dataset for image classification.\n",
        "- Implementing and training different types of neural networks using PyTorch.\n",
        "- Debugging and troubleshooting issues that may arise during the development process.\n",
        "\n",
        "Please note that it may take some time to run the entire notebook and prepare the submission version. Make sure to allocate enough time for this task and start early. If you have any questions or run into any issues, please feel free to raise them in the Piazza forum or search the internet for debugging purposes. However, please do not directly copy code from other sources.\n",
        "\n",
        "This assignment is due on ***Feb 12th***.\n",
        "\n",
        "**Do not use any Code AI to finish the assignment.**\n",
        "\n",
        "\n",
        "Good luck and happy coding! Remember, the most important thing is to have fun and learn something new.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Uvbsv8TolDv"
      },
      "source": [
        "## Q1: Data Loader (20 pts)\n",
        "\n",
        "In this part, you will implement a PyTorch data loader to load and preprocess the MiniPlaces dataset.\n",
        "\n",
        "To begin, you will need to download the MiniPlaces dataset using the provided link. \n",
        "\n",
        "-----\n",
        "\n",
        "### Q1.0: Data prepartion (0 pts)\n",
        "\n",
        "Recall the introduction about the storage system of CoLab we went through in the assignment 1. For efficient development of our models, we will still use the temporary storage space to hold our data. This means that every time you open up this notebook, we will need to re-download and process the dataset. Don't worry though - this shouldn't take long, usually just a minute or less. Okay, let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTrafXmEq2Pp"
      },
      "source": [
        "As in the assignment 1, we download the Miniplaces Dataset from its google drive file ID.\n",
        "\n",
        "Downloading this dataset only takes less than 15 seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCASvPcijiBs"
      },
      "outputs": [],
      "source": [
        "# Downloading this file takes about a few seconds.\n",
        "# Download the tar.gz file from google drive using its file ID.\n",
        "!pip3 install --upgrade gdown --quiet\n",
        "!gdown 16GYHdSWS3iMYwMPv5FpeDZN2rH7PR0F2 # this is the file ID of miniplaces dataset\n",
        "# back-up commands (try the following it previous file id is overload)\n",
        "# !gdown 1CyIQOJienhNITwGcQ9h-nv8z6GOjV2HX\n",
        "# !wget https://web.cs.ucla.edu/~smo3/data.tar.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VccA5K7OltGS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "import urllib.request\n",
        "\n",
        "def setup(file_link_dict={},\n",
        "          folder_name='Assignment2'):\n",
        "  # Let's make our assignment directory\n",
        "  CS188_path = './'\n",
        "  os.makedirs(os.path.join(CS188_path, 'Assignment2', 'data'), exist_ok=True)\n",
        "  # Now, let's specify the assignment path we will be working with as the root.\n",
        "  root_dir = os.path.join(CS188_path, 'Assignment2')\n",
        "  # Open the tar.gz file\n",
        "  tar = tarfile.open(\"data.tar.gz\", \"r:gz\")\n",
        "  # Extract the file \"./Assignment2/data\" folder\n",
        "  total_size = sum(f.size for f in tar.getmembers())\n",
        "  with tqdm(total=total_size, unit=\"B\", unit_scale=True, desc=\"Extracting tar.gz file\") as pbar:\n",
        "      for member in tar.getmembers():\n",
        "          tar.extract(member, os.path.join(root_dir, 'data'))\n",
        "          pbar.update(member.size)\n",
        "  # Close the tar.gz file\n",
        "  tar.close()\n",
        "  # Next, we download the train/val/test txt files:\n",
        "  for file_name, file_link in file_link_dict.items():\n",
        "      print(f'Downloding {file_name}.txt from {file_link}')\n",
        "      urllib.request.urlretrieve(file_link, f'{root_dir}/data/{file_name}.txt')\n",
        "  return root_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exQ_3kzoj5a6"
      },
      "outputs": [],
      "source": [
        "val_url = 'https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/val.txt'\n",
        "train_url = 'https://raw.githubusercontent.com/CSAILVision/miniplaces/master/data/train.txt'\n",
        "root_dir = setup(\n",
        "    file_link_dict={'train':train_url, 'val':val_url},\n",
        "    folder_name='Assignment2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8la5tXIqqyqh"
      },
      "source": [
        "If you are curious, however, here is an example output if you choose to extract the files to Google Drive:\n",
        "\n",
        "```\n",
        "Extracting tar.gz file: 100%|██████████| 566M/566M [21:49<00:00, 432kB/s] \n",
        "```\n",
        "\n",
        "In contrast, extracting files to temporary workspace only takes ~20-40 seconds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzAcB9mrKaor"
      },
      "source": [
        "### Q1.1 Data Transform (10 pts)\n",
        "\n",
        "In this sub-question, you will be loading a sample image from the MiniPlaces dataset and applying a data transformation to it using the `torchvision.transforms.Compose` function.\n",
        "\n",
        "First, let's load a sample image from the `{root_dir}/data/images/train/a/abbey/00000001.jpg` file and print some properties about it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeFenrVsMcy5"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from PIL import Image  # we call this library \"pillow\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load sample image\n",
        "image = Image.open(f'{root_dir}/data/images/train/a/abbey/00000001.jpg')\n",
        "print(f'Data type of my image: {type(image)}')\n",
        "print(f\"Shape of image: {np.array(image).shape}\")\n",
        "print(f'Channel mode:', image.mode)\n",
        "print(f\"Value range of image: {image.getextrema()}\")\n",
        "\n",
        "# Display image using matplotlib\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kT0T8y2miHg"
      },
      "source": [
        "You should be aware of the following data properties:\n",
        "* The loaded image is in the format of \"channel-last\", with the shape of (height, width, channels), instead of (channels, height, width) which is the default in **opencv**.\n",
        "* The channel mode is RGB, meaning the channels are ordered as red, green, and blue, as opposed to BGR which is commonly used in **opencv**. \n",
        "* The value range of the data is between 0 and 255, so the data type is most likely uint8.\n",
        "\n",
        "It is important to keep in mind these subtle differences when loading images using **opencv** or **PIL**. Be mindful of these variations when working with image data. **After the first assignment, most of you should know the difference between RGB and BGR images when visulizing.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb60qWrhNGm0"
      },
      "source": [
        "In addition to the sample image, you can also experiment with loading other images from the dataset for further exploration.\n",
        "\n",
        "---\n",
        "\n",
        "All images in the MiniPlaces dataset have a shape of (128, 128), and have a `uint8` (0~255) data type. \n",
        "\n",
        "Next, you should define a data transform function using torchvision.transforms.Compose that resizes the image to 64x64, converts it to a tensor, and normalizes it using ImageNet statistics, i.e., `mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjPcbrCQOH-q"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Define data transformation\n",
        "data_transform = transforms.Compose([\n",
        "    ################# Your Implementations #####################################\n",
        "    # TODO: Resize image to 64x64\n",
        "    pass\n",
        "\n",
        "    ################# End of your Implementations ##############################\n",
        "    transforms.ToTensor(),\n",
        "\n",
        "\n",
        "    ################# Your Implementations #####################################\n",
        "    # TODO: Normalize image using ImageNet statistics\n",
        "    pass\n",
        "    \n",
        "    ################# End of your Implementations ##############################\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je4SPUnDOwWC"
      },
      "source": [
        "Let's see how `data_transform` works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZRrhcXqOv2F"
      },
      "outputs": [],
      "source": [
        "transformed_img = data_transform(image)\n",
        "print('shape:', transformed_img.shape)\n",
        "print('values:', transformed_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox1GrQPnMWD4"
      },
      "source": [
        "You should get results similar to the following if you're using the same image (`f'{root_dir}/data/images/train/a/abbey/00000001.jpg`):\n",
        "\n",
        "```\n",
        "shape: torch.Size([3, 64, 64])\n",
        "values: tensor([[[-1.8782, -1.8097, -1.4672,  ..., -1.8953, -1.9638, -1.9980],\n",
        "         [-1.7069, -1.6384, -1.4158,  ..., -1.9124, -1.9809, -1.9980],\n",
        "         [-1.4843, -1.4500, -1.4158,  ..., -1.6898, -1.9124, -1.9638],      \n",
        "         ...,\n",
        "         [ 1.1237,  0.9668,  1.0539,  ..., -0.2532,  0.2522, -0.0267],\n",
        "         [ 0.8274,  0.5485,  0.7054,  ..., -0.9853, -0.6890, -0.6890],\n",
        "         [-0.1138, -0.5147, -0.4624,  ..., -1.1770, -1.2293, -0.9504]]])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz_XWRqGp48c"
      },
      "source": [
        "Let's take a look at the `transforms.ToTensor()` only."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62d4VB2ip-Wk"
      },
      "outputs": [],
      "source": [
        "print('original image:',\n",
        "      f'  type: {type(image)}',\n",
        "      f'  shape: {np.array(image).shape}',\n",
        "      f'  values: max={np.max(image)}, min={np.min(image)}',\n",
        "      sep='\\n')\n",
        "\n",
        "_t_img = transforms.ToTensor()(image)\n",
        "print('after ToTensor transformation:',\n",
        "      f'  type: {type(_t_img)}',\n",
        "      f'  shape: {_t_img.shape}',\n",
        "      f'  values: max={_t_img.max()}, min={_t_img.min()}',\n",
        "      sep='\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv-_atd5omc8"
      },
      "source": [
        "In 1-2 sentences, explain in your own words what the `transforms.ToTensor()` function does."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MrI50zsovM9"
      },
      "source": [
        "(Double click to edit)\n",
        "\n",
        "Your answer:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms--ngTbP_it"
      },
      "source": [
        "After defining the data transform function, you can use it to transform the sample image and visualize the result. Here is an example.\n",
        "\n",
        "\n",
        "Note that the `imshow` function expects the image to be in the format `(height, width, channels)`, but the transformed image has the shape `(channels, height, width)`. We can use the permute method to rearrange the dimensions of the transformed image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt_wMo5dQUeO"
      },
      "outputs": [],
      "source": [
        "plt.subplot(1,2,1)\n",
        "plt.imshow(image.resize((64,64)))\n",
        "plt.title('original')\n",
        "plt.axis('off')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(transformed_img.permute(1, 2, 0))\n",
        "plt.title('transformed')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ2czm6_Qora"
      },
      "source": [
        "You will find that the transformed image is significantly distorted in terms of color.\n",
        "\n",
        "This is caused by the normalization step. Therefore, we need to undo this step to recover the original image.\n",
        "\n",
        "To do this, let's write a `tensor_to_image` function that inverts the data transformation process. This function should take a tensor image as input and return a displayable image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mp7t5IhHRLlB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "def tensor_to_image(image):\n",
        "    \"\"\"\n",
        "    Convert a tensor image back to a displayable image.\n",
        "    \n",
        "    Args:\n",
        "        image (torch.Tensor): Tensor image to convert.\n",
        "        \n",
        "    Returns:\n",
        "        PIL.Image: Displayable image.\n",
        "    \"\"\"\n",
        "    tensor_image = image.clone().detach()\n",
        "    # TODO: transform the tensor_image into a numpy_image that is displayable by plt.\n",
        "    ################# Your Implementations #####################################\n",
        "    # Hints: Transpose tensor image to (64, 64, 3)\n",
        "    #         Undo normalization\n",
        "    #         Convert tensor image to numpy array\n",
        "    pass\n",
        "\n",
        "    \n",
        "    ################# End of your Implementations ##############################\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prU73Qv_Rqcn"
      },
      "outputs": [],
      "source": [
        "# Let's see the results.\n",
        "fig = plt.figure(figsize=(10,3))\n",
        "plt.subplot(1,3,1)\n",
        "plt.title('original')\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.subplot(1,3,2)\n",
        "plt.title('original resized')\n",
        "plt.imshow(image.resize((64,64)))\n",
        "plt.axis('off')\n",
        "plt.subplot(1,3,3)\n",
        "plt.imshow(tensor_to_image(transformed_img))\n",
        "plt.title('transformed')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz1Z2zxZsr0D"
      },
      "source": [
        "### Q1.2 MiniPlaces Dataloader (10 pts)\n",
        "\n",
        "You will implement a pytorch data loader for the MiniPlaces dataset, following the tutorial [here](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/57a471142057f27da635118e88a99bf6/data_tutorial.ipynb\n",
        ") or referring to our discussion 2.\n",
        "\n",
        "Complete the following cell and debug it if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6zWsdy_sZD6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class MiniPlaces(Dataset):\n",
        "    def __init__(self, root_dir, split, transform=None, label_dict=None):\n",
        "        \"\"\"\n",
        "        Initialize the MiniPlaces dataset with the root directory for the images, \n",
        "        the split (train/val/test), an optional data transformation, \n",
        "        and an optional label dictionary.\n",
        "        \n",
        "        Args:\n",
        "            root_dir (str): Root directory for the MiniPlaces images.\n",
        "            split (str): Split to use ('train', 'val', or 'test').\n",
        "            transform (callable, optional): Optional data transformation to apply to the images.\n",
        "            label_dict (dict, optional): Optional dictionary mapping integer labels to class names.\n",
        "        \"\"\"\n",
        "        assert split in ['train', 'val', 'test']\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.filenames = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Take a second to think why we need this line.\n",
        "        # Hints: training set / validation set / test set.\n",
        "        self.label_dict = label_dict if label_dict is not None else {}\n",
        "\n",
        "        # You should\n",
        "        #   1. Load the train/val/test text file based on the `split` argument and\n",
        "        #     store the image filenames and labels.\n",
        "        #   2. Extract the class names from the image filenames and store them in \n",
        "        #     self.label_dict.\n",
        "        #   3. Construct a label dict that maps integer labels to class names, if \n",
        "        #     the current split is \"train\" \n",
        "        ################# Your Implementations #####################################\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ################# End of your Implementations ##############################\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the number of images in the dataset.\n",
        "        \n",
        "        Returns:\n",
        "            int: Number of images in the dataset.\n",
        "        \"\"\"\n",
        "        dataset_len = 0\n",
        "        ################# Your Implementations #####################################\n",
        "        # Return the number of images in the dataset\n",
        "        pass\n",
        "\n",
        "        ################# End of your Implementations ##############################\n",
        "        return dataset_len\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Return a single image and its corresponding label when given an index.\n",
        "        \n",
        "        Args:\n",
        "            idx (int): Index of the image to retrieve.\n",
        "            \n",
        "        Returns:\n",
        "            tuple: Tuple containing the image and its label.\n",
        "        \"\"\"\n",
        "        ################# Your Implementations #####################################\n",
        "        # Load and preprocess image using self.root_dir, \n",
        "        # self.filenames[idx], and self.transform (if specified)\n",
        "        pass \n",
        "\n",
        "        ################# End of your Implementations ##############################\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGDMSfLHzD5L"
      },
      "outputs": [],
      "source": [
        "data_root = os.path.join(root_dir, 'data')\n",
        "# Create MiniPlaces dataset object\n",
        "miniplaces_train = MiniPlaces(data_root, split='train', transform=data_transform)\n",
        "\n",
        "# Check our implementation\n",
        "print('len of trainining dataset:', len(miniplaces_train))\n",
        "print('label_dict:', miniplaces_train.label_dict)\n",
        "random_idxs = np.random.choice(len(miniplaces_train), 3)\n",
        "print('Example filenames:', [miniplaces_train.filenames[i] for i in random_idxs])\n",
        "print('Example class IDs:', [miniplaces_train.labels[i] for i in random_idxs])\n",
        "print('Example class names:', \n",
        "      [miniplaces_train.label_dict[miniplaces_train.labels[i]] for i in random_idxs])\n",
        "print()\n",
        "\n",
        "miniplaces_val = MiniPlaces(data_root, split='val', transform=data_transform)\n",
        "print('val label_dict:', miniplaces_val.label_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqcIrcMW2l5d"
      },
      "source": [
        "If you have successfully implemented the dataset, the above code should give you results like this:\n",
        "\n",
        "\n",
        "```\n",
        "len of trainining dataset: 100000\n",
        "label_dict: {0: 'abbey', 1: 'airport_terminal', 2: 'amphitheater', 3: 'amusement_park', 4: 'aquarium', 5: 'aqueduct', 6: 'art_gallery', 7: 'assembly_line', 8: 'auditorium', 9: 'badlands', 10: 'bakery', 11: 'ballroom', 12: 'bamboo_forest', 13: 'banquet_hall', 14: 'bar', 15: 'baseball_field', 16: 'bathroom', 17: 'beauty_salon', 18: 'bedroom', 19: 'boat_deck', 20: 'bookstore', 21: 'botanical_garden', 22: 'bowling_alley', 23: 'boxing_ring', 24: 'bridge', 25: 'bus_interior', 26: 'butchers_shop', 27: 'campsite', 28: 'candy_store', 29: 'canyon', 30: 'cemetery', 31: 'chalet', 32: 'church', 33: 'classroom', 34: 'clothing_store', 35: 'coast', 36: 'cockpit', 37: 'coffee_shop', 38: 'conference_room', 39: 'construction_site', 40: 'corn_field', 41: 'corridor', 42: 'courtyard', 43: 'dam', 44: 'desert', 45: 'dining_room', 46: 'driveway', 47: 'fire_station', 48: 'food_court', 49: 'fountain', 50: 'gas_station', 51: 'golf_course', 52: 'harbor', 53: 'highway', 54: 'hospital_room', 55: 'hot_spring', 56: 'ice_skating_rink', 57: 'iceberg', 58: 'kindergarden_classroom', 59: 'kitchen', 60: 'laundromat', 61: 'lighthouse', 62: 'living_room', 63: 'lobby', 64: 'locker_room', 65: 'market', 66: 'martial_arts_gym', 67: 'monastery', 68: 'mountain', 69: 'museum', 70: 'office', 71: 'palace', 72: 'parking_lot', 73: 'phone_booth', 74: 'playground', 75: 'racecourse', 76: 'railroad_track', 77: 'rainforest', 78: 'restaurant', 79: 'river', 80: 'rock_arch', 81: 'runway', 82: 'shed', 83: 'shower', 84: 'ski_slope', 85: 'skyscraper', 86: 'slum', 87: 'stadium', 88: 'stage', 89: 'staircase', 90: 'subway_station', 91: 'supermarket', 92: 'swamp', 93: 'swimming_pool', 94: 'temple', 95: 'track', 96: 'trench', 97: 'valley', 98: 'volcano', 99: 'yard'}\n",
        "Example filenames: ['train/b/boat_deck/00000794.jpg', 'train/b/bookstore/00000781.jpg', 'train/s/swamp/00000783.jpg']\n",
        "Example class IDs: [19, 20, 92]\n",
        "Example class names: ['boat_deck', 'bookstore', 'swamp']\n",
        "\n",
        "val label_dict: {}\n",
        "```\n",
        "\n",
        "----\n",
        "Since validation and test annotation files do not have class information, we should pass the `label_dict` obtained from the training set to them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VptHU1eq0lFd"
      },
      "outputs": [],
      "source": [
        "miniplaces_val = MiniPlaces(\n",
        "    data_root, split='val', \n",
        "    transform=data_transform, \n",
        "    label_dict=miniplaces_train.label_dict)\n",
        "\n",
        "print('val label_dict:', miniplaces_val.label_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm2YhzKD3HZt"
      },
      "source": [
        "Now you should see the same `label_dict` as the tranining set.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlVwjsmw3R81"
      },
      "source": [
        "Next, let's visualize the image in each category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLl0iSJE3GaO"
      },
      "outputs": [],
      "source": [
        "figure = plt.figure(figsize=(24, 24))\n",
        "cols, rows = 10, 10\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(miniplaces_train), size=(1,)).item()\n",
        "    img, label = miniplaces_train[sample_idx]\n",
        "    class_name = miniplaces_train.label_dict[label]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(class_name)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(tensor_to_image(img))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ojkv0XuJXzj"
      },
      "source": [
        "## Q2: MLP Classifier (30 pts)\n",
        "\n",
        "We will implement an MLP from scratch step by step in the following sections.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhyTcx3GTTgy"
      },
      "source": [
        "\n",
        "### Q2.1 Using built-in modules (10 pts)\n",
        "In this question, you will implement a multi-layer perceptron (MLP) classifier using PyTorch's built-in `nn.Linear` and `F.relu` functions.\n",
        "\n",
        "Please refer to [this link](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/611efa3e10bb2b546f3a33742edc4ecc/modelsyt_tutorial.ipynb)\n",
        "for model writing.\n",
        "\n",
        "First, define the MLP class and implement the `__init__` method to define the layers of the MLP:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFDgb3kKTjat"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FastMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        \"\"\"\n",
        "        Initialize an MLP classifier.\n",
        "        You can use Pytorch's built-in nn.Linear function.\n",
        "        Input and output sizes of each layer:\n",
        "          1) fc1: input_size, hidden_size\n",
        "          2) fc2: hidden_size, hidden_size\n",
        "          3) fc3: hidden_size, num_classes\n",
        "        \n",
        "        Args:\n",
        "            input_size (int): Size of the input layer.\n",
        "            hidden_size (int): Size of the hidden layer.\n",
        "            num_classes (int): Number of classes in the output layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        self.fc3 = None\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Define the layers of the MLP\n",
        "        pass\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the MLP classifier.\n",
        "        Using ReLU as the activation function after each layer, except for the output layer.\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, num_classes).\n",
        "        \"\"\"\n",
        "        # Hint: call ReLu just by F.relu, where F is imported in the top line of this cell.\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Implement the forward pass of the MLP classifier\n",
        "        pass\n",
        "        \n",
        "        ################# End of your Implementations ##########################\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G8tCcrbVqKA"
      },
      "source": [
        "\n",
        "Then, define the training and evaluation functions to train and test the MLP classifier (You don't need to modify this part):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdZoW0woXU7M"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs):\n",
        "    \"\"\"\n",
        "    Train the MLP classifier on the training set and evaluate it on the validation set every epoch.\n",
        "    \n",
        "    Args:\n",
        "        model (MLP): MLP classifier to train.\n",
        "        train_loader (torch.utils.data.DataLoader): Data loader for the training set.\n",
        "        val_loader (torch.utils.data.DataLoader): Data loader for the validation set.\n",
        "        optimizer (torch.optim.Optimizer): Optimizer to use for training.\n",
        "        criterion (callable): Loss function to use for training.\n",
        "        device (torch.device): Device to use for training.\n",
        "        num_epochs (int): Number of epochs to train the model.\n",
        "    \"\"\"\n",
        "    # Place model on device\n",
        "    model = model.to(device)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        \n",
        "        # Use tqdm to display a progress bar during training\n",
        "        with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}') as pbar:\n",
        "            for inputs, labels in train_loader:\n",
        "                # Move inputs and labels to device\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                # Zero out gradients\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                # Compute the logits and loss\n",
        "                logits = model(inputs)\n",
        "                loss = criterion(logits, labels)\n",
        "                \n",
        "                # Backpropagate the loss\n",
        "                loss.backward()\n",
        "                \n",
        "                # Update the weights\n",
        "                optimizer.step()\n",
        "                \n",
        "                # Update the progress bar\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix(loss=loss.item())\n",
        "        \n",
        "        # Evaluate the model on the validation set\n",
        "        avg_loss, accuracy = evaluate(model, val_loader, criterion, device)\n",
        "        print(f'Validation set: Average loss = {avg_loss:.4f}, Accuracy = {accuracy:.4f}')\n",
        "\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    \"\"\"\n",
        "    Evaluate the MLP classifier on the test set.\n",
        "    \n",
        "    Args:\n",
        "        model (MLP): MLP classifier to evaluate.\n",
        "        test_loader (torch.utils.data.DataLoader): Data loader for the test set.\n",
        "        criterion (callable): Loss function to use for evaluation.\n",
        "        device (torch.device): Device to use for evaluation.\n",
        "        \n",
        "    Returns:\n",
        "        float: Average loss on the test set.\n",
        "        float: Accuracy on the test set.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        total_loss = 0.0\n",
        "        num_correct = 0\n",
        "        num_samples = 0\n",
        "        \n",
        "        for inputs, labels in test_loader:\n",
        "            # Move inputs and labels to device\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Compute the logits and loss\n",
        "            logits = model(inputs)\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            _, predictions = torch.max(logits, dim=1)\n",
        "            num_correct += (predictions == labels).sum().item()\n",
        "            num_samples += len(inputs)\n",
        "            \n",
        "    # Compute the average loss and accuracy\n",
        "    avg_loss = total_loss / len(test_loader)\n",
        "    accuracy = num_correct / num_samples\n",
        "    \n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTrDC3yZWOCx"
      },
      "source": [
        "Before we really train something, seed everything."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzpX9JUDWDFK"
      },
      "outputs": [],
      "source": [
        "# Also, seed everything for reproducibility\n",
        "# code from https://gist.github.com/ihoromi4/b681a9088f348942b01711f251e5f964#file-seed_everything-py\n",
        "def seed_everything(seed: int):\n",
        "    import random, os\n",
        "    import numpy as np\n",
        "    import torch\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpHEoo1LaZxj"
      },
      "outputs": [],
      "source": [
        "# Define the device to use for training\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "if device == torch.device('cuda'):\n",
        "    print(f'Using device: {device}. Good to go!')\n",
        "else:\n",
        "    print('Please set GPU via Edit -> Notebook Settings.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1JoPZ5AayT5"
      },
      "source": [
        "Since we're using MLP, the input images should be flattened.\n",
        "We should append a flatten function to the `data_transform` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QedY3VdJawA-"
      },
      "outputs": [],
      "source": [
        "data_transform_flatten = transforms.Compose(\n",
        "    [data_transform, torch.flatten]) # pay attention to the torch.flatten operation here\n",
        "print('original transform:', data_transform)\n",
        "print('new transform:', data_transform_flatten)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1Y9X13Qcc5J"
      },
      "source": [
        "#### Bookmark: Build dataset and data loader\n",
        "\n",
        "We will also use the built-in cross-entropy loss and SGD optimizer to train the model.\n",
        "Each epoch takes approximately 2 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRE09jlZWU4f"
      },
      "outputs": [],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "model = FastMLP(\n",
        "    input_size=3 * 64 * 64, \n",
        "    hidden_size=1024, \n",
        "    num_classes=len(miniplaces_train.label_dict))\n",
        "\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(), \n",
        "    lr=0.01, \n",
        "    momentum=0.9)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Define the dataset and data transform with flatten functions appended\n",
        "data_root = os.path.join(root_dir, 'data')\n",
        "train_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='train', \n",
        "    transform=data_transform_flatten)\n",
        "\n",
        "val_dataset = MiniPlaces(\n",
        "    root_dir=data_root, split='val', \n",
        "    transform=data_transform_flatten,\n",
        "    label_dict=train_dataset.label_dict)\n",
        "\n",
        "# Define the batch size and number of workers\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "\n",
        "# Define the data loaders\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb34NMncz37N"
      },
      "source": [
        "I can achieve a validation accuracy of 11.59% after training for two epochs. How about you?\n",
        "\n",
        "It's normal for the accuracy to be low since our input has `len(dataset) * 3 * 64 * 64 = len(dataset) * 12288` values, while our model only has a few parameters (3-layer MLP) and we only trained this model for 2 epochs.\n",
        "\n",
        "----\n",
        "Training this small MLP on our dataset can take approximately 4 minutes (2 epochs) on a T4 cloud GPU, which can still be a tedious wait.\n",
        "\n",
        "Remember the trick for debugging that we used in the first assignment: subsampling the dataset to obtain a small subset. But, it's important to compare different methods under the same setting: all models should be trained using the same training set and evaluated on the same validation set.\n",
        "\n",
        "Once you are satisfied with the results, you can change back to the original dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V--FqAfxTU33"
      },
      "source": [
        "### Q2.2 Building MLP from scratch (10 pts)\n",
        "\n",
        "In the next task, you will implement your own Multi-Layer Perceptron (MLP) without using built-in PyTorch modules.\n",
        "\n",
        "This task should be relatively straightforward, as you have already implemented most of the required components in the previous assignment. You can simply copy and paste them here. However, be mindful of any changes in the input and output dimensions that may be required for this specific assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grkaTkA0TBZq"
      },
      "outputs": [],
      "source": [
        "class MyLinear(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        \"\"\"\n",
        "        Initialize a linear layer.\n",
        "        \n",
        "        Args:\n",
        "            in_features (int): Size of the input layer.\n",
        "            out_features (int): Size of the output layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # You can use the initialization methods you created in Assignment 1\n",
        "        # Here is a more common way to define learnable parameters in torch.\n",
        "        self.weight = nn.Parameter(torch.randn(in_features, out_features) * 0.01)\n",
        "        self.bias = nn.Parameter(torch.zeros(out_features))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the linear layer.\n",
        "        \n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, out_features).\n",
        "        \"\"\"\n",
        "        ################# Your Implementations #####################################\n",
        "        # TODO: Implement the forward pass of the linear layer\n",
        "        pass \n",
        "        \n",
        "        ################# End of your Implementations ##############################\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuGbaxGchImh"
      },
      "source": [
        "Recall the definition of ReLu in class. Implement the ReLU activation function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_KMrso2g3d7"
      },
      "outputs": [],
      "source": [
        "def relu(input):\n",
        "    \"\"\"\n",
        "    Apply the ReLU activation function element-wise to the input tensor.\n",
        "    \n",
        "    Args:\n",
        "        input (torch.Tensor): Input tensor with any shape.\n",
        "        \n",
        "    Returns:\n",
        "        torch.Tensor: Output tensor with the same shape as the input tensor, \n",
        "        containing the element-wise ReLU of the input tensor.\n",
        "    \"\"\"\n",
        "    outputs = None\n",
        "    ################# Your Implementations #####################################\n",
        "    pass\n",
        "    \n",
        "    ################# End of your Implementations ##############################\n",
        "    return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6OkaInQhN-G"
      },
      "source": [
        "Implement your own MLP using `MyLinear` and your defined `relu`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjvAfaIJg3Q2"
      },
      "outputs": [],
      "source": [
        "class MyMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        \"\"\"\n",
        "        Initialize an MLP classifier.\n",
        "        You can use Pytorch's built-in nn.Linear function.\n",
        "        Input and output sizes of each layer:\n",
        "          1) fc1: input_size, hidden_size\n",
        "          2) fc2: hidden_size, hidden_size\n",
        "          3) fc3: hidden_size, num_classes\n",
        "        \n",
        "        Args:\n",
        "            input_size (int): Size of the input layer.\n",
        "            hidden_size (int): Size of the hidden layer.\n",
        "            num_classes (int): Number of classes in the output layer.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        self.fc3 = None\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Define the layers of the MLP using `MyLinear`\n",
        "        pass\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the MLP classifier.\n",
        "        Using ReLU as the activation function after each layer, except for the output layer.\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape (batch_size, num_classes).\n",
        "        \"\"\"\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Implement the forward pass of the MLP classifier\n",
        "        pass\n",
        "        \n",
        "        ################# End of your Implementations ##########################\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT27e9CKhi3M"
      },
      "source": [
        "Now, test your own implementation. You should get similar results as using built-in modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-ReicHKhjkT"
      },
      "outputs": [],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "model = MyMLP(input_size=3 * 64 * 64, \n",
        "                hidden_size=1024, \n",
        "                num_classes=len(miniplaces_train.label_dict))\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# train_loader and val_loader have been declared before.\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY-Pe-6V0pl9"
      },
      "source": [
        "If your implementation is correct, you should get a similar validation accuracy (most possibly lower)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNkFEFqNh6Lf"
      },
      "source": [
        "### Cross entropy loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCRff8GAiLqh"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    # Implement the softmax function\n",
        "    # x is the input data with shape (batch_size, input_size)\n",
        "    # Return the softmax output with shape (batch_size, output_size)\n",
        "    # You can copy the one you implemented in Assignment 1\n",
        "    softmax_output = None\n",
        "    ################# Your Implementations ################################\n",
        "    pass\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "    return softmax_output\n",
        "\n",
        "def nll_loss(pred_probs, targets):\n",
        "    # Calculate the negative log likelihood loss\n",
        "    # pred_probs is the predicted probability distribution with shape (batch_size, output_size)\n",
        "    # targets is the ground-truth labels with shape (batch_size,)\n",
        "    # Return the negative log likelihood loss \n",
        "\n",
        "    nll_loss = None\n",
        "    # Hine: Convert the ground-truth labels to one-hot encoding using torch.eye()\n",
        "    ################# Your Implementations ################################\n",
        "    pass\n",
        "\n",
        "    ################# End of your Implementations #################\n",
        "    return nll_loss\n",
        "\n",
        "\n",
        "class MyCrossEntropy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, logits, target):\n",
        "        \"\"\"\n",
        "        Calculate the cross entropy loss for the given input and target.\n",
        "        \n",
        "        Args:\n",
        "            logits (torch.Tensor): Input tensor with shape (batch_size, output_size).\n",
        "            target (torch.Tensor): Target tensor with shape (batch_size,).\n",
        "            \n",
        "        Returns:\n",
        "            torch.Tensor: Cross entropy loss.\n",
        "        \"\"\"\n",
        "        pred_probs = softmax(logits)\n",
        "        loss = nll_loss(pred_probs, target)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDcEtpi-jwjj"
      },
      "source": [
        "Now, test our implementation. You should get similar results as using built-in modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhrxYaJojyyZ"
      },
      "outputs": [],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "model = MyMLP(input_size=3 * 64 * 64, \n",
        "                hidden_size=1024, \n",
        "                num_classes=len(miniplaces_train.label_dict))\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Define a cross entropy loss function using the MyCrossEntropy class\n",
        "criterion = MyCrossEntropy()\n",
        "\n",
        "# train_loader and val_loader have been declared before.\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnArZ_AGhsdD"
      },
      "source": [
        "### Q2.3 SGD Optimizer (5 pts)\n",
        "\n",
        "In the next task, you will implement the Stochastic Gradient Descent (SGD) optimizer from scratch.\n",
        "\n",
        "Please note that for the purpose of this assignment, we will not be using techniques such as weight decay, damping or Nesterov. There are different variations of SGD implementations, and we have chosen this implementation for its simplicity. The pseudocode provided is a simplified version of the official implementation in PyTorch.\n",
        "\n",
        "Please follow the pseudocode provided to implement the optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ2kjQFj_-oj"
      },
      "source": [
        "\\begin{aligned}\n",
        "            &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
        "            &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n",
        "                \\text{ (objective)}, \\: \\\\\n",
        "            &\\hspace{13mm} \\:\\mu \\text{ (momentum)},\n",
        "            \\\\[-1.ex]\n",
        "            &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
        "            &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
        "            &\\hspace{5mm}\\textbf{g}_t           \\leftarrow   \\nabla_{\\theta} f_t (\\boldsymbol{\\theta}_{t-1})           \\\\\n",
        "            &\\hspace{5mm}\\textbf{if} \\: t > 1                                                   \\\\\n",
        "            &\\hspace{10mm} \\textbf{g}_t \\leftarrow \\mu \\textbf{g}_{t-1} + \\textbf{g}_t           \\\\\n",
        "            &\\hspace{5mm}\\textbf{else}                                                          \\\\\n",
        "            &\\hspace{10mm} \\textbf{g}_t \\leftarrow \\textbf{g}_t                                           \\\\\n",
        "            &\\hspace{5mm}\\boldsymbol{\\theta}_t \\leftarrow \\boldsymbol{\\theta}_{t-1} - \\gamma \\textbf{g}_t                   \\\\[-1.ex]\n",
        "            &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
        "            &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
        "            &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
        "       \\end{aligned}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVf8Mp7LlpkG"
      },
      "outputs": [],
      "source": [
        "class MySGD():\n",
        "    def __init__(self, params, lr=0.01, momentum=0.9):\n",
        "        \"\"\"\n",
        "        Initialize the MySGD optimizer.\n",
        "        Args:\n",
        "          params (iterable): An iterable of parameters to optimize.\n",
        "          lr (float, optional): learning rate. (default: 0.01)\n",
        "          momentum (float, optional): momentum factor. (default: 0.9)\n",
        "        \"\"\"\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.grad_buffer = {}\n",
        "\n",
        "    def zero_grad(self):\n",
        "        # Set the gradients of all parameters to zero.\n",
        "        for param in self.params:\n",
        "            if param.grad is not None:\n",
        "                param.grad.zero_()\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Perform one optimization step on the parameters.\n",
        "        \"\"\"\n",
        "        for i, param in enumerate(self.params):\n",
        "            if param.grad is None:\n",
        "              continue\n",
        "            # use i as a key to retrieve velocity from self.grad_buffer.\n",
        "            # use param.data to obtain its value.\n",
        "            ################# Your Implementations ################################\n",
        "            pass\n",
        "            \n",
        "            ################# End of your Implementations #################\n",
        "            # You don't need to return anything\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJbo-cW_6a5S"
      },
      "source": [
        "Now, test your own implementation. You should get similar results as using built-in modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyTETpXxod0t"
      },
      "outputs": [],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "model = MyMLP(input_size=3*64*64, \n",
        "                hidden_size=1024, \n",
        "                num_classes=len(miniplaces_train.label_dict))\n",
        "optimizer = MySGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Define a cross entropy loss function using the MyCrossEntropy class\n",
        "criterion = MyCrossEntropy()\n",
        "\n",
        "# train_loader and val_loader have been declared before.\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qALnAjsckZkm"
      },
      "source": [
        "### Q2.4 Adam Optimizer (5 pts)\n",
        "\n",
        "Now, move on to the Adam optimizer.\n",
        "\n",
        "Let's see how the Pytorch's built-in Adam works like at first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "511ZvFkM-yPq"
      },
      "outputs": [],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "\n",
        "# Your own implementation\n",
        "model = MyMLP(input_size=3 * 64 * 64, \n",
        "                hidden_size=1024, \n",
        "                num_classes=len(miniplaces_train.label_dict))\n",
        "\n",
        "######## Pytorch built-in Adam optimizer ################\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "#########################################################\n",
        "\n",
        "# Define a cross entropy loss function using the MyCrossEntropy class\n",
        "criterion = None\n",
        "########## Your own implementation ########\n",
        "pass\n",
        "\n",
        "###########################################\n",
        "\n",
        "# train_loader and val_loader have been declared before.\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8W5GFGW_WHa"
      },
      "source": [
        "I can get a validation accuracy of 6.97%. How about you?\n",
        "\n",
        "Keep in mind that a low validation accuracy is to be expected when working with a large dataset and a small model with a limited number of parameters. Additionally, the number of training epochs also plays a role in the final validation accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6dbXTo2_Kvi"
      },
      "source": [
        "----\n",
        "Next, you will implement the Adam optimizer from scratch.\n",
        "\n",
        "Note that here we don't use weight decay or amsgrad here. There are different variants of Adam implementations. We choose this implementation for simplicity. The pseudocode here is a simplified version from the official implementation in Pytorch.\n",
        "\n",
        "Follow the pseudocode provided to implement your own optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kil9ZSvtA5mS"
      },
      "source": [
        "\\begin{aligned}\n",
        "            &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
        "            &\\textbf{input}      : \\gamma \\text{ (lr)}, \\beta_1, \\beta_2\n",
        "                \\text{ (betas)},\\theta_0 \\text{ (params)},f(\\theta) \\text{ (objective)}          \\\\\n",
        "            &\\textbf{initialize} :  m_0 \\leftarrow 0 \\text{ ( first moment)},\n",
        "                v_0\\leftarrow 0 \\text{ (second moment)}\\\\[-1.ex]\n",
        "            &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
        "            &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
        "            &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n",
        "            &\\hspace{5mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n",
        "            &\\hspace{5mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n",
        "            &\\hspace{5mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\n",
        "            &\\hspace{5mm}\\widehat{v_t} \\leftarrow   v_t/\\big(1-\\beta_2^t \\big)                   \\\\\n",
        "            &\\hspace{5mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/\n",
        "                \\big(\\sqrt{\\hat{v_t}} + \\epsilon \\big)                                       \\\\\n",
        "            &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
        "            &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
        "            &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
        "       \\end{aligned}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i_C7PKvqt_D"
      },
      "outputs": [],
      "source": [
        "class MyAdam():\n",
        "    def __init__(self, params, lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        \"\"\"\n",
        "        Initialize the MyAdam optimizer.\n",
        "        Args:\n",
        "          params (iterable): An iterable of parameters to optimize.\n",
        "          lr (float, optional): learning rate. (default: 0.01)\n",
        "          beta1 (float, optional): first moment factor. (default: 0.9)\n",
        "          beta2 (float, optional): second moment factor. (default: 0.999)\n",
        "          eps (float, optional): term to avoid division by zero. (default: 1e-8)\n",
        "        \"\"\"\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.first_moments = {}\n",
        "        self.second_moments = {}\n",
        "        # Iteration counter\n",
        "        self.t = 0\n",
        "        \n",
        "    def zero_grad(self):\n",
        "        # Set the gradients of all parameters to zero.\n",
        "        for param in self.params:\n",
        "            if param.grad is not None:\n",
        "                param.grad.zero_()\n",
        "        \n",
        "    def step(self):\n",
        "        self.t += 1\n",
        "        for i, param in enumerate(self.params):\n",
        "            if param.grad is None:\n",
        "              continue\n",
        "            # use i as a key to retrieve velocity from self.grad_buffer.\n",
        "            # use param.data to obtain its value.\n",
        "            ################# Your Implementations ################################\n",
        "            pass\n",
        "            \n",
        "            ################# End of your Implementations #################\n",
        "            # You don't need to return anything\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WepVHKxyrldN"
      },
      "outputs": [],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "model = MyMLP(input_size=3*64*64, \n",
        "                hidden_size=1024, \n",
        "                num_classes=len(miniplaces_train.label_dict))\n",
        "\n",
        "optimizer = None \n",
        "######## Your own implementation ################\n",
        "pass\n",
        "\n",
        "#################################################\n",
        "\n",
        "# Define a cross entropy loss function using the MyCrossEntropy class\n",
        "criterion = None\n",
        "########## Your own implementation ########\n",
        "pass\n",
        "\n",
        "###########################################\n",
        "\n",
        "# train_loader and val_loader have been declared before.\n",
        "# Train the model\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EZYoQc-tIJm"
      },
      "source": [
        "I got an accuracy of 7.04% using my own implementation. How about you?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z81jog_JhVg"
      },
      "source": [
        "## Q3: Convolutional Neural Network (CNN), (30 pts)\n",
        "\n",
        "In this question, you will learn how to implement convolutional neural networks (CNNs) using PyTorch. \n",
        "\n",
        "Similarly as Q2, we will first build a two-layer convolutional neural networks using PyTorch built-in function `nn.Conv2D` and then build the convolution operation from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmbxqOLytza_"
      },
      "source": [
        "### Q3.1 Fast Conv 10 pts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSck1VsjukiW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FastConv(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        input_channels, conv_hidden_channels, conv_out_channels,\n",
        "        input_size=(64,64),\n",
        "        dropout_rate1=0.25, dropout_rate2=0.5,\n",
        "        fc_out_channels=128, num_classes=100,\n",
        "        kernel_size=3, stride=1, padding=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          input_channels (int): Number of channels in the input image.\n",
        "          conv_hidden_channels (int): Number of channels in the first convolutional layer.\n",
        "          conv_out_channels (int): Number of channels in the second convolutional layer.\n",
        "          input_size (tuple, optional): Height and width of the input image. (default: (64,64))\n",
        "          dropout_rate1, dropout_rate2 (float, optional): Dropout rate for \n",
        "              the first/second dropout layer. (default: 0.25, 0.5)\n",
        "          fc_out_channels (int, optional): Number of neurons in the first fully \n",
        "              connected layer. (default: 128)\n",
        "          num_classes (int, optional): Number of classes in the final output layer. (default: 100)\n",
        "          kernel_size, stride, padding (int, optional): Parameters of convolutional layers.\n",
        "\n",
        "        Initialize a convolutional neural network.\n",
        "        You can use Pytorch's built-in nn.Conv2d function.\n",
        "        Input and output shapes of each layer:\n",
        "        1) conv1: (batch_size, input_channels, H, W) -> (batch_size, conv_hidden_channels, H, W)\n",
        "        2) conv2: (batch_size, conv_hidden_channels, H, W) -> (batch_size, conv_out_channels, H, W)\n",
        "        3) max_pooling: (batch_size, conv_out_channels, H//2, W//2)\n",
        "        4) fc1: (batch_size, flatten_size) -> (batch_size, fc_out_channels)\n",
        "        5) fc2: (batch_size, fc_out_channels) -> (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        \n",
        "        self.conv1 = None\n",
        "        self.conv2 = None\n",
        "        self.max_pooling = None\n",
        "        self.dropout1 = None\n",
        "        self.dropout2 = None\n",
        "        self.fc1 = None\n",
        "        self.fc2 = None\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Define the layers of the convolutional neural network\n",
        "        # replace \"None\"s with your implementations. \n",
        "        # All you need to do is to pass the input arguments to different constructors\n",
        "        pass\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        \n",
        "    def forward(self, x, return_intermediate=False):\n",
        "        \"\"\"\n",
        "        Forward pass of the convolutional neural network.\n",
        "        The input tensor 'x' should pass through the following layers:\n",
        "        1) conv1: (batch_size, input_channels, H, W) -> (batch_size, conv_hidden_channels, H, W)\n",
        "        2) Apply relu.\n",
        "        3) conv2: (batch_size, conv_hidden_channels, H, W) -> (batch_size, conv_out_channels, H, W)\n",
        "        4) max_pooling: Perform max pooling on the output from conv2\n",
        "        5) dropout1: Perform dropout on the output from max_pooling\n",
        "        6) Flatten the output from dropout1\n",
        "        7) fc1: Pass through a fully connected layer\n",
        "        8) dropout2: Perform dropout on the output from fc1\n",
        "        9) Apply relu.\n",
        "        7) fc2: Pass the output from the actiction layer to through a fully connected \n",
        "                layer to produce the final output\n",
        "        \"\"\"\n",
        "        ################# Your Implementations #################################\n",
        "        # TODO: Implement the forward pass of the convolutional neural network\n",
        "        pass\n",
        "\n",
        "        ################# End of your Implementations ##########################\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ppnmbcfEdAX"
      },
      "source": [
        "We're using convolutional neural network here, so the `flatten` operation that we use for the MLP is not longer needed. However, this operation has been passed when we contructed the datasets and data loaders.\n",
        "\n",
        "Therefore, we nned to define new datasets and new dataloaders.\n",
        "\n",
        "Follow the example in `Bookmark: Build dataset and data loader` to construct onstruct new data loaders `conv_train_loader` and `conv_val_loader` based on the new datasets `conv_train_dataset` and `conv_val_dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ug3ZOJj-JpPV"
      },
      "outputs": [],
      "source": [
        "# Define the dataset and data transform **without** flatten functions\n",
        "# Construct new data loaders conv_train_loader and conv_val_loader based the new datasets.\n",
        "\n",
        "# You should construct these objects\n",
        "conv_train_loader = None\n",
        "conv_val_loader = None\n",
        "conv_train_loader = None\n",
        "conv_val_loader = None\n",
        "\n",
        "################# Your Implementations #################################\n",
        "# Define the dataset\n",
        "# Define the batch size and number of workers\n",
        "# Define the data loaders\n",
        "pass\n",
        "################# End of your Implementations ##########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MafiXWN0EdYv"
      },
      "outputs": [],
      "source": [
        "seed_everything(0)\n",
        "\n",
        "# Define the model, optimizer, and criterion (loss_fn)\n",
        "model = FastConv(\n",
        "    input_channels=3, conv_hidden_channels=64, conv_out_channels=128,\n",
        "    input_size=(64,64),\n",
        "    dropout_rate1=0.25, dropout_rate2=0.5,\n",
        "    fc_out_channels=128,\n",
        "    kernel_size=3, stride=1, padding=1,\n",
        "    num_classes=len(miniplaces_train.label_dict))\n",
        "\n",
        "# Let's use the built-in optimizer for a full version of SGD optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# For loss function, your implementation and the built-in loss function should \n",
        "# be almost identical.\n",
        "criterion = MyCrossEntropy()\n",
        "\n",
        "# Train the model\n",
        "train(model, conv_train_loader, conv_val_loader, optimizer, criterion, device, num_epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z69YoNYJ0zq"
      },
      "source": [
        "I can get 15.25% accuracy after two epochs. How about you?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCK1g8RAMp-4"
      },
      "source": [
        "### Q3.2 Building 2D Convolutional layer from sratch (20pts)\n",
        "\n",
        "Next, we will build convolutional neural networks from scratch.\n",
        "\n",
        "\n",
        "Let's ignore channels for now and see how this works with two-dimensional data had hidden representations. In the following figure, the input is a two-dimensional tensor with hegith of 3 and width of 3. We mark the shape of the tensor as $3\\times3$. The height and width of the kernel are both 2. The shape of the kernel window (or convolution window) is given by the height and width of the kernel (here it is $2\\times2$). \n",
        "\n",
        "![correlation](https://web.cs.ucla.edu/~smo3/cs188/assignment2/correlation.png)\n",
        "\n",
        "\n",
        "In the two-dimensional convolution operation, we begin wiht the convolution windo position at the upper-left corner of the input tensor and slide it across the intput tensor, both from left to right and top to bottom. When the convolution window slides to a certain position, the input subtensor contained in that window and the kernel tensor are multiplied elementwise and the resulting tensor is summed up yielding a single scalar value. This result gives the value of the output tensor at the corresponding location. Here, the output tensor has a height of 2 and width of 2 and the four elements are derived from the two-dimensional cross-correlation operation:\n",
        "$$\n",
        "0\\times0+1\\times1+3\\times2+4\\times3=19,\\\\\n",
        "1\\times0+2\\times1+4\\times2+5\\times3=25,\\\\\n",
        "3\\times0+4\\times1+6\\times2+7\\times3=37,\\\\\n",
        "4\\times0+5\\times1+7\\times2+8\\times3=43.\n",
        "$$\n",
        "\n",
        "Note that along each axis, the output size\n",
        "is slightly smaller than the input size.\n",
        "Because the kernel has width and height greater than one,\n",
        "we can only properly compute the cross-correlation\n",
        "for locations where the kernel fits wholly within the image,\n",
        "the output size is given by the input size $n_h \\times n_w$\n",
        "minus the size of the convolution kernel $k_h \\times k_w$\n",
        "via\n",
        "\n",
        "$$(n_h-k_h+1) \\times (n_w-k_w+1).$$\n",
        "\n",
        "This is the case since we need enough space\n",
        "to \"shift\" the convolution kernel across the image."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q3.2.1 One Channel Convolution (10pts)\n",
        "That's start with 2D convolution with only 1 input channel and 1 kernel.\n",
        "If your implementation is right, you'll get:\n",
        "```\n",
        "tensor([[19., 25.],\n",
        "        [37., 43.]])\n",
        "```"
      ],
      "metadata": {
        "id": "gnTt09Ozf-IK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv2d(X, K):  \n",
        "    \"\"\"\n",
        "    Compute 2D convolution. You are supposed to use matrix multiplication \n",
        "    to achieve the 2D convolution instead of torch build in convlution method.\n",
        "\n",
        "        Args:\n",
        "          X (tensor): Input tensor in the shape of (H,W).\n",
        "          K (tensor): Kernel tensor in the shape of (H,W).\n",
        "\n",
        "        Return:\n",
        "          Y (tensor): Output tensor in the shape of (H,W).\n",
        "\n",
        "    \"\"\"\n",
        "    Y = None\n",
        "    ################# Start of your Implementations ##########################\n",
        "    # Hint: Using two for loops\n",
        "    pass\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "    return Y"
      ],
      "metadata": {
        "id": "AVtnL-dfgBbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to debug using this code segment:\n",
        "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
        "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
        "conv2d(X, K)"
      ],
      "metadata": {
        "id": "T67P1_UQgDBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we typically use small kernels,\n",
        "for any given convolution,\n",
        "we might only lose a few pixels,\n",
        "but this can add up as we apply\n",
        "many successive convolutional layers.\n",
        "One straightforward solution to this problem\n",
        "is to add extra pixels of filler around the boundary of our input image,\n",
        "thus increasing the effective size of the image.\n",
        "Typically, we set the values of the extra pixels to zero.\n",
        "In the following image, we pad a $3 \\times 3$ input,\n",
        "increasing its size to $5 \\times 5$.\n",
        "The corresponding output then increases to a $4 \\times 4$ matrix.\n",
        "The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\times0+0\\times1+0\\times2+0\\times3=0$.\n",
        "\n",
        "![Two-dimensional cross-correlation with padding.](https://web.cs.ucla.edu/~smo3/cs188/assignment2/conv-pad.png)\n",
        "\n",
        "In general, if we add a total of $p_h$ rows of padding\n",
        "(roughly half on top and half on bottom)\n",
        "and a total of $p_w$ columns of padding\n",
        "(roughly half on the left and half on the right),\n",
        "the output shape will be\n",
        "\n",
        "$$(n_h-k_h+p_h+1)\\times(n_w-k_w+p_w+1).$$\n",
        "\n",
        "This means that the height and width of the output\n",
        "will increase by $p_h$ and $p_w$, respectively.\n",
        "\n",
        "In many cases, we will want to set $p_h=k_h-1$ and $p_w=k_w-1$\n",
        "to give the input and output the same height and width.\n",
        "This will make it easier to predict the output shape of each layer\n",
        "when constructing the network."
      ],
      "metadata": {
        "id": "0HFbIjkOgEl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell, you will need to add stride to the conv2d function with padding.\n",
        "\n",
        "If your implementation is right, the correct output will be\n",
        "```\n",
        "tensor([[ 3.,  8.,  4.],\n",
        "        [13., 25., 10.],\n",
        "        [ 4.,  5.,  0.]])\n",
        "```"
      ],
      "metadata": {
        "id": "TM3qu0IsgGwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv2d_padding(X, K, padding=0):  \n",
        "    \"\"\"Compute 2D convolution with zeros padding. Here, we applying the same \n",
        "        padding on both top, bottom, left and right of the input tensor.\n",
        "        If the shape of X is (3,3), the size after padding 1 is (3+1x2, 3+1x2).\n",
        "\n",
        "        Args:\n",
        "          X (tensor): Input tensor in the shape of (H,W).\n",
        "          K (tensor): Kernel tensor in the shape of (H,W).\n",
        "          padding (int): padding in both dimensions\n",
        "\n",
        "        Return:\n",
        "          Y (tensor): Output tensor in the shape of (H,W).\n",
        "\n",
        "    \"\"\"\n",
        "    Y = None\n",
        "    ################# Start of your Implementations ##########################\n",
        "    # Hint： Compute the shape of padded tensor first\n",
        "    pass\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "    return Y"
      ],
      "metadata": {
        "id": "GtfccV2GgIc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to debug using this code segment:\n",
        "X = torch.tensor([[1.0, 2.0], [4.0, 5.0]])\n",
        "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
        "conv2d_padding(X, K, padding=1)"
      ],
      "metadata": {
        "id": "RUTBF5_9gJ0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When computing the cross-correlation,\n",
        "we start with the convolution window\n",
        "at the upper-left corner of the input tensor,\n",
        "and then slide it over all locations both down and to the right.\n",
        "In the previous examples, we defaulted to sliding one element at a time.\n",
        "However, sometimes, either for computational efficiency\n",
        "or because we wish to downsample,\n",
        "we move our window more than one element at a time,\n",
        "skipping the intermediate locations. This is particularly useful if the convolution \n",
        "kernel is large since it captures a large area of the underlying image.\n",
        "\n",
        "We refer to the number of rows and columns traversed per slide as *stride*.\n",
        "So far, we have used strides of 1, both for height and width.\n",
        "Sometimes, we may want to use a larger stride.\n",
        "The following image shows a two-dimensional cross-correlation operation\n",
        "with a stride of 3 vertically and 2 horizontally.\n",
        "The shaded portions are the output elements as well as the input and kernel tensor elements used for the output computation: $0\\times0+0\\times1+1\\times2+2\\times3=8$, $0\\times0+6\\times1+0\\times2+0\\times3=6$.\n",
        "We can see that when the second element of the first column is generated,\n",
        "the convolution window slides down three rows.\n",
        "The convolution window slides two columns to the right\n",
        "when the second element of the first row is generated.\n",
        "When the convolution window continues to slide two columns to the right on the input,\n",
        "there is no output because the input element cannot fill the window\n",
        "(unless we add another column of padding).\n",
        "\n",
        "![Cross-correlation with strides of 3 and 2 for height and width, respectively.](https://web.cs.ucla.edu/~smo3/cs188/assignment2/conv-stride.png)\n",
        "\n",
        "In general, when the stride for the height is $s_h$\n",
        "and the stride for the width is $s_w$, the output shape is\n",
        "\n",
        "$$\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor.$$\n"
      ],
      "metadata": {
        "id": "M1Em6cbWgLHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the next cell, you will need to add stride to the corr2d function. \n",
        "\n",
        "If your implementation is right, the correct output is\n",
        "```\n",
        "tensor([[19., 31.], \n",
        "        [50., 62.]])\n",
        "```"
      ],
      "metadata": {
        "id": "-uqxaRJ_gNXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv2d_padding_stride(X, K, padding=0,stride = 0):  \n",
        "    \"\"\"Compute 2D convolution with stride. Here the stride applied both\n",
        "       horizontally and vertically.\n",
        "\n",
        "        Args:\n",
        "          X (tensor): Input tensor in the shape of (H,W)\n",
        "          K (tensor): Kernel tensor in the shape of (H,W)\n",
        "          padding (int): padding in both dimensions\n",
        "          stride (int): Stride in both dimensions\n",
        "\n",
        "        Return:\n",
        "          Y (tensor): Output tensor in the shape of (H,W)\n",
        "    \"\"\"\n",
        "    Y = None\n",
        "    ################# Start of your Implementations ##########################\n",
        "    pass\n",
        "\n",
        "    ################# End of your Implementations ##########################\n",
        "    return Y"
      ],
      "metadata": {
        "id": "bW8tgO4fgO82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to debug using this code segment:\n",
        "X = torch.tensor([[0.0, 1.0, 2.0, 3.0], [3.0, 4.0, 5.0, 6.0], [6.0, 7.0, 8.0, 9.0], [8.0, 9.0, 10.0, 11.0]])\n",
        "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
        "conv2d_padding_stride(X, K, padding=0,stride=2)"
      ],
      "metadata": {
        "id": "AYNxh7U1gRAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q3.2.3 Multiple Channels 2D Convlution 10pts\n",
        "When the input data contains multiple channels,\n",
        "we need to construct a convolution kernel\n",
        "with the same number of input channels as the input data,\n",
        "so that it can perform cross-correlation with the input data.\n",
        "Assuming that the number of channels for the input data is $c_i$,\n",
        "the number of input channels of the convolution kernel also needs to be $c_i$. If our convolution kernel's window shape is $k_h\\times k_w$,\n",
        "then when $c_i=1$, we can think of our convolution kernel\n",
        "as just a two-dimensional tensor of shape $k_h\\times k_w$.\n",
        "\n",
        "However, when $c_i>1$, we need a kernel\n",
        "that contains a tensor of shape $k_h\\times k_w$ for *every* input channel. Concatenating these $c_i$ tensors together\n",
        "yields a convolution kernel of shape $c_i\\times k_h\\times k_w$.\n",
        "Since the input and convolution kernel each have $c_i$ channels,\n",
        "we can perform a cross-correlation operation\n",
        "on the two-dimensional tensor of the input\n",
        "and the two-dimensional tensor of the convolution kernel\n",
        "for each channel, adding the $c_i$ results together\n",
        "(summing over the channels)\n",
        "to yield a two-dimensional tensor.\n",
        "This is the result of a two-dimensional cross-correlation\n",
        "between a multi-channel input and\n",
        "a multi-input-channel convolution kernel.\n",
        "\n",
        "The following image provides an example \n",
        "of a two-dimensional cross-correlation with two input channels.\n",
        "The shaded portions are the first output element\n",
        "as well as the input and kernel tensor elements used for the output computation:\n",
        "$(1\\times1+2\\times2+4\\times3+5\\times4)+(0\\times0+1\\times1+3\\times2+4\\times3)=56$.\n",
        "\n",
        "![Cross-correlation computation with 2 input channels.](https://web.cs.ucla.edu/~smo3/cs188/assignment2/conv-multi-in.png)\n",
        "\n",
        "\n",
        "To make sure we really understand what is going on here,\n",
        "we can (**implement cross-correlation operations with multiple input channels**) ourselves.\n",
        "Notice that all we are doing is performing a cross-correlation operation\n",
        "per channel and then adding up the results."
      ],
      "metadata": {
        "id": "WGsrB1OsgSsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we'll consider the cases where there are multiple input channels. If your implementation is right, the correct output is \n",
        "\n",
        "\n",
        "```\n",
        "tensor([[  4.,  26.,  18.],\n",
        "        [ 57., 120.,  55.],\n",
        "        [ 29.,  46.,  13.]])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "nhGufyi1gUi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv2d_multi_in(X, K,padding=0, stride=0) -> torch.tensor:\n",
        "\n",
        "    \"\"\"Compute 2D cross-correlation with multiple input channels. \n",
        "        Here, X and K should have the same chanel number\n",
        "\n",
        "        Args:\n",
        "          X (tensor): Input tensor in the shape of (in_Channels,H,W).\n",
        "          K (tensor): Kernel tensor (in_Channels,H,W)\n",
        "          padding (int): padding in H,W dimensions\n",
        "          stride (int): Stride in H,W dimensions\n",
        "\n",
        "        Return:\n",
        "          Y (tensor): Output tensor in the shape of (H,W)\n",
        "\n",
        "    \"\"\"\n",
        "    # Hint： Iterate through the 0th dimension (channel) of K first, then stack them up, then take the sum\n",
        "    Y = None\n",
        "    ######################################Your Code######################################\n",
        "    pass\n",
        "    \n",
        "    #####################################################################################\n",
        "    return Y"
      ],
      "metadata": {
        "id": "0s0AcVzZgV13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to debug using this code segment:\n",
        "X = torch.tensor([[[0.0, 1.0, 2.0,3.0], [3.0, 4.0, 5.0,6.0], [6.0, 7.0, 8.0,9.0],[9.0,10.0,11.0,12.0]],\n",
        "               [[1.0, 2.0, 3.0,4.0], [4.0, 5.0, 6.0,7.0], [7.0, 8.0, 9.0,10.0],[10.0,11.0,12.0,13.0]]])\n",
        "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
        "\n",
        "out = conv2d_multi_in(X, K,padding=1, stride=2)\n",
        "out"
      ],
      "metadata": {
        "id": "XTR9cZMdgXY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare our method with torch build-in method\n",
        "X = (torch.rand(2,10,10)).to(torch.float32)\n",
        "K = (torch.rand(2,3,3)).to(torch.float32)\n",
        "for i in [1,2,3,4,5]:\n",
        "  for j in [1,2,3,4,5]:\n",
        "    out = conv2d_multi_in(X, K,padding=i, stride=j)\n",
        "    answer = F.conv2d(X.unsqueeze(0),K.unsqueeze(0),stride=j,padding=i).squeeze(0).squeeze(0)\n",
        "    equal = torch.allclose(out,answer)\n",
        "\n",
        "    if equal:\n",
        "      print('Good! For padding: %d and stride: %d, the output match' %(i,j))\n",
        "    else:\n",
        "      print('Uh-oh! The output are different')\n",
        "      break"
      ],
      "metadata": {
        "id": "3hnalmQogZS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we'll consider the cases where there are multiple input channels and multiple output channels. Notice the input tensor X has a batch demension."
      ],
      "metadata": {
        "id": "QwUiM5pMgcZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batched_conv2d_multi_in_out(X, K,padding=0, stride=0) -> torch.tensor:\n",
        "\n",
        "    \"\"\"Compute 2D cross-correlation with multiple input channels and multiple \n",
        "        output channels. \n",
        "\n",
        "        Args:\n",
        "          X (tensor): Input tensor in the shape of (batch_size,in_channels,H,W).\n",
        "          K (tensor): Kernel tensor (out_channels,in_channels,H,W)\n",
        "          padding (int): padding in H,W dimensions\n",
        "          stride (int): Stride in H,W dimensions\n",
        "\n",
        "        Return:\n",
        "          Y (tensor): Output tensor in the shape of (batch_size,out_channels,H,W)\n",
        "\n",
        "    \"\"\"\n",
        "    # Hint: Loop though the batch dimension and then loop though the out_channel\n",
        "    #       dimension \n",
        "    Y = None\n",
        "    ######################################Your Code######################################\n",
        "    pass\n",
        "\n",
        "    #####################################################################################\n",
        "    return Y"
      ],
      "metadata": {
        "id": "TRPHXLyoga04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare our method with torch build-in method\n",
        "X = (torch.rand(2,2,10,10)).to(torch.float32)\n",
        "K = (torch.rand(2,2,3,3)).to(torch.float32)\n",
        "for i in [1,2,3,4,5]:\n",
        "  for j in [1,2,3,4,5]:\n",
        "    out = batched_conv2d_multi_in_out(X, K,padding=i, stride=j)\n",
        "    answer = F.conv2d(X,K,stride=j,padding=i)\n",
        "    equal = torch.allclose(out,answer)\n",
        "\n",
        "    if equal:\n",
        "      print('Good! For padding: %d and stride: %d, the output match' %(i,j))\n",
        "    else:\n",
        "      print('Uh-oh! The output are different')\n",
        "      break"
      ],
      "metadata": {
        "id": "qbGauSqQgfMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLNVqBYnKBWQ"
      },
      "source": [
        "## Q4: Your own model (20 pts)\n",
        "\n",
        "You will construct your own model using built-in convolutional layers. You can base your model on the provided `FastConv` class, or choose to modify the number of convolutional layers, feature size, learning rate, optimizer, and other parameters to suit your needs. You should write this model in the following code cells.\n",
        "\n",
        "You will select the best model using the validation set, and then evaluate it on the test set. You should create a file named \"test_UID.json\", containing \"{id: predicted_label}\" pairs.\n",
        "\n",
        "We will hold a leaderboard on the accuracy on the test set after the deadline. The top 20% of students can receive a bonus.\n",
        "\n",
        "Feel free to organize your code in different ways. Below is an example of one possible organization for your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztTHqlCkJ4zw"
      },
      "outputs": [],
      "source": [
        "# Define your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rEuqAaiJXY9"
      },
      "outputs": [],
      "source": [
        "# Build train/val/test datasets and data loaders using the modules we built before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgVFPB_L4q8P"
      },
      "outputs": [],
      "source": [
        "# Train on the training set, validate on the validation set.\n",
        "# Search hyper-parameters to find the best model on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w_D5j7ziGMk"
      },
      "outputs": [],
      "source": [
        "# Generate test-set results..\n",
        "# For instance: the id of image 'test/00000001.jpg' should be '00000001' in string"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}